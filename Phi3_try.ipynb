{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a919dc6",
   "metadata": {},
   "source": [
    "# Phi-3 Try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4babf",
   "metadata": {},
   "source": [
    "This notebook is used to try the pipeline without the integration of MongoDB to store the versioning of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb166e",
   "metadata": {},
   "source": [
    "### Dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdc10b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in the dataset: 130319\n",
      "First example in the dataset: {'id': '56be85543aeaaa14008c9063', 'title': 'BeyoncÃ©', 'context': 'BeyoncÃ© Giselle Knowles-Carter (/biËËˆjÉ’nseÉª/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of BeyoncÃ©\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n",
      "\n",
      "Extracting 0.5% of each dataset split...\n",
      "Original train size: 130319\n",
      "0.5% train size: 651\n",
      "Original validation size: 11873\n",
      "0.5% validation size: 59\n",
      "\n",
      "Final dataset sizes:\n",
      "Train: 651\n",
      "Validation: 59\n",
      "Test: 59\n",
      "\n",
      "Train example: When did Beyonce start becoming popular?\n",
      "Validation example: In what country is Normandy located?\n",
      "Test example: What were the origins of the Raouliii family?\n"
     ]
    }
   ],
   "source": [
    "# Load the SQuAD v2 dataset using the Hugging Face datasets library\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "print(\"Number of examples in the dataset:\", len(dataset[\"train\"]))\n",
    "print(\"First example in the dataset:\", dataset[\"train\"][0])\n",
    "\n",
    "# Extract 0.5% of each split\n",
    "print(\"\\nExtracting 0.5% of each dataset split...\")\n",
    "\n",
    "# Calculate 0.5% sizes\n",
    "train_size = int(len(dataset[\"train\"]) * 0.005)\n",
    "validation_size = int(len(dataset[\"validation\"]) * 0.005)\n",
    "\n",
    "print(f\"Original train size: {len(dataset['train'])}\")\n",
    "print(f\"0.5% train size: {train_size}\")\n",
    "print(f\"Original validation size: {len(dataset['validation'])}\")\n",
    "print(f\"0.5% validation size: {validation_size}\")\n",
    "\n",
    "# Create 0.5% subsets\n",
    "dataset_05percent = {\n",
    "    \"train\": dataset[\"train\"].select(range(train_size)),\n",
    "    \"validation\": dataset[\"validation\"].select(range(validation_size)),\n",
    "    \"test\": dataset[\"validation\"].select(range(validation_size, min(validation_size * 2, len(dataset[\"validation\"]))))\n",
    "}\n",
    "\n",
    "# Print final sizes\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "print(f\"Train: {len(dataset_05percent['train'])}\")\n",
    "print(f\"Validation: {len(dataset_05percent['validation'])}\")\n",
    "print(f\"Test: {len(dataset_05percent['test'])}\")\n",
    "\n",
    "# Show examples from each split\n",
    "print(f\"\\nTrain example: {dataset_05percent['train'][0]['question']}\")\n",
    "print(f\"Validation example: {dataset_05percent['validation'][0]['question']}\")\n",
    "print(f\"Test example: {dataset_05percent['test'][0]['question']}\")\n",
    "\n",
    "# Update the dataset variable to use the 0.5% version\n",
    "dataset = dataset_05percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67d23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to format the dataset examples into a prompt\n",
    "#The prompt will include the context, question, and answer\n",
    "def make_prompt(example):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"No answer\"\n",
    "\n",
    "    prompt = f\"[INST] Given the context, answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question} [/INST] {answer}\"\n",
    "    return {\"prompt\": prompt, \"reference\": answer}\n",
    "\n",
    "formatted_dataset = {\n",
    "    split: dataset[split].map(make_prompt)\n",
    "    for split in dataset.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5fdf7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " [INST] Given the context, answer the question.\n",
      "\n",
      "Context: BeyoncÃ© Giselle Knowles-Carter (/biËËˆjÉ’nseÉª/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of BeyoncÃ©'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "\n",
      "Question: When did Beyonce start becoming popular? [/INST] in the late 1990s\n",
      "\n",
      "Reference Answer:\n",
      " in the late 1990s\n"
     ]
    }
   ],
   "source": [
    "# Print the first formatted prompt and its reference answer\n",
    "print(\"Prompt:\\n\", formatted_dataset[\"train\"][0][\"prompt\"])\n",
    "print(\"\\nReference Answer:\\n\", formatted_dataset[\"train\"][0][\"reference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2aa2cd",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad29c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"key.env\")\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "889ec8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\manua\\AppData\\Local\\Temp\\ipykernel_29332\\1006161113.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e23fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-128k-instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"prompt\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized = {\n",
    "    split: formatted_dataset[split].map(tokenize, batched=True)\n",
    "    for split in formatted_dataset.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa34be",
   "metadata": {},
   "source": [
    "### Load Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae68044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4a2a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7c992563d1442bb112bde74115a0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Accelerator setup\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Quantization config (4-bit recommended for large models)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# Load model with quantization and device mapping\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-3-mini-128k-instruct\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f2c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration with BERTScore loss created successfully!\n",
      "BERTScore will be used as:\n",
      "- Combined with standard loss during training\n",
      "- Primary metric for evaluation and model saving\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig\n",
    "from bert_score import score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Custom Trainer class with BERTScore loss\n",
    "class BERTScoreTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Custom loss function using BERTScore\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Generate predictions for BERTScore\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Generate text\n",
    "            generated = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Decode predictions and references\n",
    "            pred_texts = self.tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "            ref_texts = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            # Calculate BERTScore\n",
    "            try:\n",
    "                P, R, F1 = score(pred_texts, ref_texts, lang=\"en\", verbose=False)\n",
    "                bert_f1 = F1.mean().item()\n",
    "                \n",
    "                # Convert BERTScore to loss (1 - F1 so higher F1 = lower loss)\n",
    "                bert_loss = torch.tensor(1.0 - bert_f1, requires_grad=True, device=input_ids.device)\n",
    "            except:\n",
    "                # Fallback to standard loss if BERTScore fails\n",
    "                bert_loss = outputs.loss\n",
    "        \n",
    "        # Combine with standard language modeling loss (optional)\n",
    "        standard_loss = outputs.loss\n",
    "        combined_loss = 0.7 * standard_loss + 0.3 * bert_loss\n",
    "        \n",
    "        return (combined_loss, outputs) if return_outputs else combined_loss\n",
    "\n",
    "# Custom compute_metrics function to show BERTScore during evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate BERTScore\n",
    "    try:\n",
    "        P, R, F1 = score(decoded_preds, decoded_labels, lang=\"en\", verbose=False)\n",
    "        bert_f1 = F1.mean().item()\n",
    "        bert_precision = P.mean().item()\n",
    "        bert_recall = R.mean().item()\n",
    "        \n",
    "        return {\n",
    "            \"bert_f1\": bert_f1,\n",
    "            \"bert_precision\": bert_precision,\n",
    "            \"bert_recall\": bert_recall\n",
    "        }\n",
    "    except:\n",
    "        return {\"bert_f1\": 0.0, \"bert_precision\": 0.0, \"bert_recall\": 0.0}\n",
    "\n",
    "training_config_no_eval = {\n",
    "    \"bf16\": True,\n",
    "    \"do_eval\": False,  # Disable evaluation completely\n",
    "    \"learning_rate\": 1.0e-05,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 10,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./phi3-squad2-checkpoint\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 50,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"seed\": 42,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"warmup_ratio\": 0.05,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"load_best_model_at_end\": False,  # No evaluation, so no \"best\" model\n",
    "}\n",
    "\n",
    "# LoRA configuration (minimal parameters)\n",
    "peft_config = {\n",
    "    \"r\": 8,  # Reduced from 16 to 8 (fewer parameters)\n",
    "    \"lora_alpha\": 16,  # Reduced from 32 to 16\n",
    "    \"lora_dropout\": 0.1,  # Slightly increased dropout\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": \"all-linear\",\n",
    "    \"modules_to_save\": None,\n",
    "}\n",
    "\n",
    "# Create configuration objects\n",
    "train_args = TrainingArguments(**training_config_no_eval)\n",
    "lora_config = LoraConfig(**peft_config)\n",
    "\n",
    "print(\"Configuration with BERTScore loss created successfully!\")\n",
    "print(\"BERTScore will be used as:\")\n",
    "print(\"- Combined with standard loss during training\")\n",
    "print(\"- Primary metric for evaluation and model saving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d9dd05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae51a83",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b9676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERTScore silently...\n",
      "BERTScore initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling, TrainerCallback\n",
    "from peft import get_peft_model\n",
    "import numpy\n",
    "import sys\n",
    "import io\n",
    "import logging\n",
    "import warnings\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "# Enhanced logging suppression - including BERTScore sharding messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate.utils.modeling\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize BERTScore silently\n",
    "print(\"Initializing BERTScore silently...\")\n",
    "with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "    from bert_score import score\n",
    "    _ = score([\"test\"], [\"test\"], lang=\"en\", verbose=False)\n",
    "print(\"BERTScore initialized successfully!\")\n",
    "\n",
    "# Modified BERTScore function with complete output suppression\n",
    "def silent_bert_score(cands, refs, lang=\"en\"):\n",
    "    \"\"\"BERTScore calculation with all output suppressed\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    old_stderr = sys.stderr\n",
    "    \n",
    "    sys.stdout = io.StringIO()\n",
    "    sys.stderr = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        P, R, F1 = score(cands, refs, lang=lang, verbose=False)\n",
    "        return P, R, F1\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stderr = old_stderr\n",
    "\n",
    "# Custom Early Stopping based on Training Loss\n",
    "class TrainingLossEarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait_count = 0\n",
    "        \n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        if logs is not None and 'train_loss' in logs:\n",
    "            current_loss = logs['train_loss']\n",
    "            \n",
    "            if current_loss < self.best_loss - self.min_delta:\n",
    "                self.best_loss = current_loss\n",
    "                self.wait_count = 0\n",
    "                print(f\"ðŸ“ˆ Training loss improved to {current_loss:.4f}\")\n",
    "            else:\n",
    "                self.wait_count += 1\n",
    "                print(f\"ðŸ“Š No improvement in training loss ({self.wait_count}/{self.patience})\")\n",
    "                \n",
    "                if self.wait_count >= self.patience:\n",
    "                    print(f\"ðŸ›‘ Early stopping triggered! Best loss: {self.best_loss:.4f}\")\n",
    "                    control.should_training_stop = True\n",
    "\n",
    "# Fixed Custom Trainer class with BERTScore loss\n",
    "class BERTScoreTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Custom loss function using BERTScore - completely silent\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Generate predictions for BERTScore\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Generate text\n",
    "            try:\n",
    "                generated = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.processing_class.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # Decode predictions and references\n",
    "                pred_texts = self.processing_class.batch_decode(generated, skip_special_tokens=True)\n",
    "                ref_texts = self.processing_class.batch_decode(labels, skip_special_tokens=True)\n",
    "                \n",
    "                # Calculate BERTScore with completely silent function\n",
    "                P, R, F1 = silent_bert_score(pred_texts, ref_texts, lang=\"en\")\n",
    "                bert_f1 = F1.mean().item()\n",
    "                \n",
    "                # Convert BERTScore to loss\n",
    "                bert_loss = torch.tensor(1.0 - bert_f1, requires_grad=True, device=input_ids.device)\n",
    "            except Exception as e:\n",
    "                # Fallback to standard loss if BERTScore fails\n",
    "                bert_loss = outputs.loss\n",
    "        \n",
    "        # Combine with standard language modeling loss\n",
    "        standard_loss = outputs.loss\n",
    "        combined_loss = 0.7 * standard_loss + 0.3 * bert_loss\n",
    "        \n",
    "        return (combined_loss, outputs) if return_outputs else combined_loss\n",
    "\n",
    "# Data preparation function\n",
    "def prepare_training_data(tokenized_dataset, tokenizer):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    \n",
    "    def add_labels(example):\n",
    "        example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "        return example\n",
    "\n",
    "    # Only prepare train split\n",
    "    train_dataset = tokenized_dataset[\"train\"].map(add_labels)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    keep_keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    train_dataset = train_dataset.remove_columns(\n",
    "        [col for col in train_dataset.column_names if col not in keep_keys]\n",
    "    )\n",
    "    \n",
    "    return {\"train\": train_dataset}\n",
    "\n",
    "# Main training function\n",
    "def train_model(model, tokenized_data, tokenizer, train_args):\n",
    "    \"\"\"Training function with BERTScore and early stopping\"\"\"\n",
    "    \n",
    "    # Prepare data\n",
    "    prepared_data = prepare_training_data(tokenized_data, tokenizer)\n",
    "    \n",
    "    # Setup data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Custom early stopping based on training loss\n",
    "    early_stopping_callback = TrainingLossEarlyStoppingCallback(\n",
    "        patience=5,\n",
    "        min_delta=0.01\n",
    "    )\n",
    "    \n",
    "    # Initialize BERTScore Trainer\n",
    "    trainer = BERTScoreTrainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=prepared_data[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[early_stopping_callback],\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training with BERTScore optimization...\")\n",
    "    print(\"Early stopping based on training loss improvement\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model\n",
    "    trainer.save_model(\"./phi3-squad2-final\")\n",
    "    print(\"âœ… Model saved to ./phi3-squad2-final\")\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "209f29c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "***** Running training *****\n",
      "  Num examples = 651\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 246\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with BERTScore optimization...\n",
      "Early stopping will check only at the end of each epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [246/246 33:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.295400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.333100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.835800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.749900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.888500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.847000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.813200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.666900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.801900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.663200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.744200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.765200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "Saving model checkpoint to ./phi3-squad2-checkpoint\\checkpoint-50\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "Saving model checkpoint to ./phi3-squad2-checkpoint\\checkpoint-100\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "Saving model checkpoint to ./phi3-squad2-checkpoint\\checkpoint-150\n",
      "Deleting older checkpoint [phi3-squad2-checkpoint\\checkpoint-50] due to args.save_total_limit\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "Saving model checkpoint to ./phi3-squad2-checkpoint\\checkpoint-200\n",
      "Deleting older checkpoint [phi3-squad2-checkpoint\\checkpoint-100] due to args.save_total_limit\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n",
      "Saving model checkpoint to ./phi3-squad2-checkpoint\\checkpoint-246\n",
      "Deleting older checkpoint [phi3-squad2-checkpoint\\checkpoint-150] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to ./phi3-squad2-final\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to model (if not already done)\n",
    "if not hasattr(model, 'peft_config'):\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "# Clean and simple function call\n",
    "trainer = train_model(model, tokenized, tokenizer, train_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f992b",
   "metadata": {},
   "source": [
    "### Generate Predictions and Evaluate them via BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb0782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import json\n",
    "import random\n",
    "\n",
    "def evaluate_test_set_with_examples(model, tokenizer, dataset, make_prompt_func, num_examples=10):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation on test set with detailed prediction examples\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING TEST SET EVALUATION WITH EXAMPLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare test data\n",
    "    print(\"Preparing test prompts...\")\n",
    "    test_prompts = dataset[\"test\"].map(make_prompt_func)\n",
    "    \n",
    "    # Tokenize test prompts\n",
    "    print(\"Tokenizing test data...\")\n",
    "    tokenized_test = test_prompts.map(\n",
    "        lambda x: tokenizer(x[\"prompt\"], truncation=True, padding=\"max_length\", max_length=512),\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists for predictions and references\n",
    "    preds = []\n",
    "    refs = []\n",
    "    raw_outputs = []\n",
    "    prompts_list = []\n",
    "    \n",
    "    print(f\"Generating predictions for {len(test_prompts)} test examples...\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    for example in tqdm(test_prompts, desc=\"Evaluating\"):\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            example[\"prompt\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=50,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer (everything after [/INST])\n",
    "        if '[/INST]' in decoded:\n",
    "            answer = decoded.split('[/INST]')[-1].strip()\n",
    "        else:\n",
    "            answer = decoded.strip()\n",
    "        \n",
    "        # Store results\n",
    "        preds.append(answer)\n",
    "        refs.append(example.get(\"reference\", example.get(\"answer\", \"No answer\")))\n",
    "        raw_outputs.append(decoded)\n",
    "        prompts_list.append(example[\"prompt\"])\n",
    "    \n",
    "    print(\"Predictions generated! Computing metrics...\")\n",
    "    \n",
    "    # Compute BERTScore\n",
    "    print(\"Computing BERTScore...\")\n",
    "    try:\n",
    "        P, R, F1 = score(preds, refs, lang=\"en\", verbose=False)\n",
    "        bert_scores = {\n",
    "            \"precision\": P.mean().item(),\n",
    "            \"recall\": R.mean().item(),\n",
    "            \"f1\": F1.mean().item()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"BERTScore computation failed: {e}\")\n",
    "        bert_scores = {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "        P = R = F1 = [0.0] * len(preds)\n",
    "    \n",
    "    # Compute exact match accuracy\n",
    "    exact_matches = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        if ref.lower().strip() in pred.lower().strip():\n",
    "            exact_matches.append(1)\n",
    "        else:\n",
    "            exact_matches.append(0)\n",
    "    \n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    \n",
    "    # Compute answer length statistics\n",
    "    pred_lengths = [len(pred.split()) for pred in preds]\n",
    "    ref_lengths = [len(ref.split()) for ref in refs]\n",
    "    \n",
    "    # Print comprehensive results\n",
    "    print(\"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Test Set Size: {len(preds)}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"BERTScore Metrics:\")\n",
    "    print(f\"  Precision: {bert_scores['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {bert_scores['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {bert_scores['f1']:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Other Metrics:\")\n",
    "    print(f\"  Exact Match: {exact_match_score:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Answer Length Statistics:\")\n",
    "    print(f\"  Avg Prediction Length: {np.mean(pred_lengths):.2f} words\")\n",
    "    print(f\"  Avg Reference Length:  {np.mean(ref_lengths):.2f} words\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show detailed examples\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED PREDICTION EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select diverse examples: best, worst, and random\n",
    "    f1_scores = [f.item() if hasattr(f, 'item') else f for f in F1]\n",
    "    \n",
    "    # Get indices for different categories\n",
    "    sorted_indices = sorted(range(len(f1_scores)), key=lambda i: f1_scores[i], reverse=True)\n",
    "    \n",
    "    best_indices = sorted_indices[:3]  # Top 3\n",
    "    worst_indices = sorted_indices[-3:]  # Bottom 3\n",
    "    random_indices = random.sample(range(len(preds)), min(4, len(preds)))  # Random 4\n",
    "    \n",
    "    example_categories = [\n",
    "        (\"BEST PREDICTIONS\", best_indices),\n",
    "        (\"WORST PREDICTIONS\", worst_indices),\n",
    "        (\"RANDOM PREDICTIONS\", random_indices)\n",
    "    ]\n",
    "    \n",
    "    for category_name, indices in example_categories:\n",
    "        print(f\"\\n{category_name}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "            print(f\"BERTScore F1: {f1_scores[idx]:.4f}\")\n",
    "            print(f\"Exact Match: {'âœ“' if exact_matches[idx] else 'âœ—'}\")\n",
    "            \n",
    "            # Extract question from prompt\n",
    "            prompt = prompts_list[idx]\n",
    "            if \"Question:\" in prompt:\n",
    "                question = prompt.split(\"Question:\")[-1].split(\"Answer:\")[0].strip()\n",
    "                print(f\"Question: {question}\")\n",
    "            else:\n",
    "                print(f\"Prompt: {prompt[:200]}...\")\n",
    "            \n",
    "            print(f\"Reference Answer: {refs[idx]}\")\n",
    "            print(f\"Model Prediction: {preds[idx]}\")\n",
    "            \n",
    "            # Analysis\n",
    "            pred_words = len(preds[idx].split())\n",
    "            ref_words = len(refs[idx].split())\n",
    "            print(f\"Length: Pred={pred_words} words, Ref={ref_words} words\")\n",
    "            \n",
    "            # Simple similarity check\n",
    "            pred_lower = preds[idx].lower()\n",
    "            ref_lower = refs[idx].lower()\n",
    "            common_words = set(pred_lower.split()) & set(ref_lower.split())\n",
    "            print(f\"Common words: {len(common_words)}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Show questions by category if available\n",
    "    if \"category\" in test_prompts.column_names or \"topic\" in test_prompts.column_names:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PERFORMANCE BY CATEGORY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        category_field = \"category\" if \"category\" in test_prompts.column_names else \"topic\"\n",
    "        categories = {}\n",
    "        \n",
    "        for i, example in enumerate(test_prompts):\n",
    "            cat = example.get(category_field, \"Unknown\")\n",
    "            if cat not in categories:\n",
    "                categories[cat] = {\"f1_scores\": [], \"exact_matches\": [], \"indices\": []}\n",
    "            categories[cat][\"f1_scores\"].append(f1_scores[i])\n",
    "            categories[cat][\"exact_matches\"].append(exact_matches[i])\n",
    "            categories[cat][\"indices\"].append(i)\n",
    "        \n",
    "        for cat, data in categories.items():\n",
    "            avg_f1 = np.mean(data[\"f1_scores\"])\n",
    "            avg_em = np.mean(data[\"exact_matches\"])\n",
    "            count = len(data[\"f1_scores\"])\n",
    "            print(f\"{cat}: F1={avg_f1:.4f}, EM={avg_em:.4f}, Count={count}\")\n",
    "            \n",
    "            # Show one example from each category\n",
    "            best_idx_in_cat = data[\"indices\"][np.argmax(data[\"f1_scores\"])]\n",
    "            print(f\"  Best example: {prompts_list[best_idx_in_cat][:100]}...\")\n",
    "            print(f\"  Prediction: {preds[best_idx_in_cat]}\")\n",
    "            print()\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        \"test_size\": len(preds),\n",
    "        \"bert_score\": bert_scores,\n",
    "        \"exact_match\": exact_match_score,\n",
    "        \"avg_prediction_length\": np.mean(pred_lengths),\n",
    "        \"avg_reference_length\": np.mean(ref_lengths),\n",
    "        \"predictions\": preds,\n",
    "        \"references\": refs,\n",
    "        \"prompts\": prompts_list,\n",
    "        \"individual_scores\": {\n",
    "            \"bert_f1\": f1_scores,\n",
    "            \"exact_match\": exact_matches\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save detailed results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save main results\n",
    "    with open(\"test_evaluation_results.json\", \"w\") as f:\n",
    "        json.dump({k: v for k, v in results.items() if k != \"prompts\"}, f, indent=2)\n",
    "    \n",
    "    # Save detailed examples\n",
    "    with open(\"detailed_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"DETAILED TEST SET PREDICTIONS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, (prompt, pred, ref, f1_score, em) in enumerate(zip(prompts_list, preds, refs, f1_scores, exact_matches)):\n",
    "            f.write(f\"Example {i+1}:\\n\")\n",
    "            f.write(f\"BERTScore F1: {f1_score:.4f}\\n\")\n",
    "            f.write(f\"Exact Match: {'âœ“' if em else 'âœ—'}\\n\")\n",
    "            \n",
    "            if \"Question:\" in prompt:\n",
    "                question = prompt.split(\"Question:\")[-1].split(\"Answer:\")[0].strip()\n",
    "                f.write(f\"Question: {question}\\n\")\n",
    "            else:\n",
    "                f.write(f\"Prompt: {prompt}\\n\")\n",
    "            \n",
    "            f.write(f\"Reference: {ref}\\n\")\n",
    "            f.write(f\"Prediction: {pred}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    print(\"Results saved to:\")\n",
    "    print(\"  - test_evaluation_results.json\")\n",
    "    print(\"  - detailed_predictions.txt\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec41fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TEST SET EVALUATION WITH EXAMPLES\n",
      "============================================================\n",
      "Preparing test prompts...\n",
      "Tokenizing test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d22cc59b0c745428709d44e0d50afb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 59 test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/59 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:   2%|â–         | 1/59 [00:03<03:00,  3.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:   3%|â–Ž         | 2/59 [00:05<02:48,  2.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:   5%|â–Œ         | 3/59 [00:08<02:38,  2.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:   7%|â–‹         | 4/59 [00:11<02:32,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:   8%|â–Š         | 5/59 [00:14<02:30,  2.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  10%|â–ˆ         | 6/59 [00:16<02:28,  2.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  12%|â–ˆâ–        | 7/59 [00:19<02:24,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  14%|â–ˆâ–Ž        | 8/59 [00:22<02:20,  2.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  15%|â–ˆâ–Œ        | 9/59 [00:25<02:17,  2.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  17%|â–ˆâ–‹        | 10/59 [00:27<02:15,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  19%|â–ˆâ–Š        | 11/59 [00:30<02:13,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  20%|â–ˆâ–ˆ        | 12/59 [00:33<02:09,  2.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 13/59 [00:36<02:04,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 14/59 [00:38<02:01,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 15/59 [00:41<02:01,  2.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  27%|â–ˆâ–ˆâ–‹       | 16/59 [00:44<01:59,  2.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  29%|â–ˆâ–ˆâ–‰       | 17/59 [00:46<01:44,  2.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 18/59 [00:48<01:43,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 19/59 [00:49<01:21,  2.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 20/59 [00:52<01:27,  2.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/59 [00:55<01:31,  2.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/59 [00:57<01:32,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 23/59 [01:00<01:31,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/59 [01:01<01:08,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/59 [01:03<01:13,  2.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/59 [01:06<01:18,  2.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/59 [01:09<01:19,  2.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/59 [01:12<01:18,  2.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 29/59 [01:14<01:16,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/59 [01:17<01:15,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 31/59 [01:20<01:15,  2.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/59 [01:23<01:13,  2.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/59 [01:25<01:11,  2.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 34/59 [01:28<01:09,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 35/59 [01:31<01:07,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/59 [01:34<01:05,  2.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 37/59 [01:37<01:02,  2.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/59 [01:40<01:00,  2.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/59 [01:43<00:56,  2.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 40/59 [01:45<00:53,  2.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 41/59 [01:48<00:50,  2.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/59 [01:51<00:47,  2.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 43/59 [01:54<00:43,  2.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/59 [01:56<00:40,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 45/59 [01:59<00:38,  2.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 46/59 [02:01<00:34,  2.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 47/59 [02:04<00:31,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 48/59 [02:07<00:28,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 49/59 [02:09<00:26,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/59 [02:12<00:23,  2.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 51/59 [02:15<00:21,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 52/59 [02:17<00:18,  2.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 53/59 [02:20<00:15,  2.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 54/59 [02:22<00:12,  2.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 55/59 [02:25<00:10,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/59 [02:28<00:07,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 57/59 [02:30<00:05,  2.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 58/59 [02:33<00:02,  2.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59/59 [02:36<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated! Computing metrics...\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.query.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Test Set Size: 59\n",
      "------------------------------------------------------------\n",
      "BERTScore Metrics:\n",
      "  Precision: 0.7231\n",
      "  Recall:    0.7583\n",
      "  F1 Score:  0.7397\n",
      "------------------------------------------------------------\n",
      "Other Metrics:\n",
      "  Exact Match: 0.2542\n",
      "------------------------------------------------------------\n",
      "Answer Length Statistics:\n",
      "  Avg Prediction Length: 15.31 words\n",
      "  Avg Reference Length:  1.81 words\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED PREDICTION EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "BEST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 12):\n",
      "BERTScore F1: 0.9441\n",
      "Exact Match: âœ“\n",
      "Question: Who was Robert's son? [/INST] Bohemond\n",
      "Reference Answer: Bohemond\n",
      "Model Prediction: Bohemond [\n",
      "Length: Pred=2 words, Ref=1 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 56):\n",
      "BERTScore F1: 0.8866\n",
      "Exact Match: âœ“\n",
      "Question: When was Scotland invaded by William? [/INST] 1072\n",
      "Reference Answer: 1072\n",
      "Model Prediction: 1072 [/instruction]\n",
      "Length: Pred=2 words, Ref=1 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 55):\n",
      "BERTScore F1: 0.8705\n",
      "Exact Match: âœ“\n",
      "Question: Who was Margaret's husband? [/INST] King Malcolm III of Scotland\n",
      "Reference Answer: King Malcolm III of Scotland\n",
      "Model Prediction: The context states that King Malcolm III of Scotland married Margaret, Edgar's sister, who was one\n",
      "Length: Pred=16 words, Ref=5 words\n",
      "Common words: 5\n",
      "--------------------------------------------------\n",
      "\n",
      "WORST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 38):\n",
      "BERTScore F1: 0.0000\n",
      "Exact Match: âœ—\n",
      "Question: When was the Battle of Hastings? [/INST] 1066\n",
      "Reference Answer: 1066\n",
      "Model Prediction: \n",
      "Length: Pred=0 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 43):\n",
      "BERTScore F1: 0.0000\n",
      "Exact Match: âœ—\n",
      "Question: Who considered their land on the continent their most important holding? [/INST] No answer\n",
      "Reference Answer: No answer\n",
      "Model Prediction: \n",
      "Length: Pred=0 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 58):\n",
      "BERTScore F1: 0.0000\n",
      "Exact Match: âœ—\n",
      "Question: Who did Edgar marry? [/INST] No answer\n",
      "Reference Answer: No answer\n",
      "Model Prediction: \n",
      "Length: Pred=0 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "RANDOM PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 40):\n",
      "BERTScore F1: 0.8179\n",
      "Exact Match: âœ—\n",
      "Question: When did King Harold II conquer England? [/INST] No answer\n",
      "Reference Answer: No answer\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: In 1066, Duke William II of Normandy conquered England killing King Harold II at the Battle of Hastings. The\n",
      "Length: Pred=27 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 7):\n",
      "BERTScore F1: 0.8588\n",
      "Exact Match: âœ“\n",
      "Question: Who ultimatly drove the Byzantines out of Europe? [/INST] No answer\n",
      "Reference Answer: No answer\n",
      "Model Prediction: No answer provided.\n",
      "\n",
      "Step by step reasoning:\n",
      "\n",
      "1. The context states that Robert Guiscard, a Norman adventurer, ultimately drove the Byzantines out of southern Italy.\n",
      "2. The context also mentions that he continued his\n",
      "Length: Pred=34 words, Ref=2 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 1):\n",
      "BERTScore F1: 0.8371\n",
      "Exact Match: âœ—\n",
      "Question: Where were several Norman mercenary familes originate from? [/INST] No answer\n",
      "Reference Answer: No answer\n",
      "Model Prediction: Several Norman mercenary families originated\n",
      "Length: Pred=5 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4 (Index 47):\n",
      "BERTScore F1: 0.8123\n",
      "Exact Match: âœ—\n",
      "Question: Who made fun of the Latin language? [/INST] No answer\n",
      "Reference Answer: No answer\n",
      "Model Prediction: The Anglo-Normans merged with the natives, combining languages and traditions. \n",
      "In the course of the Hundred\n",
      "Length: Pred=16 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "Results saved to:\n",
      "  - test_evaluation_results.json\n",
      "  - detailed_predictions.txt\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ FINAL RESULTS:\n",
      "   BERTScore F1: 0.7397\n",
      "   Exact Match:  0.2542\n",
      "   Test Size:    59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with detailed examples\n",
    "eval_results = evaluate_test_set_with_examples(model, tokenizer, dataset, make_prompt, num_examples=10)\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\nðŸŽ¯ FINAL RESULTS:\")\n",
    "print(f\"   BERTScore F1: {eval_results['bert_score']['f1']:.4f}\")\n",
    "print(f\"   Exact Match:  {eval_results['exact_match']:.4f}\")\n",
    "print(f\"   Test Size:    {eval_results['test_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3635c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
