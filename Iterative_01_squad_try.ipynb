{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecb8a69",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d4eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from bert_score import score\n",
    "from datasets import Dataset, load_from_disk, DatasetDict\n",
    "from dotenv import load_dotenv\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88e747",
   "metadata": {},
   "source": [
    "## Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3548d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad_subset(dataset_dir=\"squad_v2_05percent\"):\n",
    "    \"\"\"\n",
    "    Load the saved SQuAD v2 0.5% dataset from disk.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir (str): Directory where the dataset was saved\n",
    "        \n",
    "    Returns:\n",
    "        DatasetDict: The loaded dataset with train and test splits\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{dataset_dir}' not found. Please run the extraction script first.\")\n",
    "    \n",
    "    print(f\"Loading dataset from {dataset_dir}...\")\n",
    "    \n",
    "    # Load the dataset using Hugging Face datasets\n",
    "    dataset = load_from_disk(dataset_dir)\n",
    "    \n",
    "    # Load and display metadata\n",
    "    metadata_path = os.path.join(dataset_dir, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"Dataset metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"\\nLoaded dataset splits:\")\n",
    "    for split_name, split_data in dataset.items():\n",
    "        print(f\"  {split_name}: {len(split_data)} examples\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_synthetic_train(base_dataset_dir, train_dataset_dir):\n",
    "    \"\"\"\n",
    "    Load the saved synthetic train dataset from disk, merged with the test dataset if available.\n",
    "    \n",
    "    Args:\n",
    "        base_dataset_dir (str): Base directory where the dataset was saved, in order to retrieve the test\n",
    "        train_dataset_dir (str): Directory where the previous generation train dataset was saved\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: The loaded train dataset\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{train_dataset_dir}' not found.\")\n",
    "    if not os.path.exists(train_dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{train_dataset_dir}' not found.\")\n",
    "    \n",
    "    print(f\"Loading train dataset from {train_dataset_dir}...\")\n",
    "    \n",
    "    # Try to load the full dataset first\n",
    "    try:\n",
    "        full_dataset = load_from_disk(base_dataset_dir)\n",
    "        train_dataset = load_from_disk(train_dataset_dir)\n",
    "        \n",
    "        # Check if it's a DatasetDict with train split\n",
    "        if isinstance(full_dataset, dict) and 'train' in full_dataset:\n",
    "            train_dataset = full_dataset['train']\n",
    "        elif hasattr(full_dataset, 'column_names'):\n",
    "            # It's already a single Dataset\n",
    "            train_dataset = full_dataset\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected dataset format in {train_dataset_dir}\")\n",
    "            \n",
    "    except Exception:\n",
    "        # Try to load from train subdirectory if full dataset fails\n",
    "        train_dir = os.path.join(train_dataset_dir, \"train\")\n",
    "        if os.path.exists(train_dir):\n",
    "            train_dataset = load_from_disk(train_dir)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No train dataset found in {train_dataset_dir}\")\n",
    "    \n",
    "    # Load and display metadata if available\n",
    "    metadata_path = os.path.join(train_dataset_dir, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"Dataset metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Display train dataset info\n",
    "    #print(f\"\\nLoaded train dataset:\")\n",
    "    #print(f\"  Examples: {len(train_dataset)}\")\n",
    "    #print(f\"  Columns: {train_dataset.column_names}\")\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': full_dataset['test']\n",
    "    })\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259e321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from squad_v2_01percent...\n",
      "Dataset metadata:\n",
      "  original_train_size: 130319\n",
      "  original_validation_size: 11873\n",
      "  extracted_train_size: 130\n",
      "  extracted_test_size: 11\n",
      "  extraction_percentage: 0.1\n",
      "  sampling_method: random\n",
      "  seed: 42\n",
      "  train_answerable: 94\n",
      "  test_answerable: 3\n",
      "  train_answerable_percentage: 72.3076923076923\n",
      "  test_answerable_percentage: 27.27272727272727\n",
      "  dataset_format: squad_v2\n",
      "  splits: ['train', 'test']\n",
      "\n",
      "Loaded dataset splits:\n",
      "  train: 130 examples\n",
      "  test: 11 examples\n",
      "\n",
      "Example from each split:\n",
      "Train: What century did Nasser rule in?\n",
      "Test: How many State of California University campuses are there?\n",
      "\n",
      "Dataset features: {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_squad_subset(\"squad_v2_01percent\")\n",
    "    \n",
    "# Show examples\n",
    "print(f\"\\nExample from each split:\")\n",
    "print(f\"Train: {dataset['train'][0]['question']}\")\n",
    "print(f\"Test: {dataset['test'][0]['question']}\")\n",
    "\n",
    "# Access specific fields\n",
    "print(f\"\\nDataset features: {dataset['train'].features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774e7922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec012e6ebf5642de99f2a0a06b360ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2a2b22ac444aaeab7f4e75838b3d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " [INST] Given the context, answer the question.\n",
      "\n",
      "Context: Nasser remains an iconic figure in the Arab world, particularly for his strides towards social justice and Arab unity, modernization policies, and anti-imperialist efforts. His presidency also encouraged and coincided with an Egyptian cultural boom, and launched large industrial projects, including the Aswan Dam and Helwan City. Nasser's detractors criticize his authoritarianism, his government's human rights violations, his populist relationship with the citizenry, and his failure to establish civil institutions, blaming his legacy for future dictatorial governance in Egypt. Historians describe Nasser as a towering political figure of the Middle East in the 20th century.\n",
      "\n",
      "Question: What century did Nasser rule in? [/INST] Answer: 20th\n",
      "\n",
      "Reference Answer:\n",
      " 20th\n"
     ]
    }
   ],
   "source": [
    "def make_prompt(example):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"No answer\"\n",
    "\n",
    "    prompt = f\"[INST] Given the context, answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question} [/INST] Answer: {answer}\"\n",
    "    return {\"prompt\": prompt, \"reference\": answer}\n",
    "\n",
    "formatted_dataset = {\n",
    "    split: dataset[split].map(make_prompt)\n",
    "    for split in dataset.keys()\n",
    "}\n",
    "\n",
    "# Print the first formatted prompt and its reference answer\n",
    "print(\"Prompt:\\n\", formatted_dataset[\"train\"][0][\"prompt\"])\n",
    "print(\"\\nReference Answer:\\n\", formatted_dataset[\"train\"][0][\"reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79667a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"key.env\")\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b104edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced logging suppression - including BERTScore sharding messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate.utils.modeling\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395fcac",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0073640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERTScore silently...\n",
      "BERTScore initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERTScore silently\n",
    "print(\"Initializing BERTScore silently...\")\n",
    "with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "    from bert_score import score\n",
    "    _ = score([\"test\"], [\"test\"], lang=\"en\", verbose=False)\n",
    "print(\"BERTScore initialized successfully!\")\n",
    "\n",
    "# Modified BERTScore function with complete output suppression\n",
    "def silent_bert_score(cands, refs, lang=\"en\"):\n",
    "    \"\"\"BERTScore calculation with all output suppressed\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    old_stderr = sys.stderr\n",
    "    \n",
    "    sys.stdout = io.StringIO()\n",
    "    sys.stderr = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        P, R, F1 = score(cands, refs, lang=lang, verbose=False)\n",
    "        return P, R, F1\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stderr = old_stderr\n",
    "\n",
    "# Custom Early Stopping based on Training Loss\n",
    "class TrainingLossEarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait_count = 0\n",
    "        \n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        if logs is not None and 'train_loss' in logs:\n",
    "            current_loss = logs['train_loss']\n",
    "            \n",
    "            if current_loss < self.best_loss - self.min_delta:\n",
    "                self.best_loss = current_loss\n",
    "                self.wait_count = 0\n",
    "                print(f\"ðŸ“ˆ Training loss improved to {current_loss:.4f}\")\n",
    "            else:\n",
    "                self.wait_count += 1\n",
    "                print(f\"ðŸ“Š No improvement in training loss ({self.wait_count}/{self.patience})\")\n",
    "                \n",
    "                if self.wait_count >= self.patience:\n",
    "                    print(f\"ðŸ›‘ Early stopping triggered! Best loss: {self.best_loss:.4f}\")\n",
    "                    control.should_training_stop = True\n",
    "\n",
    "# Fixed Custom Trainer class with BERTScore loss\n",
    "class BERTScoreTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.label_names = [\"labels\"]\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Custom loss function using BERTScore - completely silent\n",
    "        Extracts only the answer part after \"Answer: \" for BERTScore computation\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Temporarily disable cache for forward pass\n",
    "        model.config.use_cache = False\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Generate predictions for BERTScore\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Re-enable cache for generation\n",
    "            model.config.use_cache = True\n",
    "            \n",
    "            # Generate text\n",
    "            try:\n",
    "                generated = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.processing_class.eos_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "                \n",
    "                # Decode predictions and references\n",
    "                pred_texts = self.processing_class.batch_decode(generated, skip_special_tokens=True)\n",
    "                ref_texts = self.processing_class.batch_decode(labels, skip_special_tokens=True)\n",
    "                \n",
    "                # Extract only the answer part from both predictions and references\n",
    "                extracted_preds = []\n",
    "                extracted_refs = []\n",
    "                \n",
    "                for pred_text in pred_texts:\n",
    "                    # Extract answer after \"Answer: \" in prediction\n",
    "                    if \"Answer: \" in pred_text:\n",
    "                        answer_part = pred_text.split(\"Answer: \")[-1].strip()\n",
    "                    elif \"[/INST]\" in pred_text:\n",
    "                        # Fallback: extract after [/INST] if \"Answer: \" not found\n",
    "                        answer_part = pred_text.split(\"[/INST]\")[-1].strip()\n",
    "                        # Remove \"Answer: \" prefix if it exists\n",
    "                        if answer_part.startswith(\"Answer: \"):\n",
    "                            answer_part = answer_part[8:].strip()\n",
    "                    else:\n",
    "                        answer_part = pred_text.strip()\n",
    "                    \n",
    "                    extracted_preds.append(answer_part)\n",
    "                \n",
    "                for ref_text in ref_texts:\n",
    "                    # Extract answer after \"Answer: \" in reference\n",
    "                    if \"Answer: \" in ref_text:\n",
    "                        answer_part = ref_text.split(\"Answer: \")[-1].strip()\n",
    "                    elif \"[/INST]\" in ref_text:\n",
    "                        # Fallback: extract after [/INST] if \"Answer: \" not found\n",
    "                        answer_part = ref_text.split(\"[/INST]\")[-1].strip()\n",
    "                        # Remove \"Answer: \" prefix if it exists\n",
    "                        if answer_part.startswith(\"Answer: \"):\n",
    "                            answer_part = answer_part[8:].strip()\n",
    "                    else:\n",
    "                        answer_part = ref_text.strip()\n",
    "                    \n",
    "                    extracted_refs.append(answer_part)\n",
    "                \n",
    "                # Calculate BERTScore with completely silent function on extracted answers only\n",
    "                P, R, F1 = silent_bert_score(extracted_preds, extracted_refs, lang=\"en\")\n",
    "                bert_f1 = F1.mean().item()\n",
    "                \n",
    "                # Convert BERTScore to loss\n",
    "                bert_loss = torch.tensor(1.0 - bert_f1, requires_grad=True, device=input_ids.device)\n",
    "            except Exception as e:\n",
    "                # Fallback to standard loss if BERTScore fails\n",
    "                bert_loss = outputs.loss\n",
    "            finally:\n",
    "                # Disable cache again for gradient checkpointing compatibility\n",
    "                model.config.use_cache = False\n",
    "        \n",
    "        # Combine with standard language modeling loss\n",
    "        standard_loss = outputs.loss\n",
    "        combined_loss = 0.7 * standard_loss + 0.3 * bert_loss\n",
    "        \n",
    "        return (combined_loss, outputs) if return_outputs else combined_loss\n",
    "\n",
    "# Data preparation function - removed tokenizer parameter since it's not used\n",
    "def prepare_training_data(tokenized_dataset):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    \n",
    "    def add_labels(example):\n",
    "        example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "        return example\n",
    "\n",
    "    # Only prepare train split\n",
    "    train_dataset = tokenized_dataset[\"train\"].map(add_labels)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    keep_keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    train_dataset = train_dataset.remove_columns(\n",
    "        [col for col in train_dataset.column_names if col not in keep_keys]\n",
    "    )\n",
    "    \n",
    "    return {\"train\": train_dataset}\n",
    "\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    \"\"\"Remove checkpoint directories and files\"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        # Find all checkpoint directories\n",
    "        checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith('checkpoint-')]\n",
    "        \n",
    "        for checkpoint_dir in checkpoint_dirs:\n",
    "            checkpoint_path = os.path.join(output_dir, checkpoint_dir)\n",
    "            if os.path.isdir(checkpoint_path):\n",
    "                print(f\"ðŸ—‘ï¸ Removing checkpoint: {checkpoint_path}\")\n",
    "                shutil.rmtree(checkpoint_path)\n",
    "        \n",
    "        # Remove any other checkpoint-related files\n",
    "        checkpoint_files = [f for f in os.listdir(output_dir) if 'checkpoint' in f.lower()]\n",
    "        for checkpoint_file in checkpoint_files:\n",
    "            file_path = os.path.join(output_dir, checkpoint_file)\n",
    "            if os.path.isfile(file_path):\n",
    "                print(f\"ðŸ—‘ï¸ Removing checkpoint file: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        print(\"âœ… Checkpoint cleanup completed!\")\n",
    "\n",
    "def configure_model_for_training(model):\n",
    "    \"\"\"Configure model for training with proper cache settings\"\"\"\n",
    "    \n",
    "    # Disable use_cache for training compatibility with gradient checkpointing\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = False\n",
    "        print(\"âœ… Set use_cache=False for gradient checkpointing compatibility\")\n",
    "    \n",
    "    # Enable gradient checkpointing if available\n",
    "    if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"âœ… Enabled gradient checkpointing for memory efficiency\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main training function\n",
    "def train_model(model, tokenized_data, tokenizer, train_args):\n",
    "    \"\"\"Training function with BERTScore and early stopping\"\"\"\n",
    "    \n",
    "    # Configure model for training\n",
    "    model = configure_model_for_training(model)\n",
    "    \n",
    "    # Prepare data\n",
    "    prepared_data = prepare_training_data(tokenized_data)\n",
    "    \n",
    "    # Setup data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Custom early stopping based on training loss\n",
    "    early_stopping_callback = TrainingLossEarlyStoppingCallback(\n",
    "        patience=5,\n",
    "        min_delta=0.01\n",
    "    )\n",
    "    \n",
    "    # Initialize BERTScore Trainer\n",
    "    trainer = BERTScoreTrainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=prepared_data[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[early_stopping_callback],\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training with BERTScore optimization...\")\n",
    "    print(\"Early stopping based on training loss improvement\")\n",
    "    print(\"Cache disabled for gradient checkpointing compatibility\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Re-enable cache for inference after training\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = True\n",
    "        print(\"âœ… Re-enabled use_cache for inference\")\n",
    "    \n",
    "    # Save model\n",
    "    final_model_path = \"./phi3-squad2-final\"\n",
    "    trainer.save_model(final_model_path)\n",
    "    print(f\"âœ… Model saved to {final_model_path}\")\n",
    "    \n",
    "    # Clean up checkpoints after saving the final model\n",
    "    print(\"\\nðŸ§¹ Cleaning up checkpoints...\")\n",
    "    if hasattr(train_args, 'output_dir') and train_args.output_dir:\n",
    "        cleanup_checkpoints(train_args.output_dir)\n",
    "    \n",
    "    # Also clean up from the final model directory if it has checkpoints\n",
    "    cleanup_checkpoints(final_model_path)\n",
    "    \n",
    "    # Clean up any checkpoint directories in the current working directory\n",
    "    current_dir_checkpoints = [d for d in os.listdir('.') if d.startswith('checkpoint-')]\n",
    "    for checkpoint_dir in current_dir_checkpoints:\n",
    "        if os.path.isdir(checkpoint_dir):\n",
    "            print(f\"ðŸ—‘ï¸ Removing checkpoint: {checkpoint_dir}\")\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    print(\"ðŸŽ‰ Training completed and checkpoints cleaned up!\")\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2977f59",
   "metadata": {},
   "source": [
    "## Synthetic Answer generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09d9cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_answers(model, tokenizer, formatted_dataset, device, generation_num=1):\n",
    "    \"\"\"\n",
    "    Generate synthetic answers using the fine-tuned causal language model.\n",
    "    Takes formatted_dataset with prompts as input.\n",
    "    Updated to handle \"Answer: \" format and write \"No answer\" when appropriate.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Generating synthetic answers (Generation {generation_num})...\")\n",
    "    \n",
    "    synthetic_data = []\n",
    "    \n",
    "    # Use the train split from formatted_dataset\n",
    "    train_dataset = formatted_dataset['train']\n",
    "    \n",
    "    # Enhanced progress bar with statistics\n",
    "    progress_bar = tqdm(\n",
    "        train_dataset, \n",
    "        desc=f\"ðŸ¤– Gen {generation_num} - Generating answers\",\n",
    "        unit=\"examples\",\n",
    "        position=1,\n",
    "        leave=False,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "    \n",
    "    # Statistics tracking\n",
    "    successful_generations = 0\n",
    "    failed_generations = 0\n",
    "    total_examples = len(train_dataset)\n",
    "    \n",
    "    for idx, example in enumerate(progress_bar):\n",
    "        # Get the prompt that was created by make_prompt function\n",
    "        prompt = example['prompt']\n",
    "        \n",
    "        # For your format: \"[INST] ... [/INST] Answer: {answer}\"\n",
    "        # We need to generate starting from just before \"Answer:\"\n",
    "        if '[/INST] Answer:' in prompt:\n",
    "            # Remove the existing answer to create generation prompt\n",
    "            generation_prompt = prompt.split(' Answer:')[0] + ' Answer:'\n",
    "        elif '[/INST]' in prompt:\n",
    "            # Fallback: if no \"Answer:\" found, add it\n",
    "            generation_prompt = prompt.split('[/INST]')[0] + '[/INST] Answer:'\n",
    "        else:\n",
    "            # Fallback: use the full prompt\n",
    "            generation_prompt = prompt\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input - increased max_length to handle full context\n",
    "            inputs = tokenizer(\n",
    "                generation_prompt,\n",
    "                max_length=512,  # Increased from 50 to handle full context\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Generate answer using causal LM\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,   # Reduced for concise answers\n",
    "                    do_sample=True,      # Keep diversity\n",
    "                    temperature=0.7,     # Controlled randomness\n",
    "                    top_p=0.9,          # Nucleus sampling\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract only the answer part after \"Answer: \"\n",
    "            if 'Answer:' in generated_text:\n",
    "                # Split by \"Answer:\" and take the last part (the generated answer)\n",
    "                answer_parts = generated_text.split('Answer:')\n",
    "                if len(answer_parts) > 1:\n",
    "                    synthetic_answer = answer_parts[-1].strip()\n",
    "                else:\n",
    "                    synthetic_answer = \"No answer\"\n",
    "            else:\n",
    "                # Fallback: extract after [/INST] if \"Answer:\" not found\n",
    "                if '[/INST]' in generated_text:\n",
    "                    synthetic_answer = generated_text.split('[/INST]')[-1].strip()\n",
    "                    # Remove \"Answer:\" prefix if it exists\n",
    "                    if synthetic_answer.startswith('Answer:'):\n",
    "                        synthetic_answer = synthetic_answer[7:].strip()\n",
    "                else:\n",
    "                    synthetic_answer = \"No answer\"\n",
    "            \n",
    "            # Additional cleanup and validation\n",
    "            if synthetic_answer:\n",
    "                # Stop at first sentence if answer is too long\n",
    "                sentences = synthetic_answer.split('.')\n",
    "                if len(sentences) > 1 and len(sentences[0]) > 5:\n",
    "                    synthetic_answer = sentences[0].strip()\n",
    "                \n",
    "                # Remove any remaining formatting artifacts\n",
    "                synthetic_answer = synthetic_answer.replace('\\n', ' ').strip()\n",
    "                \n",
    "                # Check if answer is reasonable (not empty, not too long)\n",
    "                if len(synthetic_answer) < 2 or len(synthetic_answer) > 200:\n",
    "                    synthetic_answer = \"No answer\"\n",
    "                    failed_generations += 1\n",
    "                else:\n",
    "                    # Check if the answer exists in the context (if context is available)\n",
    "                    context = example.get('context', '')\n",
    "                    if context and synthetic_answer.lower() not in context.lower():\n",
    "                        # Answer not found in context, mark as \"No answer\"\n",
    "                        synthetic_answer = \"No answer\"\n",
    "                        failed_generations += 1\n",
    "                    else:\n",
    "                        successful_generations += 1\n",
    "            else:\n",
    "                synthetic_answer = \"No answer\"\n",
    "                failed_generations += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any generation errors\n",
    "            synthetic_answer = \"No answer\"\n",
    "            failed_generations += 1\n",
    "            print(f\"\\nWarning: Generation failed for example {idx}: {str(e)}\")\n",
    "        \n",
    "        # Create new example with synthetic answer\n",
    "        new_example = example.copy()\n",
    "        \n",
    "        # Update the prompt to include the generated answer in correct format\n",
    "        # Format: \"[INST] ... [/INST] Answer: {synthetic_answer}\"\n",
    "        if '[/INST]' in prompt:\n",
    "            base_prompt = prompt.split('[/INST]')[0] + '[/INST] Answer:'\n",
    "            new_example['prompt'] = base_prompt + ' ' + synthetic_answer\n",
    "        else:\n",
    "            new_example['prompt'] = prompt + ' ' + synthetic_answer\n",
    "        \n",
    "        # Update the reference to the synthetic answer\n",
    "        new_example['reference'] = synthetic_answer\n",
    "        \n",
    "        # If original data has structured fields, preserve them and update answers\n",
    "        if 'answers' in example:\n",
    "            if synthetic_answer != \"No answer\":\n",
    "                # Try to find answer in context if context exists\n",
    "                context = example.get('context', '')\n",
    "                answer_start = context.find(synthetic_answer) if context else 0\n",
    "                if answer_start == -1:\n",
    "                    answer_start = 0\n",
    "                \n",
    "                new_example['answers'] = {\n",
    "                    'text': [synthetic_answer],\n",
    "                    'answer_start': [answer_start]\n",
    "                }\n",
    "            else:\n",
    "                # No answer case - follow SQuAD v2 format\n",
    "                new_example['answers'] = {\n",
    "                    'text': [],\n",
    "                    'answer_start': []\n",
    "                }\n",
    "        \n",
    "        # Add generation metadata\n",
    "        new_example['generation_num'] = generation_num\n",
    "        new_example['synthetic'] = True\n",
    "        \n",
    "        synthetic_data.append(new_example)\n",
    "        \n",
    "        # Update progress bar with statistics\n",
    "        success_rate = (successful_generations / (idx + 1)) * 100\n",
    "        progress_bar.set_postfix({\n",
    "            'Success': f'{successful_generations}/{idx + 1}',\n",
    "            'Rate': f'{success_rate:.1f}%',\n",
    "            'Failed': failed_generations,\n",
    "            'No Answer': failed_generations\n",
    "        })\n",
    "        \n",
    "        # Update description every 100 examples\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            progress_bar.set_description(\n",
    "                f\"ðŸ¤– Gen {generation_num} - Generated {idx + 1}/{total_examples}\"\n",
    "            )\n",
    "    \n",
    "    # Close progress bar\n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f\"âœ… Generation {generation_num} completed:\")\n",
    "    print(f\"   ðŸ“Š Total examples processed: {total_examples}\")\n",
    "    print(f\"   âœ… Successful generations: {successful_generations}\")\n",
    "    print(f\"   âŒ No answer cases: {failed_generations}\")\n",
    "    print(f\"   ðŸ“ˆ Success rate: {(successful_generations/total_examples)*100:.1f}%\")\n",
    "    print(f\"   ðŸ“ˆ No answer rate: {(failed_generations/total_examples)*100:.1f}%\")\n",
    "    \n",
    "    # Create a new formatted dataset with the synthetic data\n",
    "    print(\"ðŸ“¦ Creating synthetic dataset...\")\n",
    "    with tqdm(total=1, desc=\"ðŸ“¦ Building Dataset\", position=1, leave=False) as dataset_pbar:\n",
    "        synthetic_dataset = Dataset.from_list(synthetic_data)\n",
    "        dataset_pbar.update(1)\n",
    "    \n",
    "    # Return in the same format as input\n",
    "    return {\n",
    "        'train': synthetic_dataset\n",
    "    }\n",
    "\n",
    "\n",
    "def save_synthetic_dataset(dataset_dir, synthetic_formatted_dataset, generation_num):\n",
    "    \"\"\"\n",
    "    Save the synthetic formatted dataset to a new subdirectory.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Base dataset directory\n",
    "        synthetic_formatted_dataset: Formatted dataset with synthetic answers\n",
    "        generation_num: Generation number\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create new subdirectory\n",
    "    new_dir = os.path.join(dataset_dir, f\"generation_{generation_num}\")\n",
    "    os.makedirs(new_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the train split\n",
    "    synthetic_formatted_dataset['train'].save_to_disk(os.path.join(new_dir, \"train\"))\n",
    "    \n",
    "    # Also save as JSON for inspection\n",
    "    json_file = os.path.join(new_dir, \"synthetic_data.json\")\n",
    "    synthetic_formatted_dataset['train'].to_json(json_file)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"generation_number\": generation_num,\n",
    "        \"total_examples\": len(synthetic_formatted_dataset['train']),\n",
    "        \"generated_from\": \"fine_tuned_model\",\n",
    "        \"description\": f\"Synthetic answers generated using fine-tuned model (Generation {generation_num})\",\n",
    "        \"format\": \"formatted_dataset_with_prompts\"\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(new_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved synthetic formatted dataset to {new_dir}\")\n",
    "    return new_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d47390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_nick(model_path):\n",
    "    # Extract the part after the first \"/\"\n",
    "    model_name = model_path.split(\"/\")[1]\n",
    "    \n",
    "    # Match common patterns like \"phi-3\" or \"Mistral-7B\"\n",
    "    match = re.match(r\"([A-Za-z0-9\\-]+?)(?=-\\d|-[a-zA-Z])\", model_name)\n",
    "    \n",
    "    return match.group(1) if match else model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241ecd9",
   "metadata": {},
   "source": [
    "## Iterative training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58d50fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_training_and_generation(\n",
    "    base_dataset_dir=\"squad_v2_01percent\",\n",
    "    model_path=\"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    num_generations=3,\n",
    "    start_generation=1,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform iterative training and synthetic data generation.\n",
    "    \n",
    "    Args:\n",
    "        base_dataset_dir: Directory containing the original dataset\n",
    "        model_path: Base model path or fine-tuned model path\n",
    "        num_generations: Number of generations to create\n",
    "        start_generation: Starting generation number (1 for base model)\n",
    "        device: Device for inference\n",
    "    \"\"\"\n",
    "    \n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    # Determine if starting from base model or fine-tuned model\n",
    "    is_base_model = start_generation == 1\n",
    "    \n",
    "    # Load tokenizer once at the beginning\n",
    "    if is_base_model:\n",
    "        base_model_name = model_path\n",
    "        model_nick = extract_model_nick(base_model_name)\n",
    "    else:\n",
    "        # For fine-tuned models, extract base model name for tokenizer\n",
    "        # Assume model_path format: \"./phi3-squad2-gen{X}-final\"\n",
    "        base_model_name = \"microsoft/Phi-3-mini-128k-instruct\"  # Default fallback\n",
    "        model_nick = extract_model_nick(base_model_name)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"Starting iterative training and generation for {num_generations} generations...\")\n",
    "    \n",
    "    # Used to load dataset\n",
    "    dataset = None\n",
    "    formatted_dataset = None\n",
    "    \n",
    "    # Main progress bar for generations\n",
    "    generation_progress = tqdm(\n",
    "        range(start_generation, start_generation + num_generations), \n",
    "        desc=\"ðŸ”„ Overall Progress\", \n",
    "        unit=\"generation\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    for generation in generation_progress:\n",
    "        torch.cuda.empty_cache()\n",
    "        generation_progress.set_description(f\"ðŸ”„ Generation {generation}/{start_generation+num_generations-1}\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"GENERATION {generation}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Step 1: Fine-tune model with current training dataset\n",
    "        print(\"Step 1: Fine-tuning model...\")\n",
    "        \n",
    "        # Prepare training arguments for this generation\n",
    "        train_config = {\n",
    "            \"bf16\": True,\n",
    "            \"do_eval\": False,  # Disable evaluation completely\n",
    "            \"learning_rate\": 1.0e-05,\n",
    "            \"log_level\": \"info\",\n",
    "            \"logging_steps\": 10,\n",
    "            \"logging_strategy\": \"steps\",\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"num_train_epochs\": 3,\n",
    "            \"max_steps\": -1,\n",
    "            \"output_dir\": f\"./phi3-squad2-gen{generation}\",  # Update for each generation\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"per_device_train_batch_size\": 4,\n",
    "            \"remove_unused_columns\": True,\n",
    "            \"save_steps\": 50,\n",
    "            \"save_total_limit\": 2,\n",
    "            \"seed\": 42,\n",
    "            \"gradient_checkpointing\": True,\n",
    "            \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "            \"warmup_ratio\": 0.05,\n",
    "            \"save_strategy\": \"steps\",\n",
    "            \"load_best_model_at_end\": False,  # No evaluation, so no \"best\" model\n",
    "            \"disable_tqdm\": False,  # Enable tqdm progress bars for training\n",
    "        }\n",
    "\n",
    "        train_args = TrainingArguments(**train_config)\n",
    "\n",
    "        offload_cache_dir = \"./offload_cache\"\n",
    "        os.makedirs(offload_cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Load model for this generation\n",
    "        print(\"ðŸ“¥ Loading model...\")\n",
    "        with tqdm(total=1, desc=\"ðŸ¤– Model Loading\", position=1, leave=False) as model_pbar:\n",
    "            if generation == start_generation and is_base_model:\n",
    "                # First generation: load base model with quantization\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    base_model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    quantization_config=bnb_config\n",
    "                )\n",
    "                # Prepare dataset for first generation using original function\n",
    "                dataset = load_squad_subset(base_dataset_dir)\n",
    "                formatted_dataset = {\n",
    "                    split: dataset[split].map(make_prompt)\n",
    "                    for split in dataset.keys()\n",
    "                }\n",
    "            elif generation == start_generation and not is_base_model:\n",
    "                # Starting from fine-tuned model: load without quantization\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16,        # Use float16 to save memory\n",
    "                    low_cpu_mem_usage=True           # Enable low CPU memory usage\n",
    "                )\n",
    "                # Prepare dataset based on start generation\n",
    "                if start_generation == 1:\n",
    "                    dataset = load_squad_subset(base_dataset_dir)\n",
    "                    formatted_dataset = {\n",
    "                        split: dataset[split].map(make_prompt)\n",
    "                        for split in dataset.keys()\n",
    "                    }\n",
    "                else:\n",
    "                    # Use the dedicated function to load synthetic data\n",
    "                    synthetic_data_dir = os.path.join(base_dataset_dir, f\"generation_{start_generation - 1}\", \"train\")\n",
    "                    dataset = load_synthetic_train(base_dataset_dir, synthetic_data_dir)\n",
    "                    formatted_dataset = dataset  # Already formatted\n",
    "                            \n",
    "            else:\n",
    "                # Subsequent generations: load previous model - FIXED VERSION\n",
    "                previous_model_path = f\"./phi3-squad2-gen{generation-1}-final\"\n",
    "                \n",
    "                # Load the model without offloading to avoid dispatch issues\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    previous_model_path,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "                \n",
    "                # Manually move to device after loading\n",
    "                if torch.cuda.is_available():\n",
    "                    model = model.to(\"cuda\")\n",
    "                \n",
    "                # Use the dedicated function to load synthetic data\n",
    "                synthetic_data_dir = os.path.join(base_dataset_dir, f\"generation_{generation - 1}\", \"train\")\n",
    "                dataset = load_synthetic_train(base_dataset_dir, synthetic_data_dir)\n",
    "                formatted_dataset = dataset  # Already formatted\n",
    "\n",
    "            model_pbar.update(1)\n",
    "        \n",
    "        # Apply PEFT configuration\n",
    "        print(\"ðŸ”§ Applying PEFT configuration...\")\n",
    "        with tqdm(total=1, desc=\"âš™ï¸ PEFT Setup\", position=1, leave=False) as peft_pbar:\n",
    "            peft_config = {\n",
    "                \"r\": 8,  # Reduced from 16 to 8 (fewer parameters)\n",
    "                \"lora_alpha\": 16,  # Reduced from 32 to 16\n",
    "                \"lora_dropout\": 0.1,  # Slightly increased dropout\n",
    "                \"bias\": \"none\",\n",
    "                \"task_type\": \"CAUSAL_LM\",\n",
    "                \"target_modules\": \"all-linear\",\n",
    "                \"modules_to_save\": None,\n",
    "            }\n",
    "            lora_config = LoraConfig(**peft_config)\n",
    "            model = get_peft_model(model, lora_config)\n",
    "            peft_pbar.update(1)\n",
    "        \n",
    "        # Define tokenization function based on data structure\n",
    "        def tokenize(example):\n",
    "            # Check if example has 'prompt' key (formatted data) or needs to be created\n",
    "            if 'prompt' in example:\n",
    "                text_to_tokenize = example[\"prompt\"]\n",
    "            else:\n",
    "                # Create prompt from raw data using make_prompt logic\n",
    "                context = example[\"context\"]\n",
    "                question = example[\"question\"]\n",
    "                \n",
    "                # Handle different answer formats\n",
    "                if \"answers\" in example:\n",
    "                    answers = example[\"answers\"]\n",
    "                    # Check if answers is a dict with 'text' key (original format)\n",
    "                    if isinstance(answers, dict) and \"text\" in answers:\n",
    "                        answer = answers[\"text\"][0] if answers[\"text\"] else \"No answer\"\n",
    "                    # Check if answers is a list (some synthetic data format)\n",
    "                    elif isinstance(answers, list) and len(answers) > 0:\n",
    "                        # If it's a list of strings\n",
    "                        if isinstance(answers[0], str):\n",
    "                            answer = answers[0]\n",
    "                        # If it's a list of dicts with 'text' key\n",
    "                        elif isinstance(answers[0], dict) and \"text\" in answers[0]:\n",
    "                            answer = answers[0][\"text\"]\n",
    "                        else:\n",
    "                            answer = \"No answer\"\n",
    "                    else:\n",
    "                        answer = \"No answer\"\n",
    "                else:\n",
    "                    # Fallback: check if there's a 'reference' field\n",
    "                    answer = example.get(\"reference\", \"No answer\")\n",
    "                \n",
    "                text_to_tokenize = f\"[INST] Given the context, answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question} [/INST] Answer: {answer}\"\n",
    "            \n",
    "            return tokenizer(\n",
    "                text_to_tokenize,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512\n",
    "            )\n",
    "        \n",
    "        # Tokenize current training dataset\n",
    "        print(\"ðŸ”¤ Tokenizing dataset...\")\n",
    "        print(f\"ðŸ” Checking dataset structure...\")\n",
    "        \n",
    "        # Check the structure of the first example to debug\n",
    "        sample_example = formatted_dataset['train'][0]\n",
    "        print(f\"ðŸ“‹ Available columns in training data: {list(sample_example.keys())}\")\n",
    "        \n",
    "        # Debug: Check the structure of answers field\n",
    "        if 'answers' in sample_example:\n",
    "            print(f\"ðŸ” Answers field type: {type(sample_example['answers'])}\")\n",
    "            print(f\"ðŸ” Answers field content: {sample_example['answers']}\")\n",
    "        \n",
    "        if generation == 1 or (generation == start_generation and is_base_model):\n",
    "            # For first generation or starting from base model, tokenize normally\n",
    "            tokenized = {\n",
    "                split: formatted_dataset[split].map(tokenize, batched=False)\n",
    "                for split in formatted_dataset.keys()\n",
    "            }\n",
    "        else:\n",
    "            # For subsequent generations, data is already formatted with prompts\n",
    "            tokenized = {\n",
    "                split: formatted_dataset[split].map(tokenize, batched=False)\n",
    "                for split in formatted_dataset.keys()\n",
    "            }\n",
    "        \n",
    "        # Fine-tune the model\n",
    "        print(\"ðŸš€ Starting training...\")\n",
    "        trainer = train_model(model, tokenized, tokenizer, train_args)\n",
    "        \n",
    "        # Save the fine-tuned model for this generation\n",
    "        print(\"ðŸ’¾ Saving model...\")\n",
    "        with tqdm(total=1, desc=\"ðŸ’¾ Saving Model\", position=1, leave=False) as save_pbar:\n",
    "            final_model_path = f\"./phi3-squad2-gen{generation}-final\"\n",
    "            trainer.save_model(final_model_path)\n",
    "            save_pbar.update(1)\n",
    "        print(f\"âœ… Generation {generation} model saved to {final_model_path}\")\n",
    "        \n",
    "        # Step 2: Generate synthetic answers using the fine-tuned model\n",
    "        print(\"Step 2: Generating synthetic answers...\")\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # For synthetic generation, ensure we have proper formatted dataset structure\n",
    "        if generation == 1 or (generation == start_generation and is_base_model):\n",
    "            generation_dataset = formatted_dataset\n",
    "        else:\n",
    "            # Ensure synthetic data has the right format for generation\n",
    "            # If it doesn't have 'prompt' column, create it\n",
    "            if 'prompt' not in formatted_dataset['train'].column_names:\n",
    "                print(\"ðŸ”§ Reformatting synthetic data for generation...\")\n",
    "                formatted_dataset['train'] = formatted_dataset['train'].map(make_prompt)\n",
    "            generation_dataset = formatted_dataset\n",
    "        \n",
    "        synthetic_dataset = generate_synthetic_answers(\n",
    "            model, tokenizer, generation_dataset, device, generation\n",
    "        )\n",
    "        \n",
    "        # Step 3: Save synthetic dataset\n",
    "        print(\"Step 3: Saving synthetic dataset...\")\n",
    "        with tqdm(total=1, desc=\"ðŸ’¾ Saving Dataset\", position=1, leave=False) as dataset_save_pbar:\n",
    "            new_dir = save_synthetic_dataset(base_dataset_dir, synthetic_dataset, generation)\n",
    "            dataset_save_pbar.update(1)\n",
    "        \n",
    "        print(f\"Generation {generation} completed!\")\n",
    "        print(f\"Synthetic dataset saved to: {new_dir}\")\n",
    "        print(f\"Model saved to: {final_model_path}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    generation_progress.close()\n",
    "    print(f\"\\nðŸŽ‰ All {num_generations} generations completed!\")\n",
    "    print(\"Final models and datasets are ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3216d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iterative training and generation for 3 generations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Generation 2/4:   0%|          | 0/3 [00:00<?, ?generation/s]  PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GENERATION 2\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n",
      "ðŸ“¥ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b41330d2a314da4afaa4cf80ee9be26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from squad_v2_01percent\\generation_1\\train...\n",
      "ðŸ”§ Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Tokenizing dataset...\n",
      "ðŸ” Checking dataset structure...\n",
      "ðŸ“‹ Available columns in training data: ['id', 'title', 'context', 'question', 'answers']\n",
      "ðŸ” Answers field type: <class 'dict'>\n",
      "ðŸ” Answers field content: {'text': ['20th'], 'answer_start': [667]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b476be1c6e14c11bb487b1464bab54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting training...\n",
      "âœ… Set use_cache=False for gradient checkpointing compatibility\n",
      "âœ… Enabled gradient checkpointing for memory efficiency\n",
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 130\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 51\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 05:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.917900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.870800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.802900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen2\\checkpoint-50\n",
      "Saving model checkpoint to ./phi3-squad2-gen2\\checkpoint-51\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Training loss improved to 3.7299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Re-enabled use_cache for inference\n",
      "âœ… Model saved to ./phi3-squad2-final\n",
      "\n",
      "ðŸ§¹ Cleaning up checkpoints...\n",
      "ðŸ—‘ï¸ Removing checkpoint: ./phi3-squad2-gen2\\checkpoint-50\n",
      "ðŸ—‘ï¸ Removing checkpoint: ./phi3-squad2-gen2\\checkpoint-51\n",
      "âœ… Checkpoint cleanup completed!\n",
      "âœ… Checkpoint cleanup completed!\n",
      "ðŸŽ‰ Training completed and checkpoints cleaned up!\n",
      "ðŸ’¾ Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generation 2 model saved to ./phi3-squad2-gen2-final\n",
      "Step 2: Generating synthetic answers...\n",
      "ðŸ”§ Reformatting synthetic data for generation...\n",
      "Generating synthetic answers (Generation 2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generation 2 completed:\n",
      "   ðŸ“Š Total examples processed: 130\n",
      "   âœ… Successful generations: 36\n",
      "   âŒ No answer cases: 94\n",
      "   ðŸ“ˆ Success rate: 27.7%\n",
      "   ðŸ“ˆ No answer rate: 72.3%\n",
      "ðŸ“¦ Creating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Saving synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605371d42f204ac29ab6cffadcf01577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531f5c9bbb3240cdadc0b401a2628033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Generation 3/4:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [09:36<19:13, 576.63s/generation]PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic formatted dataset to squad_v2_01percent\\generation_2\n",
      "Generation 2 completed!\n",
      "Synthetic dataset saved to: squad_v2_01percent\\generation_2\n",
      "Model saved to: ./phi3-squad2-gen2-final\n",
      "\n",
      "==================================================\n",
      "GENERATION 3\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n",
      "ðŸ“¥ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a9c083a5b246d895259ee28373eba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from squad_v2_01percent\\generation_2\\train...\n",
      "ðŸ”§ Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Tokenizing dataset...\n",
      "ðŸ” Checking dataset structure...\n",
      "ðŸ“‹ Available columns in training data: ['id', 'title', 'context', 'question', 'answers']\n",
      "ðŸ” Answers field type: <class 'dict'>\n",
      "ðŸ” Answers field content: {'text': ['20th'], 'answer_start': [667]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c67fe8a3a54e73883aad76a34f0b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2767151a4d54729a989ed7f20839142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting training...\n",
      "âœ… Set use_cache=False for gradient checkpointing compatibility\n",
      "âœ… Enabled gradient checkpointing for memory efficiency\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94481e387f8f4c01b08f56f7b9242fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 130\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 51\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 06:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.918400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.797700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-50\n",
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-51\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Training loss improved to 3.7264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Re-enabled use_cache for inference\n",
      "âœ… Model saved to ./phi3-squad2-final\n",
      "\n",
      "ðŸ§¹ Cleaning up checkpoints...\n",
      "ðŸ—‘ï¸ Removing checkpoint: ./phi3-squad2-gen3\\checkpoint-50\n",
      "ðŸ—‘ï¸ Removing checkpoint: ./phi3-squad2-gen3\\checkpoint-51\n",
      "âœ… Checkpoint cleanup completed!\n",
      "âœ… Checkpoint cleanup completed!\n",
      "ðŸŽ‰ Training completed and checkpoints cleaned up!\n",
      "ðŸ’¾ Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen3-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generation 3 model saved to ./phi3-squad2-gen3-final\n",
      "Step 2: Generating synthetic answers...\n",
      "ðŸ”§ Reformatting synthetic data for generation...\n",
      "Generating synthetic answers (Generation 3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generation 3 completed:\n",
      "   ðŸ“Š Total examples processed: 130\n",
      "   âœ… Successful generations: 35\n",
      "   âŒ No answer cases: 95\n",
      "   ðŸ“ˆ Success rate: 26.9%\n",
      "   ðŸ“ˆ No answer rate: 73.1%\n",
      "ðŸ“¦ Creating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Saving synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f148608f03c446b69717cbe727835b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b596d5ee731e49c2a23fae641c12e7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Generation 4/4:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [20:29<10:21, 621.41s/generation]PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic formatted dataset to squad_v2_01percent\\generation_3\n",
      "Generation 3 completed!\n",
      "Synthetic dataset saved to: squad_v2_01percent\\generation_3\n",
      "Model saved to: ./phi3-squad2-gen3-final\n",
      "\n",
      "==================================================\n",
      "GENERATION 4\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n",
      "ðŸ“¥ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf0722db35a4347b3f8c20c57df5eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from squad_v2_01percent\\generation_3\\train...\n",
      "ðŸ”§ Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Tokenizing dataset...\n",
      "ðŸ” Checking dataset structure...\n",
      "ðŸ“‹ Available columns in training data: ['id', 'title', 'context', 'question', 'answers']\n",
      "ðŸ” Answers field type: <class 'dict'>\n",
      "ðŸ” Answers field content: {'text': ['20th'], 'answer_start': [667]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cabd6d5d5f040498af15aa220dd43f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting training...\n",
      "âœ… Set use_cache=False for gradient checkpointing compatibility\n",
      "âœ… Enabled gradient checkpointing for memory efficiency\n",
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 130\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 51\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 05:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.865300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.797300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-50\n",
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-51\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Training loss improved to 3.7257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Re-enabled use_cache for inference\n",
      "âœ… Model saved to ./phi3-squad2-final\n",
      "\n",
      "ðŸ§¹ Cleaning up checkpoints...\n",
      "ðŸ—‘ï¸ Removing checkpoint: ./phi3-squad2-gen4\\checkpoint-50\n",
      "ðŸ—‘ï¸ Removing checkpoint: ./phi3-squad2-gen4\\checkpoint-51\n",
      "âœ… Checkpoint cleanup completed!\n",
      "âœ… Checkpoint cleanup completed!\n",
      "ðŸŽ‰ Training completed and checkpoints cleaned up!\n",
      "ðŸ’¾ Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen4-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generation 4 model saved to ./phi3-squad2-gen4-final\n",
      "Step 2: Generating synthetic answers...\n",
      "ðŸ”§ Reformatting synthetic data for generation...\n",
      "Generating synthetic answers (Generation 4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generation 4 completed:\n",
      "   ðŸ“Š Total examples processed: 130\n",
      "   âœ… Successful generations: 36\n",
      "   âŒ No answer cases: 94\n",
      "   ðŸ“ˆ Success rate: 27.7%\n",
      "   ðŸ“ˆ No answer rate: 72.3%\n",
      "ðŸ“¦ Creating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Saving synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c864a6f11d0e4d089dccb83e76ecc3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660040168b9c43b7b86c25a7129f3b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Generation 4/4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [29:58<00:00, 599.36s/generation]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic formatted dataset to squad_v2_01percent\\generation_4\n",
      "Generation 4 completed!\n",
      "Synthetic dataset saved to: squad_v2_01percent\\generation_4\n",
      "Model saved to: ./phi3-squad2-gen4-final\n",
      "\n",
      "ðŸŽ‰ All 3 generations completed!\n",
      "Final models and datasets are ready for use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iterative_training_and_generation(\n",
    "        base_dataset_dir=\"squad_v2_01percent\",\n",
    "        model_path=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "        start_generation=2,\n",
    "        num_generations=3,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a997995",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26a9259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", num_examples=None):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation on test set with minimal console output.\n",
    "    All detailed analysis is written to files instead of console.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare test data using make_prompt function\n",
    "    test_prompts = dataset.map(make_prompt)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists for predictions and references\n",
    "    preds = []\n",
    "    refs = []\n",
    "    raw_outputs = []\n",
    "    prompts_list = []\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    \n",
    "    # Limit examples if specified\n",
    "    if num_examples:\n",
    "        test_prompts = test_prompts.select(range(min(num_examples, len(test_prompts))))\n",
    "    \n",
    "    # Generate predictions with progress bar\n",
    "    for example in tqdm(test_prompts, desc=\"ðŸ” Evaluating\", leave=False):\n",
    "        # Get prompt without answer (remove answer part from make_prompt output)\n",
    "        full_prompt = example[\"prompt\"]\n",
    "        if '[/INST]' in full_prompt:\n",
    "            prompt_without_answer = full_prompt.split('[/INST]')[0] + '[/INST] Answer:'\n",
    "        else:\n",
    "            prompt_without_answer = full_prompt\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            prompt_without_answer,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=50,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer - look for content after [/INST] or after \"Answer:\" if present\n",
    "        if '[/INST]' in decoded:\n",
    "            answer_part = decoded.split('[/INST]')[-1].strip()\n",
    "            \n",
    "            # If there's an \"Answer:\" pattern in the generated text, extract after it\n",
    "            if \"Answer:\" in answer_part:\n",
    "                answer = answer_part.split(\"Answer:\")[-1].strip()\n",
    "            else:\n",
    "                answer = answer_part\n",
    "        else:\n",
    "            # Fallback: look for \"Answer:\" pattern in the entire decoded text\n",
    "            if \"Answer:\" in decoded:\n",
    "                answer = decoded.split(\"Answer:\")[-1].strip()\n",
    "            else:\n",
    "                answer = decoded.strip()\n",
    "        \n",
    "        # Clean up answer - remove any trailing instruction markers or unwanted text\n",
    "        answer = answer.split('\\n')[0].strip()  # Take only first line to avoid multi-line responses\n",
    "        \n",
    "        # Store results\n",
    "        preds.append(answer)\n",
    "        refs.append(example[\"reference\"])\n",
    "        raw_outputs.append(decoded)\n",
    "        prompts_list.append(example[\"prompt\"])\n",
    "        \n",
    "        # Extract question and context for detailed analysis\n",
    "        if \"Question:\" in example[\"prompt\"]:\n",
    "            question_part = example[\"prompt\"].split(\"Question:\")[-1].split(\"[/INST]\")[0].strip()\n",
    "            questions.append(question_part)\n",
    "        else:\n",
    "            questions.append(\"\")\n",
    "            \n",
    "        if \"Context:\" in example[\"prompt\"]:\n",
    "            context_part = example[\"prompt\"].split(\"Context:\")[-1].split(\"Question:\")[0].strip()\n",
    "            contexts.append(context_part[:200] + \"...\" if len(context_part) > 200 else context_part)\n",
    "        else:\n",
    "            contexts.append(\"\")\n",
    "    \n",
    "    # Compute BERTScore using silent function\n",
    "    try:\n",
    "        P, R, F1 = silent_bert_score(preds, refs, lang=\"en\")\n",
    "        bert_scores = {\n",
    "            \"precision\": P.mean().item(),\n",
    "            \"recall\": R.mean().item(),\n",
    "            \"f1\": F1.mean().item()\n",
    "        }\n",
    "        bert_f1_scores = [f.item() if hasattr(f, 'item') else f for f in F1]\n",
    "    except Exception as e:\n",
    "        bert_scores = {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "        bert_f1_scores = [0.0] * len(preds)\n",
    "    \n",
    "    # Compute exact match accuracy - improved logic for \"No answer\" cases\n",
    "    exact_matches = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_clean = pred.lower().strip()\n",
    "        ref_clean = ref.lower().strip()\n",
    "        \n",
    "        # Handle \"No answer\" cases\n",
    "        if ref_clean == \"no answer\":\n",
    "            # For \"no answer\" references, check if prediction indicates no answer\n",
    "            no_answer_indicators = [\"no answer\", \"cannot answer\", \"not provided\", \"no information\", \"unknown\"]\n",
    "            match = any(indicator in pred_clean for indicator in no_answer_indicators)\n",
    "            exact_matches.append(1 if match else 0)\n",
    "        else:\n",
    "            # For regular answers, check if reference is contained in prediction\n",
    "            match = ref_clean in pred_clean or pred_clean in ref_clean\n",
    "            exact_matches.append(1 if match else 0)\n",
    "    \n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    \n",
    "    # Compute F1 score (token overlap)\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = set(pred.lower().split())\n",
    "        ref_tokens = set(ref.lower().split())\n",
    "        \n",
    "        if len(pred_tokens) == 0 and len(ref_tokens) == 0:\n",
    "            f1_scores.append(1.0)\n",
    "        elif len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "        else:\n",
    "            common = len(pred_tokens & ref_tokens)\n",
    "            precision = common / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "            recall = common / len(ref_tokens) if len(ref_tokens) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            f1_scores.append(f1)\n",
    "    \n",
    "    f1_score = np.mean(f1_scores)\n",
    "    \n",
    "    # Compute semantic similarity (simple word overlap)\n",
    "    semantic_similarities = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_words = set(pred.lower().split())\n",
    "        ref_words = set(ref.lower().split())\n",
    "        \n",
    "        if len(pred_words) == 0 and len(ref_words) == 0:\n",
    "            semantic_similarities.append(1.0)\n",
    "        elif len(pred_words | ref_words) == 0:\n",
    "            semantic_similarities.append(0.0)\n",
    "        else:\n",
    "            jaccard = len(pred_words & ref_words) / len(pred_words | ref_words)\n",
    "            semantic_similarities.append(jaccard)\n",
    "    \n",
    "    semantic_similarity = np.mean(semantic_similarities)\n",
    "    \n",
    "    # Compute answer length statistics\n",
    "    pred_lengths = [len(pred.split()) for pred in preds]\n",
    "    ref_lengths = [len(ref.split()) for ref in refs]\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        \"test_size\": len(preds),\n",
    "        \"exact_match\": exact_match_score,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"bert_score_f1\": bert_scores[\"f1\"],\n",
    "        \"semantic_similarity\": semantic_similarity,\n",
    "        \"avg_prediction_length\": np.mean(pred_lengths),\n",
    "        \"avg_reference_length\": np.mean(ref_lengths),\n",
    "        \"predictions\": preds,\n",
    "        \"references\": refs,\n",
    "        \"questions\": questions,\n",
    "        \"contexts\": contexts,\n",
    "        \"individual_scores\": {\n",
    "            \"bert_f1\": bert_f1_scores,\n",
    "            \"token_f1\": f1_scores,\n",
    "            \"exact_match\": exact_matches,\n",
    "            \"semantic_similarity\": semantic_similarities\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Write detailed analysis to file instead of console\n",
    "    with open(\"detailed_evaluation_analysis.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"DETAILED TEST SET EVALUATION ANALYSIS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        # Summary metrics\n",
    "        f.write(\"EVALUATION RESULTS SUMMARY\\n\")\n",
    "        f.write(\"-\"*60 + \"\\n\")\n",
    "        f.write(f\"Test Set Size: {len(preds)}\\n\")\n",
    "        f.write(f\"Exact Match: {exact_match_score:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1_score:.4f}\\n\")\n",
    "        f.write(f\"BERTScore F1: {bert_scores['f1']:.4f}\\n\")\n",
    "        f.write(f\"Semantic Similarity: {semantic_similarity:.4f}\\n\")\n",
    "        f.write(f\"Avg Prediction Length: {np.mean(pred_lengths):.2f} words\\n\")\n",
    "        f.write(f\"Avg Reference Length: {np.mean(ref_lengths):.2f} words\\n\\n\")\n",
    "        \n",
    "        # Example analysis\n",
    "        f.write(\"DETAILED PREDICTION EXAMPLES\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        # Select diverse examples: best, worst, and random\n",
    "        if bert_f1_scores and len(bert_f1_scores) > 0:\n",
    "            sorted_indices = sorted(range(len(bert_f1_scores)), key=lambda i: bert_f1_scores[i], reverse=True)\n",
    "            \n",
    "            best_indices = sorted_indices[:3]  # Top 3\n",
    "            worst_indices = sorted_indices[-3:]  # Bottom 3\n",
    "            random_indices = random.sample(range(len(preds)), min(4, len(preds)))  # Random 4\n",
    "            \n",
    "            example_categories = [\n",
    "                (\"BEST PREDICTIONS\", best_indices),\n",
    "                (\"WORST PREDICTIONS\", worst_indices),\n",
    "                (\"RANDOM PREDICTIONS\", random_indices)\n",
    "            ]\n",
    "            \n",
    "            for category_name, indices in example_categories:\n",
    "                f.write(f\"\\n{category_name}:\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "                \n",
    "                for i, idx in enumerate(indices):\n",
    "                    if idx >= len(preds):\n",
    "                        continue\n",
    "                        \n",
    "                    f.write(f\"\\nExample {i+1} (Index {idx}):\\n\")\n",
    "                    f.write(f\"BERTScore F1: {bert_f1_scores[idx]:.4f}\\n\")\n",
    "                    f.write(f\"Token F1: {f1_scores[idx]:.4f}\\n\")\n",
    "                    f.write(f\"Exact Match: {'âœ“' if exact_matches[idx] else 'âœ—'}\\n\")\n",
    "                    \n",
    "                    if idx < len(questions) and questions[idx]:\n",
    "                        f.write(f\"Question: {questions[idx]}\\n\")\n",
    "                    if idx < len(contexts) and contexts[idx]:\n",
    "                        f.write(f\"Context: {contexts[idx]}\\n\")\n",
    "                    \n",
    "                    f.write(f\"Reference Answer: {refs[idx]}\\n\")\n",
    "                    f.write(f\"Model Prediction: {preds[idx]}\\n\")\n",
    "                    \n",
    "                    # Analysis\n",
    "                    pred_words = len(preds[idx].split())\n",
    "                    ref_words = len(refs[idx].split())\n",
    "                    f.write(f\"Length: Pred={pred_words} words, Ref={ref_words} words\\n\")\n",
    "                    \n",
    "                    # Word overlap analysis\n",
    "                    pred_lower = preds[idx].lower()\n",
    "                    ref_lower = refs[idx].lower()\n",
    "                    common_words = set(pred_lower.split()) & set(ref_lower.split())\n",
    "                    f.write(f\"Common words: {len(common_words)}\\n\")\n",
    "                    \n",
    "                    f.write(\"-\" * 50 + \"\\n\")\n",
    "        \n",
    "        # Full predictions list\n",
    "        f.write(f\"\\n\\nFULL PREDICTIONS LIST\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, (prompt, pred, ref, bert_f1, token_f1, em) in enumerate(zip(\n",
    "            prompts_list, preds, refs, bert_f1_scores, f1_scores, exact_matches\n",
    "        )):\n",
    "            f.write(f\"Example {i+1}:\\n\")\n",
    "            f.write(f\"BERTScore F1: {bert_f1:.4f}\\n\")\n",
    "            f.write(f\"Token F1: {token_f1:.4f}\\n\")\n",
    "            f.write(f\"Exact Match: {'âœ“' if em else 'âœ—'}\\n\")\n",
    "            \n",
    "            if \"Question:\" in prompt:\n",
    "                question = prompt.split(\"Question:\")[-1].split(\"[/INST]\")[0].strip()\n",
    "                f.write(f\"Question: {question}\\n\")\n",
    "            \n",
    "            f.write(f\"Reference: {ref}\\n\")\n",
    "            f.write(f\"Prediction: {pred}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d23f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation(base_model_name, generation_num, base_dataset_dir, device, \n",
    "                       save_results=True, results_dir=\"evaluation_results_squad01\", show_examples=True, num_examples=5):\n",
    "    \"\"\"\n",
    "    Evaluate a specific generation model on the test set with optional result saving.\n",
    "    Loads the model and tokenizer internally based on generation number.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Base model name (e.g., \"microsoft/Phi-3-mini-128k-instruct\")\n",
    "        generation_num: Generation number to evaluate\n",
    "        base_dataset_dir: Directory containing the original test dataset\n",
    "        device: Device for inference\n",
    "        save_results: Whether to save results to JSON file\n",
    "        results_dir: Directory to save evaluation results\n",
    "        show_examples: Whether to display example predictions\n",
    "        num_examples: Number of examples to show\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    # Determine if starting from base model or fine-tuned model\n",
    "    is_base_model = generation_num == 0\n",
    "    \n",
    "    # Extract model nickname for path construction\n",
    "    model_nick = extract_model_nick(base_model_name)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"ðŸ“¥ Loading tokenizer...\")\n",
    "    with tqdm(total=1, desc=\"ðŸ”¤ Tokenizer Loading\", position=1, leave=False) as tokenizer_pbar:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer_pbar.update(1)\n",
    "    \n",
    "    # Load model based on generation number\n",
    "    print(\"ðŸ“¥ Loading model...\")\n",
    "    with tqdm(total=1, desc=\"ðŸ¤– Model Loading\", position=1, leave=False) as model_pbar:\n",
    "        if generation_num == 0:\n",
    "            # Load base model\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            # Load fine-tuned model from specific generation\n",
    "            model_path = f\"./phi3-squad2-gen{generation_num}-final\"\n",
    "            # Create offload directory\n",
    "            offload_cache_dir = \"./offload_cache\"\n",
    "            os.makedirs(offload_cache_dir, exist_ok=True)\n",
    "            \n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model for generation {generation_num} not found at {model_path}\")\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                offload_folder=offload_cache_dir,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        model_pbar.update(1)\n",
    "    \n",
    "    # Load original test dataset\n",
    "    with tqdm(total=1, desc=\"ðŸ“‚ Loading Test Data\", position=1, leave=False) as load_pbar:\n",
    "        original_dataset = load_from_disk(base_dataset_dir)\n",
    "        test_dataset = original_dataset['test']\n",
    "        load_pbar.update(1)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Run evaluation\n",
    "    with tqdm(total=1, desc=\"ðŸ“Š Evaluating\", position=1, leave=False) as eval_pbar:\n",
    "        evaluation_results = evaluate_model(model, tokenizer, test_dataset, device)\n",
    "        eval_pbar.update(1)\n",
    "    \n",
    "    # Add metadata to results\n",
    "    evaluation_results.update({\n",
    "        'generation': generation_num,\n",
    "        'test_dataset_size': len(test_dataset),\n",
    "        'base_dataset_dir': base_dataset_dir,\n",
    "        'base_model_name': base_model_name,\n",
    "        'model_nick': model_nick,\n",
    "        'model_path': f\"./phi3-squad2-gen{generation_num}-final\" if generation_num > 0 else base_model_name,\n",
    "    })\n",
    "    \n",
    "    # Create results directory and detailed report file\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    detailed_report_file = os.path.join(results_dir, f\"generation_{generation_num}_detailed_report.txt\")\n",
    "    \n",
    "    # Write all detailed output to file instead of printing\n",
    "    with open(detailed_report_file, 'w', encoding='utf-8') as report_file:\n",
    "        # Redirect detailed output to file\n",
    "        report_file.write(f\"DETAILED EVALUATION REPORT - GENERATION {generation_num}\\n\")\n",
    "        report_file.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        # Basic metrics\n",
    "        report_file.write(f\"ðŸ“Š Generation {generation_num} Evaluation Results:\\n\")\n",
    "        report_file.write(f\"   Exact Match: {evaluation_results['exact_match']:.3f}\\n\")\n",
    "        report_file.write(f\"   F1 Score: {evaluation_results['f1_score']:.3f}\\n\")\n",
    "        report_file.write(f\"   BERTScore F1: {evaluation_results['bert_score_f1']:.3f}\\n\")\n",
    "        report_file.write(f\"   Semantic Similarity: {evaluation_results['semantic_similarity']:.3f}\\n\\n\")\n",
    "    \n",
    "    # Only print brief summary to console\n",
    "    print(f\"ðŸ“Š Generation {generation_num} Evaluation Completed:\")\n",
    "    print(f\"   Exact Match: {evaluation_results['exact_match']:.3f}\")\n",
    "    print(f\"   F1 Score: {evaluation_results['f1_score']:.3f}\")\n",
    "    print(f\"   BERTScore F1: {evaluation_results['bert_score_f1']:.3f}\")\n",
    "    print(f\"   Semantic Similarity: {evaluation_results['semantic_similarity']:.3f}\")\n",
    "    \n",
    "    # Save results if requested\n",
    "    if save_results:\n",
    "        results_file = os.path.join(results_dir, f\"generation_{generation_num}_results.json\")\n",
    "        \n",
    "        # Create a serializable version of results for JSON\n",
    "        json_results = {}\n",
    "        for key, value in evaluation_results.items():\n",
    "            if isinstance(value, (list, dict, str, int, float, bool, type(None))):\n",
    "                json_results[key] = value\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                json_results[key] = value.tolist()\n",
    "            else:\n",
    "                json_results[key] = str(value)\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        \n",
    "        print(f\"ðŸ’¾ Results saved to: {results_file}\")\n",
    "        print(f\"ðŸ“„ Detailed report saved to: {detailed_report_file}\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"ðŸŽ‰ Evaluation completed for Generation {generation_num}!\")\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3eac47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiple_generations(\n",
    "    base_model_name=\"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    base_dataset_dir=\"squad_v2_01percent\",\n",
    "    start_generation=0,\n",
    "    num_generations=4,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_results=True,\n",
    "    results_dir=\"evaluation_results_squad01\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate multiple generations in sequence with progress tracking and result comparison.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Base model name for tokenizer and generation 0\n",
    "        base_dataset_dir: Directory containing the original test dataset\n",
    "        start_generation: Starting generation number (0 for base model)\n",
    "        num_generations: Number of generations to evaluate\n",
    "        device: Device for inference\n",
    "        save_results: Whether to save results to JSON files\n",
    "        results_dir: Directory to save all evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all evaluation results by generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ðŸš€ Starting evaluation of {num_generations} generations...\")\n",
    "    print(f\"ðŸ“‹ Generations: {start_generation} to {start_generation + num_generations - 1}\")\n",
    "    print(f\"ðŸ’¾ Results will be saved to: {results_dir}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Dictionary to store all results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Create results directory\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Main progress bar for all generations\n",
    "    generation_progress = tqdm(\n",
    "        range(start_generation, start_generation + num_generations),\n",
    "        desc=\"ðŸ”„ Evaluating Generations\",\n",
    "        unit=\"generation\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    # Track metrics for comparison\n",
    "    metrics_comparison = {\n",
    "        'generations': [],\n",
    "        'exact_match': [],\n",
    "        'f1_score': [],\n",
    "        'bert_score_f1': [],\n",
    "        'semantic_similarity': []\n",
    "    }\n",
    "    \n",
    "    for generation_num in generation_progress:\n",
    "        generation_progress.set_description(f\"ðŸ”„ Evaluating Generation {generation_num}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUATING GENERATION {generation_num}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Evaluate this generation\n",
    "            results = evaluate_generation(\n",
    "                base_model_name=base_model_name,\n",
    "                generation_num=generation_num,\n",
    "                base_dataset_dir=base_dataset_dir,\n",
    "                device=device,\n",
    "                save_results=save_results,\n",
    "                results_dir=results_dir\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            all_results[f\"generation_{generation_num}\"] = results\n",
    "            \n",
    "            # Track metrics for comparison\n",
    "            metrics_comparison['generations'].append(generation_num)\n",
    "            metrics_comparison['exact_match'].append(results['exact_match'])\n",
    "            metrics_comparison['f1_score'].append(results['f1_score'])\n",
    "            metrics_comparison['bert_score_f1'].append(results['bert_score_f1'])\n",
    "            metrics_comparison['semantic_similarity'].append(results['semantic_similarity'])\n",
    "            \n",
    "            # Update progress bar with current metrics\n",
    "            generation_progress.set_postfix({\n",
    "                'EM': f\"{results['exact_match']:.3f}\",\n",
    "                'F1': f\"{results['f1_score']:.3f}\",\n",
    "                'BERT': f\"{results['bert_score_f1']:.3f}\",\n",
    "                'SIM': f\"{results['semantic_similarity']:.3f}\"\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error evaluating generation {generation_num}: {str(e)}\")\n",
    "            # Store error information\n",
    "            all_results[f\"generation_{generation_num}\"] = {\n",
    "                'error': str(e),\n",
    "                'generation': generation_num,\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            continue\n",
    "    \n",
    "    generation_progress.close()\n",
    "    \n",
    "    # Create comprehensive comparison report\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(f\"{'Generation':<12} {'Exact Match':<12} {'F1 Score':<12} {'BERTScore F1':<14} {'Semantic Sim':<14}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, gen in enumerate(metrics_comparison['generations']):\n",
    "        if f\"generation_{gen}\" in all_results and 'error' not in all_results[f\"generation_{gen}\"]:\n",
    "            print(f\"{gen:<12} {metrics_comparison['exact_match'][i]:<12.3f} \"\n",
    "                  f\"{metrics_comparison['f1_score'][i]:<12.3f} \"\n",
    "                  f\"{metrics_comparison['bert_score_f1'][i]:<14.3f} \"\n",
    "                  f\"{metrics_comparison['semantic_similarity'][i]:<14.3f}\")\n",
    "        else:\n",
    "            print(f\"{gen:<12} {'FAILED':<12} {'FAILED':<12} {'FAILED':<14} {'FAILED':<14}\")\n",
    "    \n",
    "    # Find best performing generation for each metric\n",
    "    if metrics_comparison['generations']:\n",
    "        print(f\"\\nðŸ“ˆ BEST PERFORMING GENERATIONS:\")\n",
    "        if metrics_comparison['exact_match']:\n",
    "            best_em_idx = np.argmax(metrics_comparison['exact_match'])\n",
    "            best_em_gen = metrics_comparison['generations'][best_em_idx]\n",
    "            print(f\"   Exact Match: Generation {best_em_gen} ({metrics_comparison['exact_match'][best_em_idx]:.3f})\")\n",
    "        \n",
    "        if metrics_comparison['f1_score']:\n",
    "            best_f1_idx = np.argmax(metrics_comparison['f1_score'])\n",
    "            best_f1_gen = metrics_comparison['generations'][best_f1_idx]\n",
    "            print(f\"   F1 Score: Generation {best_f1_gen} ({metrics_comparison['f1_score'][best_f1_idx]:.3f})\")\n",
    "        \n",
    "        if metrics_comparison['bert_score_f1']:\n",
    "            best_bert_idx = np.argmax(metrics_comparison['bert_score_f1'])\n",
    "            best_bert_gen = metrics_comparison['generations'][best_bert_idx]\n",
    "            print(f\"   BERTScore F1: Generation {best_bert_gen} ({metrics_comparison['bert_score_f1'][best_bert_idx]:.3f})\")\n",
    "        \n",
    "        if metrics_comparison['semantic_similarity']:\n",
    "            best_sim_idx = np.argmax(metrics_comparison['semantic_similarity'])\n",
    "            best_sim_gen = metrics_comparison['generations'][best_sim_idx]\n",
    "            print(f\"   Semantic Similarity: Generation {best_sim_gen} ({metrics_comparison['semantic_similarity'][best_sim_idx]:.3f})\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    if save_results:\n",
    "        # Save all results in one file\n",
    "        comprehensive_results_file = os.path.join(results_dir, \"comprehensive_evaluation_results.json\")\n",
    "        \n",
    "        # Create serializable version\n",
    "        json_all_results = {}\n",
    "        for gen_key, gen_results in all_results.items():\n",
    "            json_gen_results = {}\n",
    "            for key, value in gen_results.items():\n",
    "                if isinstance(value, (list, dict, str, int, float, bool, type(None))):\n",
    "                    json_gen_results[key] = value\n",
    "                elif isinstance(value, np.ndarray):\n",
    "                    json_gen_results[key] = value.tolist()\n",
    "                else:\n",
    "                    json_gen_results[key] = str(value)\n",
    "            json_all_results[gen_key] = json_gen_results\n",
    "        \n",
    "        # Add comparison metrics\n",
    "        json_all_results['comparison_metrics'] = metrics_comparison\n",
    "        json_all_results['evaluation_metadata'] = {\n",
    "            'base_model_name': base_model_name,\n",
    "            'base_dataset_dir': base_dataset_dir,\n",
    "            'start_generation': start_generation,\n",
    "            'num_generations': num_generations,\n",
    "            'total_generations_evaluated': len(metrics_comparison['generations']),\n",
    "            'evaluation_timestamp': str(pd.Timestamp.now()) if 'pd' in globals() else \"timestamp_not_available\"\n",
    "        }\n",
    "        \n",
    "        with open(comprehensive_results_file, 'w') as f:\n",
    "            json.dump(json_all_results, f, indent=2)\n",
    "        \n",
    "        # Save comparison table as CSV\n",
    "        comparison_csv_file = os.path.join(results_dir, \"generations_comparison.csv\")\n",
    "        comparison_df_data = {\n",
    "            'Generation': metrics_comparison['generations'],\n",
    "            'Exact_Match': metrics_comparison['exact_match'],\n",
    "            'F1_Score': metrics_comparison['f1_score'],\n",
    "            'BERTScore_F1': metrics_comparison['bert_score_f1'],\n",
    "            'Semantic_Similarity': metrics_comparison['semantic_similarity']\n",
    "        }\n",
    "        \n",
    "        # Create simple CSV manually\n",
    "        with open(comparison_csv_file, 'w') as f:\n",
    "            f.write(\"Generation,Exact_Match,F1_Score,BERTScore_F1,Semantic_Similarity\\n\")\n",
    "            for i in range(len(metrics_comparison['generations'])):\n",
    "                f.write(f\"{metrics_comparison['generations'][i]},\"\n",
    "                       f\"{metrics_comparison['exact_match'][i]},\"\n",
    "                       f\"{metrics_comparison['f1_score'][i]},\"\n",
    "                       f\"{metrics_comparison['bert_score_f1'][i]},\"\n",
    "                       f\"{metrics_comparison['semantic_similarity'][i]}\\n\")\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ Comprehensive results saved to:\")\n",
    "        print(f\"   ðŸ“Š {comprehensive_results_file}\")\n",
    "        print(f\"   ðŸ“‹ {comparison_csv_file}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Evaluation of {len(metrics_comparison['generations'])} generations completed!\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f306733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting evaluation of 4 generations...\n",
      "ðŸ“‹ Generations: 0 to 3\n",
      "ðŸ’¾ Results will be saved to: evaluation_results_squad01\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Evaluating Generation 0:   0%|          | 0/4 [00:00<?, ?generation/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING GENERATION 0\n",
      "============================================================\n",
      "ðŸ“¥ Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83b36270aef44a78df19ff7129b4326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[AThe following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, pooler.dense.bias, encoder.layer.*.attention.self.key.weight\n",
      "ðŸ”„ Evaluating Generation 1:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:25<01:17, 25.69s/generation, EM=0.273, F1=0.123, BERT=0.843, SIM=0.075]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generation 0 Evaluation Completed:\n",
      "   Exact Match: 0.273\n",
      "   F1 Score: 0.123\n",
      "   BERTScore F1: 0.843\n",
      "   Semantic Similarity: 0.075\n",
      "ðŸ’¾ Results saved to: evaluation_results_squad01\\generation_0_results.json\n",
      "ðŸ“„ Detailed report saved to: evaluation_results_squad01\\generation_0_detailed_report.txt\n",
      "ðŸŽ‰ Evaluation completed for Generation 0!\n",
      "\n",
      "============================================================\n",
      "EVALUATING GENERATION 1\n",
      "============================================================\n",
      "ðŸ“¥ Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5478ae98f33b4fa2a083721354f37fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[AThe following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, pooler.dense.bias, encoder.layer.*.attention.self.key.weight\n",
      "ðŸ”„ Evaluating Generation 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:53<00:53, 26.80s/generation, EM=0.273, F1=0.138, BERT=0.842, SIM=0.096]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generation 1 Evaluation Completed:\n",
      "   Exact Match: 0.273\n",
      "   F1 Score: 0.138\n",
      "   BERTScore F1: 0.842\n",
      "   Semantic Similarity: 0.096\n",
      "ðŸ’¾ Results saved to: evaluation_results_squad01\\generation_1_results.json\n",
      "ðŸ“„ Detailed report saved to: evaluation_results_squad01\\generation_1_detailed_report.txt\n",
      "ðŸŽ‰ Evaluation completed for Generation 1!\n",
      "\n",
      "============================================================\n",
      "EVALUATING GENERATION 2\n",
      "============================================================\n",
      "ðŸ“¥ Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f341d85eef344168b6cb16f37ec3bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[AThe following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, pooler.dense.bias, encoder.layer.*.attention.self.key.weight\n",
      "ðŸ”„ Evaluating Generation 3:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:21<00:27, 27.58s/generation, EM=0.182, F1=0.141, BERT=0.756, SIM=0.100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generation 2 Evaluation Completed:\n",
      "   Exact Match: 0.182\n",
      "   F1 Score: 0.141\n",
      "   BERTScore F1: 0.756\n",
      "   Semantic Similarity: 0.100\n",
      "ðŸ’¾ Results saved to: evaluation_results_squad01\\generation_2_results.json\n",
      "ðŸ“„ Detailed report saved to: evaluation_results_squad01\\generation_2_detailed_report.txt\n",
      "ðŸŽ‰ Evaluation completed for Generation 2!\n",
      "\n",
      "============================================================\n",
      "EVALUATING GENERATION 3\n",
      "============================================================\n",
      "ðŸ“¥ Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c7a01b70ae445ab2a0cfdb6ded655a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[AThe following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, embeddings.LayerNorm.weight, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.intermediate.dense.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.output.dense.weight, pooler.dense.bias, encoder.layer.*.attention.self.key.weight\n",
      "ðŸ”„ Evaluating Generation 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:15<00:00, 33.87s/generation, EM=0.182, F1=0.141, BERT=0.756, SIM=0.100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generation 3 Evaluation Completed:\n",
      "   Exact Match: 0.182\n",
      "   F1 Score: 0.141\n",
      "   BERTScore F1: 0.756\n",
      "   Semantic Similarity: 0.100\n",
      "ðŸ’¾ Results saved to: evaluation_results_squad01\\generation_3_results.json\n",
      "ðŸ“„ Detailed report saved to: evaluation_results_squad01\\generation_3_detailed_report.txt\n",
      "ðŸŽ‰ Evaluation completed for Generation 3!\n",
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Generation   Exact Match  F1 Score     BERTScore F1   Semantic Sim  \n",
      "----------------------------------------------------------------------\n",
      "0            0.273        0.123        0.843          0.075         \n",
      "1            0.273        0.138        0.842          0.096         \n",
      "2            0.182        0.141        0.756          0.100         \n",
      "3            0.182        0.141        0.756          0.100         \n",
      "\n",
      "ðŸ“ˆ BEST PERFORMING GENERATIONS:\n",
      "   Exact Match: Generation 0 (0.273)\n",
      "   F1 Score: Generation 2 (0.141)\n",
      "   BERTScore F1: Generation 0 (0.843)\n",
      "   Semantic Similarity: Generation 2 (0.100)\n",
      "\n",
      "ðŸ’¾ Comprehensive results saved to:\n",
      "   ðŸ“Š evaluation_results_squad01\\comprehensive_evaluation_results.json\n",
      "   ðŸ“‹ evaluation_results_squad01\\generations_comparison.csv\n",
      "\n",
      "ðŸŽ‰ Evaluation of 4 generations completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_multiple_generations(\n",
    "        base_model_name=\"microsoft/Phi-3-mini-128k-instruct\",\n",
    "        base_dataset_dir=\"squad_v2_01percent\", \n",
    "        start_generation=0,\n",
    "        num_generations=4,  # Will evaluate generations 0, 1, 2, 3\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        save_results=True,\n",
    "        results_dir=\"evaluation_results_squad01\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39842f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
