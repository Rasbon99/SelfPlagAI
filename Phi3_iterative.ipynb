{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a919dc6",
   "metadata": {},
   "source": [
    "# Phi-3 Iterative Try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4babf",
   "metadata": {},
   "source": [
    "This notebook is used to try the iterative pipeline without the integration of MongoDB to store the versioning of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844840d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbc3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from bert_score import score\n",
    "from datasets import Dataset, load_from_disk, DatasetDict\n",
    "from dotenv import load_dotenv\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb166e",
   "metadata": {},
   "source": [
    "### Dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdc10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad_subset(dataset_dir=\"squad_v2_05percent\"):\n",
    "    \"\"\"\n",
    "    Load the saved SQuAD v2 0.5% dataset from disk.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir (str): Directory where the dataset was saved\n",
    "        \n",
    "    Returns:\n",
    "        DatasetDict: The loaded dataset with train and test splits\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{dataset_dir}' not found. Please run the extraction script first.\")\n",
    "    \n",
    "    print(f\"Loading dataset from {dataset_dir}...\")\n",
    "    \n",
    "    # Load the dataset using Hugging Face datasets\n",
    "    dataset = load_from_disk(dataset_dir)\n",
    "    \n",
    "    # Load and display metadata\n",
    "    metadata_path = os.path.join(dataset_dir, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"Dataset metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"\\nLoaded dataset splits:\")\n",
    "    for split_name, split_data in dataset.items():\n",
    "        print(f\"  {split_name}: {len(split_data)} examples\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ce1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_only(dataset_dir):\n",
    "    \"\"\"\n",
    "    Load only the train split from a saved dataset directory.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir (str): Directory where the dataset was saved\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: The loaded train dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{dataset_dir}' not found.\")\n",
    "    \n",
    "    print(f\"Loading train dataset from {dataset_dir}...\")\n",
    "    \n",
    "    # Try to load the full dataset first\n",
    "    try:\n",
    "        full_dataset = load_from_disk(dataset_dir)\n",
    "        \n",
    "        # Check if it's a DatasetDict with train split\n",
    "        if isinstance(full_dataset, dict) and 'train' in full_dataset:\n",
    "            train_dataset = full_dataset['train']\n",
    "        elif hasattr(full_dataset, 'column_names'):\n",
    "            # It's already a single Dataset\n",
    "            train_dataset = full_dataset\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected dataset format in {dataset_dir}\")\n",
    "            \n",
    "    except Exception:\n",
    "        # Try to load from train subdirectory if full dataset fails\n",
    "        train_dir = os.path.join(dataset_dir, \"train\")\n",
    "        if os.path.exists(train_dir):\n",
    "            train_dataset = load_from_disk(train_dir)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No train dataset found in {dataset_dir}\")\n",
    "    \n",
    "    # Load and display metadata if available\n",
    "    metadata_path = os.path.join(dataset_dir, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"Dataset metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Display train dataset info\n",
    "    print(f\"\\nLoaded train dataset:\")\n",
    "    print(f\"  Examples: {len(train_dataset)}\")\n",
    "    print(f\"  Columns: {train_dataset.column_names}\")\n",
    "    \n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fad3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from squad_v2_01percent...\n",
      "Dataset metadata:\n",
      "  original_train_size: 130319\n",
      "  original_validation_size: 11873\n",
      "  extracted_train_size: 130\n",
      "  extracted_test_size: 11\n",
      "  extraction_percentage: 0.1\n",
      "  sampling_method: random\n",
      "  seed: 42\n",
      "  train_answerable: 94\n",
      "  test_answerable: 3\n",
      "  train_answerable_percentage: 72.3076923076923\n",
      "  test_answerable_percentage: 27.27272727272727\n",
      "  dataset_format: squad_v2\n",
      "  splits: ['train', 'test']\n",
      "\n",
      "Loaded dataset splits:\n",
      "  train: 130 examples\n",
      "  test: 11 examples\n",
      "\n",
      "Example from each split:\n",
      "Train: What century did Nasser rule in?\n",
      "Test: How many State of California University campuses are there?\n",
      "\n",
      "Dataset features: {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_squad_subset(\"squad_v2_01percent\")\n",
    "    \n",
    "# Show examples\n",
    "print(f\"\\nExample from each split:\")\n",
    "print(f\"Train: {dataset['train'][0]['question']}\")\n",
    "print(f\"Test: {dataset['test'][0]['question']}\")\n",
    "\n",
    "# Access specific fields\n",
    "print(f\"\\nDataset features: {dataset['train'].features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c67d23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to format the dataset examples into a prompt\n",
    "#The prompt will include the context, question, and answer\n",
    "def make_prompt(example):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"No answer\"\n",
    "\n",
    "    prompt = f\"[INST] Given the context, answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question} [/INST] {answer}\"\n",
    "    return {\"prompt\": prompt, \"reference\": answer}\n",
    "\n",
    "formatted_dataset = {\n",
    "    split: dataset[split].map(make_prompt)\n",
    "    for split in dataset.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5fdf7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " [INST] Given the context, answer the question.\n",
      "\n",
      "Context: Nasser remains an iconic figure in the Arab world, particularly for his strides towards social justice and Arab unity, modernization policies, and anti-imperialist efforts. His presidency also encouraged and coincided with an Egyptian cultural boom, and launched large industrial projects, including the Aswan Dam and Helwan City. Nasser's detractors criticize his authoritarianism, his government's human rights violations, his populist relationship with the citizenry, and his failure to establish civil institutions, blaming his legacy for future dictatorial governance in Egypt. Historians describe Nasser as a towering political figure of the Middle East in the 20th century.\n",
      "\n",
      "Question: What century did Nasser rule in? [/INST] 20th\n",
      "\n",
      "Reference Answer:\n",
      " 20th\n"
     ]
    }
   ],
   "source": [
    "# Print the first formatted prompt and its reference answer\n",
    "print(\"Prompt:\\n\", formatted_dataset[\"train\"][0][\"prompt\"])\n",
    "print(\"\\nReference Answer:\\n\", formatted_dataset[\"train\"][0][\"reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad29c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"key.env\")\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "889ec8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae51a83",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3828e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced logging suppression - including BERTScore sharding messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate.utils.modeling\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f52b2384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERTScore silently...\n",
      "BERTScore initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERTScore silently\n",
    "print(\"Initializing BERTScore silently...\")\n",
    "with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "    from bert_score import score\n",
    "    _ = score([\"test\"], [\"test\"], lang=\"en\", verbose=False)\n",
    "print(\"BERTScore initialized successfully!\")\n",
    "\n",
    "# Modified BERTScore function with complete output suppression\n",
    "def silent_bert_score(cands, refs, lang=\"en\"):\n",
    "    \"\"\"BERTScore calculation with all output suppressed\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    old_stderr = sys.stderr\n",
    "    \n",
    "    sys.stdout = io.StringIO()\n",
    "    sys.stderr = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        P, R, F1 = score(cands, refs, lang=lang, verbose=False)\n",
    "        return P, R, F1\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stderr = old_stderr\n",
    "\n",
    "# Custom Early Stopping based on Training Loss\n",
    "class TrainingLossEarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait_count = 0\n",
    "        \n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        if logs is not None and 'train_loss' in logs:\n",
    "            current_loss = logs['train_loss']\n",
    "            \n",
    "            if current_loss < self.best_loss - self.min_delta:\n",
    "                self.best_loss = current_loss\n",
    "                self.wait_count = 0\n",
    "                print(f\"📈 Training loss improved to {current_loss:.4f}\")\n",
    "            else:\n",
    "                self.wait_count += 1\n",
    "                print(f\"📊 No improvement in training loss ({self.wait_count}/{self.patience})\")\n",
    "                \n",
    "                if self.wait_count >= self.patience:\n",
    "                    print(f\"🛑 Early stopping triggered! Best loss: {self.best_loss:.4f}\")\n",
    "                    control.should_training_stop = True\n",
    "\n",
    "# Fixed Custom Trainer class with BERTScore loss\n",
    "class BERTScoreTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.label_names = [\"labels\"]\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Custom loss function using BERTScore - completely silent\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Temporarily disable cache for forward pass\n",
    "        model.config.use_cache = False\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Generate predictions for BERTScore\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Re-enable cache for generation\n",
    "            model.config.use_cache = True\n",
    "            \n",
    "            # Generate text\n",
    "            try:\n",
    "                generated = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.processing_class.eos_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "                \n",
    "                # Decode predictions and references\n",
    "                pred_texts = self.processing_class.batch_decode(generated, skip_special_tokens=True)\n",
    "                ref_texts = self.processing_class.batch_decode(labels, skip_special_tokens=True)\n",
    "                \n",
    "                # Calculate BERTScore with completely silent function\n",
    "                P, R, F1 = silent_bert_score(pred_texts, ref_texts, lang=\"en\")\n",
    "                bert_f1 = F1.mean().item()\n",
    "                \n",
    "                # Convert BERTScore to loss\n",
    "                bert_loss = torch.tensor(1.0 - bert_f1, requires_grad=True, device=input_ids.device)\n",
    "            except Exception as e:\n",
    "                # Fallback to standard loss if BERTScore fails\n",
    "                bert_loss = outputs.loss\n",
    "            finally:\n",
    "                # Disable cache again for gradient checkpointing compatibility\n",
    "                model.config.use_cache = False\n",
    "        \n",
    "        # Combine with standard language modeling loss\n",
    "        standard_loss = outputs.loss\n",
    "        combined_loss = 0.7 * standard_loss + 0.3 * bert_loss\n",
    "        \n",
    "        return (combined_loss, outputs) if return_outputs else combined_loss\n",
    "\n",
    "# Data preparation function - removed tokenizer parameter since it's not used\n",
    "def prepare_training_data(tokenized_dataset):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    \n",
    "    def add_labels(example):\n",
    "        example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "        return example\n",
    "\n",
    "    # Only prepare train split\n",
    "    train_dataset = tokenized_dataset[\"train\"].map(add_labels)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    keep_keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    train_dataset = train_dataset.remove_columns(\n",
    "        [col for col in train_dataset.column_names if col not in keep_keys]\n",
    "    )\n",
    "    \n",
    "    return {\"train\": train_dataset}\n",
    "\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    \"\"\"Remove checkpoint directories and files\"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        # Find all checkpoint directories\n",
    "        checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith('checkpoint-')]\n",
    "        \n",
    "        for checkpoint_dir in checkpoint_dirs:\n",
    "            checkpoint_path = os.path.join(output_dir, checkpoint_dir)\n",
    "            if os.path.isdir(checkpoint_path):\n",
    "                print(f\"🗑️ Removing checkpoint: {checkpoint_path}\")\n",
    "                shutil.rmtree(checkpoint_path)\n",
    "        \n",
    "        # Remove any other checkpoint-related files\n",
    "        checkpoint_files = [f for f in os.listdir(output_dir) if 'checkpoint' in f.lower()]\n",
    "        for checkpoint_file in checkpoint_files:\n",
    "            file_path = os.path.join(output_dir, checkpoint_file)\n",
    "            if os.path.isfile(file_path):\n",
    "                print(f\"🗑️ Removing checkpoint file: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        print(\"✅ Checkpoint cleanup completed!\")\n",
    "\n",
    "def configure_model_for_training(model):\n",
    "    \"\"\"Configure model for training with proper cache settings\"\"\"\n",
    "    \n",
    "    # Disable use_cache for training compatibility with gradient checkpointing\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = False\n",
    "        print(\"✅ Set use_cache=False for gradient checkpointing compatibility\")\n",
    "    \n",
    "    # Enable gradient checkpointing if available\n",
    "    if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"✅ Enabled gradient checkpointing for memory efficiency\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main training function\n",
    "def train_model(model, tokenized_data, tokenizer, train_args):\n",
    "    \"\"\"Training function with BERTScore and early stopping\"\"\"\n",
    "    \n",
    "    # Configure model for training\n",
    "    model = configure_model_for_training(model)\n",
    "    \n",
    "    # Prepare data\n",
    "    prepared_data = prepare_training_data(tokenized_data)\n",
    "    \n",
    "    # Setup data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Custom early stopping based on training loss\n",
    "    early_stopping_callback = TrainingLossEarlyStoppingCallback(\n",
    "        patience=5,\n",
    "        min_delta=0.01\n",
    "    )\n",
    "    \n",
    "    # Initialize BERTScore Trainer\n",
    "    trainer = BERTScoreTrainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=prepared_data[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[early_stopping_callback],\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training with BERTScore optimization...\")\n",
    "    print(\"Early stopping based on training loss improvement\")\n",
    "    print(\"Cache disabled for gradient checkpointing compatibility\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Re-enable cache for inference after training\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = True\n",
    "        print(\"✅ Re-enabled use_cache for inference\")\n",
    "    \n",
    "    # Save model\n",
    "    final_model_path = \"./phi3-squad2-final\"\n",
    "    trainer.save_model(final_model_path)\n",
    "    print(f\"✅ Model saved to {final_model_path}\")\n",
    "    \n",
    "    # Clean up checkpoints after saving the final model\n",
    "    print(\"\\n🧹 Cleaning up checkpoints...\")\n",
    "    if hasattr(train_args, 'output_dir') and train_args.output_dir:\n",
    "        cleanup_checkpoints(train_args.output_dir)\n",
    "    \n",
    "    # Also clean up from the final model directory if it has checkpoints\n",
    "    cleanup_checkpoints(final_model_path)\n",
    "    \n",
    "    # Clean up any checkpoint directories in the current working directory\n",
    "    current_dir_checkpoints = [d for d in os.listdir('.') if d.startswith('checkpoint-')]\n",
    "    for checkpoint_dir in current_dir_checkpoints:\n",
    "        if os.path.isdir(checkpoint_dir):\n",
    "            print(f\"🗑️ Removing checkpoint: {checkpoint_dir}\")\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    print(\"🎉 Training completed and checkpoints cleaned up!\")\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f992b",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604acdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", num_examples=None):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation on test set with detailed prediction examples\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING TEST SET EVALUATION WITH EXAMPLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare test data using make_prompt function\n",
    "    print(\"Preparing test prompts...\")\n",
    "    test_prompts = dataset.map(make_prompt)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists for predictions and references\n",
    "    preds = []\n",
    "    refs = []\n",
    "    raw_outputs = []\n",
    "    prompts_list = []\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    \n",
    "    # Limit examples if specified\n",
    "    if num_examples:\n",
    "        test_prompts = test_prompts.select(range(min(num_examples, len(test_prompts))))\n",
    "    \n",
    "    print(f\"Generating predictions for {len(test_prompts)} test examples...\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    for example in tqdm(test_prompts, desc=\"Evaluating\"):\n",
    "        # Get prompt without answer (remove answer part from make_prompt output)\n",
    "        full_prompt = example[\"prompt\"]\n",
    "        if '[/INST]' in full_prompt:\n",
    "            prompt_without_answer = full_prompt.split('[/INST]')[0] + '[/INST]'\n",
    "        else:\n",
    "            prompt_without_answer = full_prompt\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            prompt_without_answer,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=50,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer (everything after [/INST])\n",
    "        if '[/INST]' in decoded:\n",
    "            answer = decoded.split('[/INST]')[-1].strip()\n",
    "        else:\n",
    "            answer = decoded.strip()\n",
    "        \n",
    "        # Store results\n",
    "        preds.append(answer)\n",
    "        refs.append(example[\"reference\"])\n",
    "        raw_outputs.append(decoded)\n",
    "        prompts_list.append(example[\"prompt\"])\n",
    "        \n",
    "        # Extract question and context for detailed analysis\n",
    "        if \"Question:\" in example[\"prompt\"]:\n",
    "            question_part = example[\"prompt\"].split(\"Question:\")[-1].split(\"[/INST]\")[0].strip()\n",
    "            questions.append(question_part)\n",
    "        if \"Context:\" in example[\"prompt\"]:\n",
    "            context_part = example[\"prompt\"].split(\"Context:\")[-1].split(\"Question:\")[0].strip()\n",
    "            contexts.append(context_part[:200] + \"...\" if len(context_part) > 200 else context_part)\n",
    "    \n",
    "    print(\"Predictions generated! Computing metrics...\")\n",
    "    \n",
    "    # Compute BERTScore using silent function from your notebook\n",
    "    print(\"Computing BERTScore...\")\n",
    "    try:\n",
    "        P, R, F1 = silent_bert_score(preds, refs, lang=\"en\")\n",
    "        bert_scores = {\n",
    "            \"precision\": P.mean().item(),\n",
    "            \"recall\": R.mean().item(),\n",
    "            \"f1\": F1.mean().item()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"BERTScore computation failed: {e}\")\n",
    "        bert_scores = {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "        P = R = F1 = [0.0] * len(preds)\n",
    "    \n",
    "    # Compute exact match accuracy\n",
    "    exact_matches = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        if ref != \"No answer\" and ref.lower().strip() in pred.lower().strip():\n",
    "            exact_matches.append(1)\n",
    "        else:\n",
    "            exact_matches.append(0)\n",
    "    \n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    \n",
    "    # Compute F1 score (token overlap)\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = set(pred.lower().split())\n",
    "        ref_tokens = set(ref.lower().split())\n",
    "        \n",
    "        if len(pred_tokens) == 0 and len(ref_tokens) == 0:\n",
    "            f1_scores.append(1.0)\n",
    "        elif len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "        else:\n",
    "            common = len(pred_tokens & ref_tokens)\n",
    "            precision = common / len(pred_tokens)\n",
    "            recall = common / len(ref_tokens)\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            f1_scores.append(f1)\n",
    "    \n",
    "    f1_score = np.mean(f1_scores)\n",
    "    \n",
    "    # Compute semantic similarity (simple word overlap)\n",
    "    semantic_similarities = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_words = set(pred.lower().split())\n",
    "        ref_words = set(ref.lower().split())\n",
    "        if len(pred_words) == 0 and len(ref_words) == 0:\n",
    "            semantic_similarities.append(1.0)\n",
    "        elif len(pred_words | ref_words) == 0:\n",
    "            semantic_similarities.append(0.0)\n",
    "        else:\n",
    "            jaccard = len(pred_words & ref_words) / len(pred_words | ref_words)\n",
    "            semantic_similarities.append(jaccard)\n",
    "    \n",
    "    semantic_similarity = np.mean(semantic_similarities)\n",
    "    \n",
    "    # Compute answer length statistics\n",
    "    pred_lengths = [len(pred.split()) for pred in preds]\n",
    "    ref_lengths = [len(ref.split()) for ref in refs]\n",
    "    \n",
    "    # Print comprehensive results\n",
    "    print(\"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Test Set Size: {len(preds)}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"BERTScore Metrics:\")\n",
    "    print(f\"  Precision: {bert_scores['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {bert_scores['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {bert_scores['f1']:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Other Metrics:\")\n",
    "    print(f\"  Exact Match: {exact_match_score:.4f}\")\n",
    "    print(f\"  F1 Score:    {f1_score:.4f}\")\n",
    "    print(f\"  Semantic Similarity: {semantic_similarity:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Answer Length Statistics:\")\n",
    "    print(f\"  Avg Prediction Length: {np.mean(pred_lengths):.2f} words\")\n",
    "    print(f\"  Avg Reference Length:  {np.mean(ref_lengths):.2f} words\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show detailed examples\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED PREDICTION EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select diverse examples: best, worst, and random\n",
    "    bert_f1_scores = [f.item() if hasattr(f, 'item') else f for f in F1]\n",
    "    \n",
    "    # Get indices for different categories\n",
    "    sorted_indices = sorted(range(len(bert_f1_scores)), key=lambda i: bert_f1_scores[i], reverse=True)\n",
    "    \n",
    "    best_indices = sorted_indices[:3]  # Top 3\n",
    "    worst_indices = sorted_indices[-3:]  # Bottom 3\n",
    "    random_indices = random.sample(range(len(preds)), min(4, len(preds)))  # Random 4\n",
    "    \n",
    "    example_categories = [\n",
    "        (\"BEST PREDICTIONS\", best_indices),\n",
    "        (\"WORST PREDICTIONS\", worst_indices),\n",
    "        (\"RANDOM PREDICTIONS\", random_indices)\n",
    "    ]\n",
    "    \n",
    "    for category_name, indices in example_categories:\n",
    "        print(f\"\\n{category_name}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "            print(f\"BERTScore F1: {bert_f1_scores[idx]:.4f}\")\n",
    "            print(f\"Token F1: {f1_scores[idx]:.4f}\")\n",
    "            print(f\"Exact Match: {'✓' if exact_matches[idx] else '✗'}\")\n",
    "            \n",
    "            if idx < len(questions):\n",
    "                print(f\"Question: {questions[idx]}\")\n",
    "            if idx < len(contexts):\n",
    "                print(f\"Context: {contexts[idx]}\")\n",
    "            \n",
    "            print(f\"Reference Answer: {refs[idx]}\")\n",
    "            print(f\"Model Prediction: {preds[idx]}\")\n",
    "            \n",
    "            # Analysis\n",
    "            pred_words = len(preds[idx].split())\n",
    "            ref_words = len(refs[idx].split())\n",
    "            print(f\"Length: Pred={pred_words} words, Ref={ref_words} words\")\n",
    "            \n",
    "            # Simple similarity check\n",
    "            pred_lower = preds[idx].lower()\n",
    "            ref_lower = refs[idx].lower()\n",
    "            common_words = set(pred_lower.split()) & set(ref_lower.split())\n",
    "            print(f\"Common words: {len(common_words)}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Create results dictionary matching your expected format\n",
    "    results = {\n",
    "        \"test_size\": len(preds),\n",
    "        \"exact_match\": exact_match_score,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"bert_score_f1\": bert_scores[\"f1\"],\n",
    "        \"semantic_similarity\": semantic_similarity,\n",
    "        \"avg_prediction_length\": np.mean(pred_lengths),\n",
    "        \"avg_reference_length\": np.mean(ref_lengths),\n",
    "        \"predictions\": preds,\n",
    "        \"references\": refs,\n",
    "        \"questions\": questions,\n",
    "        \"individual_scores\": {\n",
    "            \"bert_f1\": bert_f1_scores,\n",
    "            \"token_f1\": f1_scores,\n",
    "            \"exact_match\": exact_matches,\n",
    "            \"semantic_similarity\": semantic_similarities\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save detailed results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save detailed examples\n",
    "    with open(\"detailed_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"DETAILED TEST SET PREDICTIONS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, (prompt, pred, ref, f1_score, em) in enumerate(zip(prompts_list, preds, refs, f1_scores, exact_matches)):\n",
    "            f.write(f\"Example {i+1}:\\n\")\n",
    "            f.write(f\"BERTScore F1: {f1_score:.4f}\\n\")\n",
    "            f.write(f\"Exact Match: {'✓' if em else '✗'}\\n\")\n",
    "            \n",
    "            if \"Question:\" in prompt:\n",
    "                question = prompt.split(\"Question:\")[-1].split(\"Answer:\")[0].strip()\n",
    "                f.write(f\"Question: {question}\\n\")\n",
    "            else:\n",
    "                f.write(f\"Prompt: {prompt}\\n\")\n",
    "            \n",
    "            f.write(f\"Reference: {ref}\\n\")\n",
    "            f.write(f\"Prediction: {pred}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    print(\"Results saved to:\")\n",
    "    print(\"  - test_evaluation_results.json\")\n",
    "    print(\"  - detailed_predictions.txt\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e592cc",
   "metadata": {},
   "source": [
    "## Iterative training and dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e47ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_answers(model, tokenizer, formatted_dataset, device, generation_num=1):\n",
    "    \"\"\"\n",
    "    Generate synthetic answers using the fine-tuned causal language model.\n",
    "    Takes formatted_dataset with prompts as input.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Generating synthetic answers (Generation {generation_num})...\")\n",
    "    \n",
    "    synthetic_data = []\n",
    "    \n",
    "    # Use the train split from formatted_dataset\n",
    "    train_dataset = formatted_dataset['train']\n",
    "    \n",
    "    # Enhanced progress bar with statistics\n",
    "    progress_bar = tqdm(\n",
    "        train_dataset, \n",
    "        desc=f\"🤖 Gen {generation_num} - Generating answers\",\n",
    "        unit=\"examples\",\n",
    "        position=1,\n",
    "        leave=False,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "    \n",
    "    # Statistics tracking\n",
    "    successful_generations = 0\n",
    "    failed_generations = 0\n",
    "    total_examples = len(train_dataset)\n",
    "    \n",
    "    for idx, example in enumerate(progress_bar):\n",
    "        # Get the prompt that was created by make_prompt function\n",
    "        prompt = example['prompt']\n",
    "        \n",
    "        # Find where the prompt ends to extract the incomplete part\n",
    "        # Assuming the prompt format ends with something like \"[/INST]\" or \"### Response:\"\n",
    "        if '[/INST]' in prompt:\n",
    "            # For instruction format, generate after [/INST]\n",
    "            generation_prompt = prompt\n",
    "            stop_sequence = '[/INST]'\n",
    "        elif '### Response:' in prompt:\n",
    "            # For alpaca format, generate after ### Response:\n",
    "            generation_prompt = prompt\n",
    "            stop_sequence = '### Response:'\n",
    "        else:\n",
    "            # Fallback: use the full prompt\n",
    "            generation_prompt = prompt\n",
    "            stop_sequence = None\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                generation_prompt,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Generate answer using causal LM\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,  # Increased for better answers\n",
    "                    do_sample=True,      # Changed to True for diversity\n",
    "                    temperature=0.7,     # Added temperature for controlled randomness\n",
    "                    top_p=0.9,          # Added nucleus sampling\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract the new generated part (answer)\n",
    "            if stop_sequence and stop_sequence in generated_text:\n",
    "                # Split by the stop sequence and take everything after it\n",
    "                parts = generated_text.split(stop_sequence)\n",
    "                if len(parts) > 1:\n",
    "                    synthetic_answer = parts[-1].strip()\n",
    "                else:\n",
    "                    synthetic_answer = \"No answer found\"\n",
    "            else:\n",
    "                # If no stop sequence, take everything after the original prompt\n",
    "                if generated_text.startswith(generation_prompt):\n",
    "                    synthetic_answer = generated_text[len(generation_prompt):].strip()\n",
    "                else:\n",
    "                    synthetic_answer = generated_text.strip()\n",
    "            \n",
    "            # Clean up the answer\n",
    "            if not synthetic_answer or synthetic_answer == generation_prompt:\n",
    "                synthetic_answer = \"No answer found\"\n",
    "                failed_generations += 1\n",
    "            else:\n",
    "                successful_generations += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any generation errors\n",
    "            synthetic_answer = \"No answer found\"\n",
    "            failed_generations += 1\n",
    "            print(f\"\\nWarning: Generation failed for example {idx}: {str(e)}\")\n",
    "        \n",
    "        # Create new example with synthetic answer\n",
    "        new_example = example.copy()\n",
    "        \n",
    "        # Update the prompt to include the generated answer\n",
    "        if stop_sequence:\n",
    "            new_example['prompt'] = generation_prompt + synthetic_answer\n",
    "        else:\n",
    "            new_example['prompt'] = generation_prompt + \" \" + synthetic_answer\n",
    "        \n",
    "        # If original data has structured fields, preserve them and update answers\n",
    "        if 'answers' in example:\n",
    "            if synthetic_answer != \"No answer found\":\n",
    "                # Try to find answer in context if context exists\n",
    "                context = example.get('context', '')\n",
    "                answer_start = context.find(synthetic_answer) if context else 0\n",
    "                if answer_start == -1:\n",
    "                    answer_start = 0\n",
    "                \n",
    "                new_example['answers'] = {\n",
    "                    'text': [synthetic_answer],\n",
    "                    'answer_start': [answer_start]\n",
    "                }\n",
    "            else:\n",
    "                new_example['answers'] = {\n",
    "                    'text': [],\n",
    "                    'answer_start': []\n",
    "                }\n",
    "        \n",
    "        # Add generation metadata\n",
    "        new_example['generation_num'] = generation_num\n",
    "        new_example['synthetic'] = True\n",
    "        \n",
    "        synthetic_data.append(new_example)\n",
    "        \n",
    "        # Update progress bar with statistics\n",
    "        success_rate = (successful_generations / (idx + 1)) * 100\n",
    "        progress_bar.set_postfix({\n",
    "            'Success': f'{successful_generations}/{idx + 1}',\n",
    "            'Rate': f'{success_rate:.1f}%',\n",
    "            'Failed': failed_generations\n",
    "        })\n",
    "        \n",
    "        # Update description every 100 examples\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            progress_bar.set_description(\n",
    "                f\"🤖 Gen {generation_num} - Generated {idx + 1}/{total_examples}\"\n",
    "            )\n",
    "    \n",
    "    # Close progress bar\n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f\"✅ Generation {generation_num} completed:\")\n",
    "    print(f\"   📊 Total examples processed: {total_examples}\")\n",
    "    print(f\"   ✅ Successful generations: {successful_generations}\")\n",
    "    print(f\"   ❌ Failed generations: {failed_generations}\")\n",
    "    print(f\"   📈 Success rate: {(successful_generations/total_examples)*100:.1f}%\")\n",
    "    \n",
    "    # Create a new formatted dataset with the synthetic data\n",
    "    print(\"📦 Creating synthetic dataset...\")\n",
    "    with tqdm(total=1, desc=\"📦 Building Dataset\", position=1, leave=False) as dataset_pbar:\n",
    "        synthetic_dataset = Dataset.from_list(synthetic_data)\n",
    "        dataset_pbar.update(1)\n",
    "    \n",
    "    # Return in the same format as input\n",
    "    return {\n",
    "        'train': synthetic_dataset\n",
    "    }\n",
    "\n",
    "\n",
    "def save_synthetic_dataset(dataset_dir, synthetic_formatted_dataset, generation_num):\n",
    "    \"\"\"\n",
    "    Save the synthetic formatted dataset to a new subdirectory.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Base dataset directory\n",
    "        synthetic_formatted_dataset: Formatted dataset with synthetic answers\n",
    "        generation_num: Generation number\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create new subdirectory\n",
    "    new_dir = os.path.join(dataset_dir, f\"generation_{generation_num}\")\n",
    "    os.makedirs(new_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the train split\n",
    "    synthetic_formatted_dataset['train'].save_to_disk(os.path.join(new_dir, \"train\"))\n",
    "    \n",
    "    # Also save as JSON for inspection\n",
    "    json_file = os.path.join(new_dir, \"synthetic_data.json\")\n",
    "    synthetic_formatted_dataset['train'].to_json(json_file)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"generation_number\": generation_num,\n",
    "        \"total_examples\": len(synthetic_formatted_dataset['train']),\n",
    "        \"generated_from\": \"fine_tuned_model\",\n",
    "        \"description\": f\"Synthetic answers generated using fine-tuned model (Generation {generation_num})\",\n",
    "        \"format\": \"formatted_dataset_with_prompts\"\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(new_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved synthetic formatted dataset to {new_dir}\")\n",
    "    return new_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "653fe91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_nick(model_path):\n",
    "    # Extract the part after the first \"/\"\n",
    "    model_name = model_path.split(\"/\")[1]\n",
    "    \n",
    "    # Match common patterns like \"phi-3\" or \"Mistral-7B\"\n",
    "    match = re.match(r\"([A-Za-z0-9\\-]+?)(?=-\\d|-[a-zA-Z])\", model_name)\n",
    "    \n",
    "    return match.group(1) if match else model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388194b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_training_and_generation(\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    model_path = \"microsoft/phi-3-mini-128k-instruct\",\n",
    "    num_generations=3,\n",
    "    start_generation=1,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform iterative training and synthetic data generation.\n",
    "    \n",
    "    Args:\n",
    "        base_dataset_dir: Directory containing the original dataset\n",
    "        num_generations: Number of generations to create\n",
    "        device: Device for inference\n",
    "    \"\"\"\n",
    "    \n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    # Determine if starting from base model or fine-tuned model\n",
    "    is_base_model = start_generation == 1\n",
    "    \n",
    "    # Load tokenizer once at the beginning\n",
    "    if is_base_model:\n",
    "        base_model_name = model_path\n",
    "        model_nick = extract_model_nick(base_model_name)\n",
    "    else:\n",
    "        # For fine-tuned models, extract base model name for tokenizer\n",
    "        # Assume model_path format: \"./phi3-squad2-gen{X}-final\"\n",
    "        base_model_name = model_path  # Default fallback\n",
    "        model_nick = extract_model_nick(base_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Define tokenization function\n",
    "    def tokenize(example):\n",
    "        return tokenizer(\n",
    "            example[\"prompt\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256\n",
    "        )\n",
    "    \n",
    "    print(f\"Starting iterative training and generation for {num_generations} generations...\")\n",
    "    \n",
    "    # Used to load dataset\n",
    "    dataset = None\n",
    "    formatted_dataset = None\n",
    "    \n",
    "    # Main progress bar for generations\n",
    "    generation_progress = tqdm(\n",
    "        range(start_generation, start_generation + num_generations), \n",
    "        desc=\"🔄 Overall Progress\", \n",
    "        unit=\"generation\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    for generation in generation_progress:\n",
    "        torch.cuda.empty_cache()\n",
    "        generation_progress.set_description(f\"🔄 Generation {generation}/{num_generations}\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"GENERATION {generation}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Step 1: Fine-tune model with current training dataset\n",
    "        print(\"Step 1: Fine-tuning model...\")\n",
    "        \n",
    "        # Prepare training arguments for this generation\n",
    "        train_config = {\n",
    "            \"bf16\": True,\n",
    "            \"do_eval\": False,  # Disable evaluation completely\n",
    "            \"learning_rate\": 1.0e-05,\n",
    "            \"log_level\": \"info\",\n",
    "            \"logging_steps\": 10,\n",
    "            \"logging_strategy\": \"steps\",\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"num_train_epochs\": 3,\n",
    "            \"max_steps\": -1,\n",
    "            \"output_dir\": f\"./phi3-squad2-gen{generation}\",  # Update for each generation\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"per_device_train_batch_size\": 4,\n",
    "            \"remove_unused_columns\": True,\n",
    "            \"save_steps\": 50,\n",
    "            \"save_total_limit\": 2,\n",
    "            \"seed\": 42,\n",
    "            \"gradient_checkpointing\": True,\n",
    "            \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "            \"warmup_ratio\": 0.05,\n",
    "            \"save_strategy\": \"steps\",\n",
    "            \"load_best_model_at_end\": False,  # No evaluation, so no \"best\" model\n",
    "            \"disable_tqdm\": False,  # Enable tqdm progress bars for training\n",
    "        }\n",
    "\n",
    "        train_args = TrainingArguments(**train_config)\n",
    "\n",
    "        offload_cache_dir = \"./offload_cache\"\n",
    "        os.makedirs(offload_cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Load model for this generation\n",
    "        print(\"📥 Loading model...\")\n",
    "        with tqdm(total=1, desc=\"🤖 Model Loading\", position=1, leave=False) as model_pbar:\n",
    "            if generation == start_generation and is_base_model:\n",
    "                # First generation: load base model with quantization\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    base_model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    quantization_config=bnb_config\n",
    "                )\n",
    "                # Prepare dataset for first generation\n",
    "                dataset = load_squad_subset(base_dataset_dir)\n",
    "                formatted_dataset = {\n",
    "                    split: dataset[split].map(make_prompt)\n",
    "                    for split in dataset.keys()\n",
    "                }\n",
    "            elif generation == start_generation and not is_base_model:\n",
    "                # Starting from fine-tuned model: load without quantization\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    device_map=\"auto\",\n",
    "                    offload_folder=\"./offload_cache\",  # Add offload directory\n",
    "                    torch_dtype=torch.float16,        # Use float16 to save memory\n",
    "                    low_cpu_mem_usage=True           # Enable low CPU memory usage\n",
    "                )\n",
    "                # Prepare dataset based on start generation\n",
    "                if start_generation == 1:\n",
    "                    dataset = load_squad_subset(base_dataset_dir)\n",
    "                    formatted_dataset = {\n",
    "                        split: dataset[split].map(make_prompt)\n",
    "                        for split in dataset.keys()\n",
    "                    }\n",
    "                else:\n",
    "                    # Load synthetic data from previous generation\n",
    "                    synthetic_train = load_train_only(f\"{base_dataset_dir}/generation_{start_generation - 1}\")\n",
    "                    original_dataset = load_from_disk(base_dataset_dir)\n",
    "                    dataset = DatasetDict({\n",
    "                        'train': synthetic_train,\n",
    "                        'test': original_dataset['test']\n",
    "                    })\n",
    "                    formatted_dataset = {\n",
    "                        split: dataset[split].map(make_prompt)\n",
    "                        for split in dataset.keys()\n",
    "                    }\n",
    "                            \n",
    "            else:\n",
    "                # Subsequent generations: load previous model\n",
    "                previous_model_path = f\"./{model_nick}-squad2-gen{generation-1}-final\"\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    previous_model_path,\n",
    "                    device_map=\"auto\",\n",
    "                    offload_folder=\"./offload_cache\",  # Add offload directory\n",
    "                    torch_dtype=torch.float16,        # Use float16 to save memory\n",
    "                    low_cpu_mem_usage=True           # Enable low CPU memory usage\n",
    "                )\n",
    "                \n",
    "                # Load synthetic data and create dataset\n",
    "                synthetic_train = load_train_only(f\"{base_dataset_dir}/generation_{generation - 1}\")\n",
    "                original_dataset = load_from_disk(base_dataset_dir)\n",
    "                dataset = DatasetDict({\n",
    "                    'train': synthetic_train,\n",
    "                    'test': original_dataset['test']\n",
    "                })\n",
    "                formatted_dataset = {\n",
    "                    split: dataset[split].map(make_prompt)\n",
    "                    for split in dataset.keys()\n",
    "                }\n",
    "\n",
    "            model_pbar.update(1)\n",
    "        \n",
    "        # Apply PEFT configuration\n",
    "        print(\"🔧 Applying PEFT configuration...\")\n",
    "        with tqdm(total=1, desc=\"⚙️ PEFT Setup\", position=1, leave=False) as peft_pbar:\n",
    "            peft_config = {\n",
    "                \"r\": 8,  # Reduced from 16 to 8 (fewer parameters)\n",
    "                \"lora_alpha\": 16,  # Reduced from 32 to 16\n",
    "                \"lora_dropout\": 0.1,  # Slightly increased dropout\n",
    "                \"bias\": \"none\",\n",
    "                \"task_type\": \"CAUSAL_LM\",\n",
    "                \"target_modules\": \"all-linear\",\n",
    "                \"modules_to_save\": None,\n",
    "            }\n",
    "            lora_config = LoraConfig(**peft_config)\n",
    "            model = get_peft_model(model, lora_config)\n",
    "            peft_pbar.update(1)\n",
    "        \n",
    "        # Tokenize current training dataset\n",
    "        print(\"🔤 Tokenizing dataset...\")\n",
    "        tokenized = {\n",
    "            split: formatted_dataset[split].map(tokenize, batched=True)\n",
    "            for split in formatted_dataset.keys()\n",
    "        }\n",
    "        \n",
    "        # Fine-tune the model\n",
    "        print(\"🚀 Starting training...\")\n",
    "        trainer = train_model(model, tokenized, tokenizer, train_args)\n",
    "        \n",
    "        # Save the fine-tuned model for this generation\n",
    "        print(\"💾 Saving model...\")\n",
    "        with tqdm(total=1, desc=\"💾 Saving Model\", position=1, leave=False) as save_pbar:\n",
    "            final_model_path = f\"./{model_nick}-squad2-gen{generation}-final\"\n",
    "            trainer.save_model(final_model_path)\n",
    "            save_pbar.update(1)\n",
    "        print(f\"✅ Generation {generation} model saved to {final_model_path}\")\n",
    "        \n",
    "        # Step 2: Generate synthetic answers using the fine-tuned model\n",
    "        print(\"Step 2: Generating synthetic answers...\")\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        synthetic_dataset = generate_synthetic_answers(\n",
    "            model, tokenizer, formatted_dataset, device, generation\n",
    "        )\n",
    "        \n",
    "        # Step 3: Save synthetic dataset\n",
    "        print(\"Step 3: Saving synthetic dataset...\")\n",
    "        with tqdm(total=1, desc=\"💾 Saving Dataset\", position=1, leave=False) as dataset_save_pbar:\n",
    "            new_dir = save_synthetic_dataset(base_dataset_dir, synthetic_dataset, generation)\n",
    "            dataset_save_pbar.update(1)\n",
    "        \n",
    "        # Update current training dataset for next iteration\n",
    "        current_train_dataset = synthetic_dataset\n",
    "        \n",
    "        print(f\"Generation {generation} completed!\")\n",
    "        print(f\"Synthetic dataset saved to: {new_dir}\")\n",
    "        print(f\"Model saved to: {final_model_path}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    generation_progress.close()\n",
    "    print(f\"\\n🎉 All {num_generations} generations completed!\")\n",
    "    print(\"Final models and datasets are ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_training_and_generation(\n",
    "        base_dataset_dir=\"squad_v2_05percent\",\n",
    "        model_path=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "        start_generation=3,\n",
    "        num_generations=5,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94bcca3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04dfefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation(base_model_name, generation_num, base_dataset_dir, device, \n",
    "                       save_results=True, results_dir=\"evaluation_results\"):\n",
    "    \"\"\"\n",
    "    Evaluate a specific generation model on the test set with optional result saving.\n",
    "    Loads the model and tokenizer internally based on generation number.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Base model name (e.g., \"microsoft/phi-3-mini-128k-instruct\")\n",
    "        generation_num: Generation number to evaluate\n",
    "        base_dataset_dir: Directory containing the original test dataset\n",
    "        device: Device for inference\n",
    "        save_results: Whether to save results to JSON file\n",
    "        results_dir: Directory to save evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    # Determine if starting from base model or fine-tuned model\n",
    "    is_base_model = generation_num == 0\n",
    "    \n",
    "    # Load tokenizer once at the beginning\n",
    "    if is_base_model:\n",
    "        base_model_name = base_model_name\n",
    "        model_nick = extract_model_nick(base_model_name)\n",
    "    else:\n",
    "        # For fine-tuned models, extract base model name for tokenizer\n",
    "        # Assume model_path format: \"./phi3-squad2-gen{X}-final\"\n",
    "        base_model_name = base_model_name  # Default fallback\n",
    "        model_nick = extract_model_nick(base_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Extract model nickname for path construction\n",
    "    model_nick = extract_model_nick(base_model_name)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"📥 Loading tokenizer...\")\n",
    "    with tqdm(total=1, desc=\"🔤 Tokenizer Loading\", position=1, leave=False) as tokenizer_pbar:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer_pbar.update(1)\n",
    "    \n",
    "    # Load model based on generation number\n",
    "    print(\"📥 Loading model...\")\n",
    "    with tqdm(total=1, desc=\"🤖 Model Loading\", position=1, leave=False) as model_pbar:\n",
    "        if generation_num == 0:\n",
    "            # Load base model\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            # Load fine-tuned model from specific generation\n",
    "            model_path = f\"./{model_nick}-squad2-gen{generation_num}-final\"\n",
    "            # Create offload directory\n",
    "            offload_cache_dir = \"./offload_cache\"\n",
    "            os.makedirs(offload_cache_dir, exist_ok=True)\n",
    "            \n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model for generation {generation_num} not found at {model_path}\")\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                offload_folder=offload_cache_dir,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        model_pbar.update(1)\n",
    "    \n",
    "    # Load original test dataset\n",
    "    with tqdm(total=1, desc=\"📂 Loading Test Data\", position=1, leave=False) as load_pbar:\n",
    "        original_dataset = load_from_disk(base_dataset_dir)\n",
    "        test_dataset = original_dataset['test']\n",
    "        load_pbar.update(1)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Run evaluation\n",
    "    with tqdm(total=1, desc=\"📊 Evaluating\", position=1, leave=False) as eval_pbar:\n",
    "        evaluation_results = evaluate_model(model, tokenizer, test_dataset, device)\n",
    "        eval_pbar.update(1)\n",
    "    \n",
    "    # Add metadata to results\n",
    "    evaluation_results.update({\n",
    "        'generation': generation_num,\n",
    "        'test_dataset_size': len(test_dataset),\n",
    "        'base_dataset_dir': base_dataset_dir,\n",
    "        'base_model_name': base_model_name,\n",
    "        'model_nick': model_nick,\n",
    "        'model_path': f\"./{model_nick}-squad2-gen{generation_num}-final\" if generation_num > 1 else base_model_name,\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"📊 Generation {generation_num} Evaluation Results:\")\n",
    "    print(f\"   Exact Match: {evaluation_results['exact_match']:.3f}\")\n",
    "    print(f\"   F1 Score: {evaluation_results['f1_score']:.3f}\")\n",
    "    print(f\"   BERTScore F1: {evaluation_results['bert_score_f1']:.3f}\")\n",
    "    print(f\"   Semantic Similarity: {evaluation_results['semantic_similarity']:.3f}\")\n",
    "    \n",
    "    # Save results if requested\n",
    "    if save_results:\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        results_file = os.path.join(results_dir, f\"generation_{generation_num}_results.json\")\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(evaluation_results, f, indent=2)\n",
    "        \n",
    "        print(f\"💾 Results saved to: {results_file}\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "464dcfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b0f6e3720b4f789b906f3b6130dda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: lm_head.weight, model.norm.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.embed_tokens.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TEST SET EVALUATION WITH EXAMPLES\n",
      "============================================================\n",
      "Preparing test prompts...\n",
      "Generating predictions for 59 test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 59/59 [01:34<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated! Computing metrics...\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.query.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.query.bias, pooler.dense.bias, encoder.layer.*.attention.self.key.bias, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Test Set Size: 59\n",
      "------------------------------------------------------------\n",
      "BERTScore Metrics:\n",
      "  Precision: 0.7729\n",
      "  Recall:    0.8187\n",
      "  F1 Score:  0.7949\n",
      "------------------------------------------------------------\n",
      "Other Metrics:\n",
      "  Exact Match: 0.3051\n",
      "  F1 Score:    0.0364\n",
      "  Semantic Similarity: 0.0195\n",
      "------------------------------------------------------------\n",
      "Answer Length Statistics:\n",
      "  Avg Prediction Length: 30.24 words\n",
      "  Avg Reference Length:  1.88 words\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED PREDICTION EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "BEST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 34):\n",
      "BERTScore F1: 0.8574\n",
      "Token F1: 0.2222\n",
      "Exact Match: ✗\n",
      "Question: What did maternal Old Norse traditions merge with?\n",
      "Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, ...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: ### Instruction: [INST] Given the context, answer the\n",
      "Length: Pred=8 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 1):\n",
      "BERTScore F1: 0.8518\n",
      "Token F1: 0.2000\n",
      "Exact Match: ✓\n",
      "Question: When were the Normans in Normandy?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: 10th and 11th centuries\n",
      "Model Prediction: The Normans were in Normandy during the 10th and 11th centuries. They were descendants of Norse raiders and pirates who settled in the region and gradually assimilated with the local populations. Over time,\n",
      "Length: Pred=33 words, Ref=4 words\n",
      "Common words: 3\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 2):\n",
      "BERTScore F1: 0.8446\n",
      "Token F1: 0.1250\n",
      "Exact Match: ✗\n",
      "Question: From which countries did the Norse originate?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: Denmark, Iceland and Norway\n",
      "Model Prediction: The Norse people, who later became known as the Normans, originated from Denmark, Iceland, and Norway. They were originally raiders and pirates from these regions. Their leader, Rollo, came from one of these\n",
      "Length: Pred=33 words, Ref=4 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "WORST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 17):\n",
      "BERTScore F1: 0.7700\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What is the original meaning of the word Norman?\n",
      "Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from ...\n",
      "Reference Answer: Viking\n",
      "Model Prediction: The original meaning of the word Norman is derived from Old Low Franconian Nortmann or directly from Old Norse Norðmaðr, Latinized as Nortmannus, Normannus, or Nordmannus. It refers to\n",
      "Length: Pred=29 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 18):\n",
      "BERTScore F1: 0.7670\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: When was the Latin version of the word Norman first recorded?\n",
      "Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from ...\n",
      "Reference Answer: 9th century\n",
      "Model Prediction: The Latin version of the word Norman, which can be Latinized as Nortmannus, Normannus, or Nordmannus\n",
      "Length: Pred=16 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 46):\n",
      "BERTScore F1: 0.0000\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: When did Robert Crispin go up against the Turks?\n",
      "Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They we...\n",
      "Reference Answer: 1060s\n",
      "Model Prediction: \n",
      "Length: Pred=0 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "RANDOM PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 47):\n",
      "BERTScore F1: 0.8419\n",
      "Token F1: 0.0769\n",
      "Exact Match: ✓\n",
      "Question: Who ruined Roussel de Bailleul's plans for an independent state?\n",
      "Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They we...\n",
      "Reference Answer: Alexius Komnenos\n",
      "Model Prediction: The Byzantine general who stopped Roussel de Bailleul's plans for an independent state was Alexius Komnenos.\n",
      "\n",
      "\n",
      "The context provided indicates that Roussel de Bailleul, a Norman mercenary,\n",
      "Length: Pred=27 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 6):\n",
      "BERTScore F1: 0.7777\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What is France a region of?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: France is a region of Europe. It is a country located in the western part of the continent. France shares borders with Belgium, Luxembourg, Germany, Switzerland, Italy, and Spain. It also has coastlines along the Atlantic Ocean, the\n",
      "Length: Pred=38 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 43):\n",
      "BERTScore F1: 0.8178\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who did the Normans encourage to come to the south?\n",
      "Context: Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were f...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: The Normans, during their expansion, encouraged the Lombards to come to the south. This was part of their strategy to act against the By\n",
      "Length: Pred=24 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4 (Index 57):\n",
      "BERTScore F1: 0.7915\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What Frank led Norman forces?\n",
      "Context: Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further so...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: The Frank who led Norman forces was Oursel. He led a force of \"Franks\" into the upper Euphrates valley in northern Syria. This Norman force, which was part of the Armenian general Philaretus Bracham\n",
      "Length: Pred=34 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "Results saved to:\n",
      "  - test_evaluation_results.json\n",
      "  - detailed_predictions.txt\n",
      "============================================================\n",
      "📊 Generation 0 Evaluation Results:\n",
      "   Exact Match: 0.305\n",
      "   F1 Score: 0.036\n",
      "   BERTScore F1: 0.795\n",
      "   Semantic Similarity: 0.020\n",
      "💾 Results saved to: evaluation_results\\generation_0_results.json\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_generation(\n",
    "    base_model_name=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "    generation_num=0,\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_results=True,\n",
    "    results_dir=\"evaluation_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37976845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cf9b1807c34aedb6f905a88a23e7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: lm_head.weight, model.norm.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.embed_tokens.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TEST SET EVALUATION WITH EXAMPLES\n",
      "============================================================\n",
      "Preparing test prompts...\n",
      "Generating predictions for 59 test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 59/59 [01:42<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated! Computing metrics...\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.query.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.query.bias, pooler.dense.bias, encoder.layer.*.attention.self.key.bias, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Test Set Size: 59\n",
      "------------------------------------------------------------\n",
      "BERTScore Metrics:\n",
      "  Precision: 0.8084\n",
      "  Recall:    0.8387\n",
      "  F1 Score:  0.8229\n",
      "------------------------------------------------------------\n",
      "Other Metrics:\n",
      "  Exact Match: 0.2203\n",
      "  F1 Score:    0.0757\n",
      "  Semantic Similarity: 0.0527\n",
      "------------------------------------------------------------\n",
      "Answer Length Statistics:\n",
      "  Avg Prediction Length: 15.02 words\n",
      "  Avg Reference Length:  1.88 words\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED PREDICTION EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "BEST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 3):\n",
      "BERTScore F1: 1.0000\n",
      "Token F1: 1.0000\n",
      "Exact Match: ✓\n",
      "Question: Who was the Norse leader?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: Rollo\n",
      "Model Prediction: Rollo\n",
      "Length: Pred=1 words, Ref=1 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 22):\n",
      "BERTScore F1: 0.9219\n",
      "Token F1: 0.6667\n",
      "Exact Match: ✓\n",
      "Question: Who did Rollo sign the treaty of Saint-Clair-sur-Epte with?\n",
      "Context: In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal prop...\n",
      "Reference Answer: King Charles III\n",
      "Model Prediction: King Charles III of West Francia\n",
      "Length: Pred=6 words, Ref=3 words\n",
      "Common words: 3\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 9):\n",
      "BERTScore F1: 0.9136\n",
      "Token F1: 0.4615\n",
      "Exact Match: ✓\n",
      "Question: Who was the duke in the battle of Hastings?\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: William the Conqueror\n",
      "Model Prediction: William the Conqueror was the duke in the Battle of Hastings. [/INST\n",
      "Length: Pred=12 words, Ref=3 words\n",
      "Common words: 3\n",
      "--------------------------------------------------\n",
      "\n",
      "WORST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 44):\n",
      "BERTScore F1: 0.7764\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: During what campaign did the Vargian and Lombard fight?\n",
      "Context: Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were f...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: During the Sicilian campaign of George Maniaces in 1038–40, the Varangian and Lombard\n",
      "Length: Pred=13 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 52):\n",
      "BERTScore F1: 0.7689\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What was the name of the Norman castle?\n",
      "Context: Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further so...\n",
      "Reference Answer: Afranji\n",
      "Model Prediction: The Norman castle was named\n",
      "Length: Pred=5 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 47):\n",
      "BERTScore F1: 0.7580\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who ruined Roussel de Bailleul's plans for an independent state?\n",
      "Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They we...\n",
      "Reference Answer: Alexius Komnenos\n",
      "Model Prediction: According to the context, Roussel de Bailleul's plans for an independent state in\n",
      "Length: Pred=13 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "RANDOM PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 34):\n",
      "BERTScore F1: 0.8210\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What did maternal Old Norse traditions merge with?\n",
      "Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, ...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: The context provided discusses the cultural synthesis that occurred when the descendants of Rollo's Vikings and their Frankish wives settled in the region\n",
      "Length: Pred=23 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 5):\n",
      "BERTScore F1: 0.8180\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who gave their name to Normandy in the 1000's and 1100's\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: The Normans, who were descendants of Norse raiders and pirates from Denmark\n",
      "Length: Pred=12 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 37):\n",
      "BERTScore F1: 0.8388\n",
      "Token F1: 0.1250\n",
      "Exact Match: ✗\n",
      "Question: Who adopted the fuedel doctrines of the Normans?\n",
      "Context: The Normans thereafter adopted the growing feudal doctrines of the rest of France, and worked them into a functional hierarchical system in both Normandy and in England. The new Norman rulers were cul...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Normans thereafter adopted the growing feudal doctrines of the rest\n",
      "Length: Pred=18 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4 (Index 27):\n",
      "BERTScore F1: 0.7952\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What did the French promises to protect Rollo and his men from?\n",
      "Context: In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal prop...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: According to the original text \"The treaty offered Rollo and his men the French lands between the river Epte and the Atlantic coast in exchange for their protection against further Viking incursions\n",
      "Length: Pred=32 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "Results saved to:\n",
      "  - test_evaluation_results.json\n",
      "  - detailed_predictions.txt\n",
      "============================================================\n",
      "📊 Generation 1 Evaluation Results:\n",
      "   Exact Match: 0.220\n",
      "   F1 Score: 0.076\n",
      "   BERTScore F1: 0.823\n",
      "   Semantic Similarity: 0.053\n",
      "💾 Results saved to: evaluation_results\\generation_1_results.json\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_generation(\n",
    "    base_model_name=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "    generation_num=1,\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_results=True,\n",
    "    results_dir=\"evaluation_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15d4af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d73df9dd234d1380286cb06b433380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: lm_head.weight, model.norm.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.embed_tokens.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TEST SET EVALUATION WITH EXAMPLES\n",
      "============================================================\n",
      "Preparing test prompts...\n",
      "Generating predictions for 59 test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 59/59 [01:38<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated! Computing metrics...\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.query.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.query.bias, pooler.dense.bias, encoder.layer.*.attention.self.key.bias, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Test Set Size: 59\n",
      "------------------------------------------------------------\n",
      "BERTScore Metrics:\n",
      "  Precision: 0.7898\n",
      "  Recall:    0.8322\n",
      "  F1 Score:  0.8100\n",
      "------------------------------------------------------------\n",
      "Other Metrics:\n",
      "  Exact Match: 0.0508\n",
      "  F1 Score:    0.0519\n",
      "  Semantic Similarity: 0.0272\n",
      "------------------------------------------------------------\n",
      "Answer Length Statistics:\n",
      "  Avg Prediction Length: 27.71 words\n",
      "  Avg Reference Length:  1.88 words\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED PREDICTION EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "BEST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 9):\n",
      "BERTScore F1: 0.8821\n",
      "Token F1: 0.1818\n",
      "Exact Match: ✓\n",
      "Question: Who was the duke in the battle of Hastings?\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: William the Conqueror\n",
      "Model Prediction: William the Conqueror, also known as William I of England, was the Duke of Normandy before he became the King of England after his\n",
      "Length: Pred=24 words, Ref=3 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 11):\n",
      "BERTScore F1: 0.8550\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What religion were the Normans\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: Catholic\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Norman dynasty had a major\n",
      "Length: Pred=13 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 31):\n",
      "BERTScore F1: 0.8335\n",
      "Token F1: 0.0800\n",
      "Exact Match: ✓\n",
      "Question: What was the Norman religion?\n",
      "Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, ...\n",
      "Reference Answer: Catholicism\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity)\n",
      "Length: Pred=29 words, Ref=1 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "WORST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 47):\n",
      "BERTScore F1: 0.7753\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who ruined Roussel de Bailleul's plans for an independent state?\n",
      "Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They we...\n",
      "Reference Answer: Alexius Komnenos\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Norman mercenaries were employed by the Byzantine Empire in the 11th century. They were led by Robert Crispin,\n",
      "Length: Pred=26 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 18):\n",
      "BERTScore F1: 0.7730\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: When was the Latin version of the word Norman first recorded?\n",
      "Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from ...\n",
      "Reference Answer: 9th century\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrow\n",
      "Length: Pred=28 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 52):\n",
      "BERTScore F1: 0.7486\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What was the name of the Norman castle?\n",
      "Context: Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further so...\n",
      "Reference Answer: Afranji\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Norman conquest of southern Italy and Sicily began in 1060, when Robert Guiscard, Duke of Apulia and Calabria, land\n",
      "Length: Pred=28 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "RANDOM PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 2):\n",
      "BERTScore F1: 0.7819\n",
      "Token F1: 0.0833\n",
      "Exact Match: ✗\n",
      "Question: From which countries did the Norse originate?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: Denmark, Iceland and Norway\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and\n",
      "Length: Pred=24 words, Ref=4 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 1):\n",
      "BERTScore F1: 0.7931\n",
      "Token F1: 0.1667\n",
      "Exact Match: ✗\n",
      "Question: When were the Normans in Normandy?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: 10th and 11th centuries\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and\n",
      "Length: Pred=24 words, Ref=4 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 5):\n",
      "BERTScore F1: 0.7932\n",
      "Token F1: 0.0909\n",
      "Exact Match: ✗\n",
      "Question: Who gave their name to Normandy in the 1000's and 1100's\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and\n",
      "Length: Pred=24 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4 (Index 13):\n",
      "BERTScore F1: 0.8258\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who was famed for their Christian spirit?\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: The Normans were f\n",
      "Length: Pred=4 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "Results saved to:\n",
      "  - test_evaluation_results.json\n",
      "  - detailed_predictions.txt\n",
      "============================================================\n",
      "📊 Generation 2 Evaluation Results:\n",
      "   Exact Match: 0.051\n",
      "   F1 Score: 0.052\n",
      "   BERTScore F1: 0.810\n",
      "   Semantic Similarity: 0.027\n",
      "💾 Results saved to: evaluation_results\\generation_2_results.json\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_generation(\n",
    "    base_model_name=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "    generation_num=2,\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_results=True,\n",
    "    results_dir=\"evaluation_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7e5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
