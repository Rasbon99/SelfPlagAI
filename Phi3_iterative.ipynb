{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a919dc6",
   "metadata": {},
   "source": [
    "# Phi-3 Iterative Try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4babf",
   "metadata": {},
   "source": [
    "This notebook is used to try the iterative pipeline without the integration of MongoDB to store the versioning of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844840d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbc3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from bert_score import score\n",
    "from datasets import Dataset, load_from_disk\n",
    "from dotenv import load_dotenv\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb166e",
   "metadata": {},
   "source": [
    "### Dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdc10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad_subset(dataset_dir=\"squad_v2_05percent\"):\n",
    "    \"\"\"\n",
    "    Load the saved SQuAD v2 0.5% dataset from disk.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir (str): Directory where the dataset was saved\n",
    "        \n",
    "    Returns:\n",
    "        DatasetDict: The loaded dataset with train and test splits\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{dataset_dir}' not found. Please run the extraction script first.\")\n",
    "    \n",
    "    print(f\"Loading dataset from {dataset_dir}...\")\n",
    "    \n",
    "    # Load the dataset using Hugging Face datasets\n",
    "    dataset = load_from_disk(dataset_dir)\n",
    "    \n",
    "    # Load and display metadata\n",
    "    metadata_path = os.path.join(dataset_dir, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"Dataset metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"\\nLoaded dataset splits:\")\n",
    "    for split_name, split_data in dataset.items():\n",
    "        print(f\"  {split_name}: {len(split_data)} examples\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5fad3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from squad_v2_05percent...\n",
      "Dataset metadata:\n",
      "  original_train_size: 130319\n",
      "  original_validation_size: 11873\n",
      "  extracted_train_size: 651\n",
      "  extracted_test_size: 59\n",
      "  extraction_percentage: 0.5\n",
      "\n",
      "Loaded dataset splits:\n",
      "  train: 651 examples\n",
      "  test: 59 examples\n",
      "\n",
      "Example from each split:\n",
      "Train: When did Beyonce start becoming popular?\n",
      "Test: In what country is Normandy located?\n",
      "\n",
      "Dataset features: {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_squad_subset()\n",
    "    \n",
    "# Show examples\n",
    "print(f\"\\nExample from each split:\")\n",
    "print(f\"Train: {dataset['train'][0]['question']}\")\n",
    "print(f\"Test: {dataset['test'][0]['question']}\")\n",
    "\n",
    "# Access specific fields\n",
    "print(f\"\\nDataset features: {dataset['train'].features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to format the dataset examples into a prompt\n",
    "#The prompt will include the context, question, and answer\n",
    "def make_prompt(example):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"No answer\"\n",
    "\n",
    "    prompt = f\"[INST] Given the context, answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question} [/INST] {answer}\"\n",
    "    return {\"prompt\": prompt, \"reference\": answer}\n",
    "\n",
    "formatted_dataset = {\n",
    "    split: dataset[split].map(make_prompt)\n",
    "    for split in dataset.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5fdf7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " [INST] Given the context, answer the question.\n",
      "\n",
      "Context: Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc√©'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "\n",
      "Question: When did Beyonce start becoming popular? [/INST] in the late 1990s\n",
      "\n",
      "Reference Answer:\n",
      " in the late 1990s\n"
     ]
    }
   ],
   "source": [
    "# Print the first formatted prompt and its reference answer\n",
    "print(\"Prompt:\\n\", formatted_dataset[\"train\"][0][\"prompt\"])\n",
    "print(\"\\nReference Answer:\\n\", formatted_dataset[\"train\"][0][\"reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad29c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"key.env\")\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "889ec8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae51a83",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3828e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced logging suppression - including BERTScore sharding messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate.utils.modeling\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f52b2384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERTScore silently...\n",
      "BERTScore initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERTScore silently\n",
    "print(\"Initializing BERTScore silently...\")\n",
    "with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "    from bert_score import score\n",
    "    _ = score([\"test\"], [\"test\"], lang=\"en\", verbose=False)\n",
    "print(\"BERTScore initialized successfully!\")\n",
    "\n",
    "# Modified BERTScore function with complete output suppression\n",
    "def silent_bert_score(cands, refs, lang=\"en\"):\n",
    "    \"\"\"BERTScore calculation with all output suppressed\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    old_stderr = sys.stderr\n",
    "    \n",
    "    sys.stdout = io.StringIO()\n",
    "    sys.stderr = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        P, R, F1 = score(cands, refs, lang=lang, verbose=False)\n",
    "        return P, R, F1\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stderr = old_stderr\n",
    "\n",
    "# Custom Early Stopping based on Training Loss\n",
    "class TrainingLossEarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait_count = 0\n",
    "        \n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        if logs is not None and 'train_loss' in logs:\n",
    "            current_loss = logs['train_loss']\n",
    "            \n",
    "            if current_loss < self.best_loss - self.min_delta:\n",
    "                self.best_loss = current_loss\n",
    "                self.wait_count = 0\n",
    "                print(f\"üìà Training loss improved to {current_loss:.4f}\")\n",
    "            else:\n",
    "                self.wait_count += 1\n",
    "                print(f\"üìä No improvement in training loss ({self.wait_count}/{self.patience})\")\n",
    "                \n",
    "                if self.wait_count >= self.patience:\n",
    "                    print(f\"üõë Early stopping triggered! Best loss: {self.best_loss:.4f}\")\n",
    "                    control.should_training_stop = True\n",
    "\n",
    "# Fixed Custom Trainer class with BERTScore loss\n",
    "class BERTScoreTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.label_names = [\"labels\"]\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Custom loss function using BERTScore - completely silent\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Temporarily disable cache for forward pass\n",
    "        model.config.use_cache = False\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Generate predictions for BERTScore\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Re-enable cache for generation\n",
    "            model.config.use_cache = True\n",
    "            \n",
    "            # Generate text\n",
    "            try:\n",
    "                generated = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.processing_class.eos_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "                \n",
    "                # Decode predictions and references\n",
    "                pred_texts = self.processing_class.batch_decode(generated, skip_special_tokens=True)\n",
    "                ref_texts = self.processing_class.batch_decode(labels, skip_special_tokens=True)\n",
    "                \n",
    "                # Calculate BERTScore with completely silent function\n",
    "                P, R, F1 = silent_bert_score(pred_texts, ref_texts, lang=\"en\")\n",
    "                bert_f1 = F1.mean().item()\n",
    "                \n",
    "                # Convert BERTScore to loss\n",
    "                bert_loss = torch.tensor(1.0 - bert_f1, requires_grad=True, device=input_ids.device)\n",
    "            except Exception as e:\n",
    "                # Fallback to standard loss if BERTScore fails\n",
    "                bert_loss = outputs.loss\n",
    "            finally:\n",
    "                # Disable cache again for gradient checkpointing compatibility\n",
    "                model.config.use_cache = False\n",
    "        \n",
    "        # Combine with standard language modeling loss\n",
    "        standard_loss = outputs.loss\n",
    "        combined_loss = 0.7 * standard_loss + 0.3 * bert_loss\n",
    "        \n",
    "        return (combined_loss, outputs) if return_outputs else combined_loss\n",
    "\n",
    "# Data preparation function - removed tokenizer parameter since it's not used\n",
    "def prepare_training_data(tokenized_dataset):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    \n",
    "    def add_labels(example):\n",
    "        example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "        return example\n",
    "\n",
    "    # Only prepare train split\n",
    "    train_dataset = tokenized_dataset[\"train\"].map(add_labels)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    keep_keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    train_dataset = train_dataset.remove_columns(\n",
    "        [col for col in train_dataset.column_names if col not in keep_keys]\n",
    "    )\n",
    "    \n",
    "    return {\"train\": train_dataset}\n",
    "\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    \"\"\"Remove checkpoint directories and files\"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        # Find all checkpoint directories\n",
    "        checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith('checkpoint-')]\n",
    "        \n",
    "        for checkpoint_dir in checkpoint_dirs:\n",
    "            checkpoint_path = os.path.join(output_dir, checkpoint_dir)\n",
    "            if os.path.isdir(checkpoint_path):\n",
    "                print(f\"üóëÔ∏è Removing checkpoint: {checkpoint_path}\")\n",
    "                shutil.rmtree(checkpoint_path)\n",
    "        \n",
    "        # Remove any other checkpoint-related files\n",
    "        checkpoint_files = [f for f in os.listdir(output_dir) if 'checkpoint' in f.lower()]\n",
    "        for checkpoint_file in checkpoint_files:\n",
    "            file_path = os.path.join(output_dir, checkpoint_file)\n",
    "            if os.path.isfile(file_path):\n",
    "                print(f\"üóëÔ∏è Removing checkpoint file: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        print(\"‚úÖ Checkpoint cleanup completed!\")\n",
    "\n",
    "def configure_model_for_training(model):\n",
    "    \"\"\"Configure model for training with proper cache settings\"\"\"\n",
    "    \n",
    "    # Disable use_cache for training compatibility with gradient checkpointing\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = False\n",
    "        print(\"‚úÖ Set use_cache=False for gradient checkpointing compatibility\")\n",
    "    \n",
    "    # Enable gradient checkpointing if available\n",
    "    if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"‚úÖ Enabled gradient checkpointing for memory efficiency\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main training function\n",
    "def train_model(model, tokenized_data, tokenizer, train_args):\n",
    "    \"\"\"Training function with BERTScore and early stopping\"\"\"\n",
    "    \n",
    "    # Configure model for training\n",
    "    model = configure_model_for_training(model)\n",
    "    \n",
    "    # Prepare data\n",
    "    prepared_data = prepare_training_data(tokenized_data)\n",
    "    \n",
    "    # Setup data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Custom early stopping based on training loss\n",
    "    early_stopping_callback = TrainingLossEarlyStoppingCallback(\n",
    "        patience=5,\n",
    "        min_delta=0.01\n",
    "    )\n",
    "    \n",
    "    # Initialize BERTScore Trainer\n",
    "    trainer = BERTScoreTrainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=prepared_data[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[early_stopping_callback],\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training with BERTScore optimization...\")\n",
    "    print(\"Early stopping based on training loss improvement\")\n",
    "    print(\"Cache disabled for gradient checkpointing compatibility\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Re-enable cache for inference after training\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = True\n",
    "        print(\"‚úÖ Re-enabled use_cache for inference\")\n",
    "    \n",
    "    # Save model\n",
    "    final_model_path = \"./phi3-squad2-final\"\n",
    "    trainer.save_model(final_model_path)\n",
    "    print(f\"‚úÖ Model saved to {final_model_path}\")\n",
    "    \n",
    "    # Clean up checkpoints after saving the final model\n",
    "    print(\"\\nüßπ Cleaning up checkpoints...\")\n",
    "    if hasattr(train_args, 'output_dir') and train_args.output_dir:\n",
    "        cleanup_checkpoints(train_args.output_dir)\n",
    "    \n",
    "    # Also clean up from the final model directory if it has checkpoints\n",
    "    cleanup_checkpoints(final_model_path)\n",
    "    \n",
    "    # Clean up any checkpoint directories in the current working directory\n",
    "    current_dir_checkpoints = [d for d in os.listdir('.') if d.startswith('checkpoint-')]\n",
    "    for checkpoint_dir in current_dir_checkpoints:\n",
    "        if os.path.isdir(checkpoint_dir):\n",
    "            print(f\"üóëÔ∏è Removing checkpoint: {checkpoint_dir}\")\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    print(\"üéâ Training completed and checkpoints cleaned up!\")\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f992b",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "604acdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", num_examples=None):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation on test set with detailed prediction examples\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING TEST SET EVALUATION WITH EXAMPLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare test data using make_prompt function\n",
    "    print(\"Preparing test prompts...\")\n",
    "    test_prompts = dataset[\"test\"].map(make_prompt)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists for predictions and references\n",
    "    preds = []\n",
    "    refs = []\n",
    "    raw_outputs = []\n",
    "    prompts_list = []\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    \n",
    "    # Limit examples if specified\n",
    "    if num_examples:\n",
    "        test_prompts = test_prompts.select(range(min(num_examples, len(test_prompts))))\n",
    "    \n",
    "    print(f\"Generating predictions for {len(test_prompts)} test examples...\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    for example in tqdm(test_prompts, desc=\"Evaluating\"):\n",
    "        # Get prompt without answer (remove answer part from make_prompt output)\n",
    "        full_prompt = example[\"prompt\"]\n",
    "        if '[/INST]' in full_prompt:\n",
    "            prompt_without_answer = full_prompt.split('[/INST]')[0] + '[/INST]'\n",
    "        else:\n",
    "            prompt_without_answer = full_prompt\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            prompt_without_answer,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=50,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer (everything after [/INST])\n",
    "        if '[/INST]' in decoded:\n",
    "            answer = decoded.split('[/INST]')[-1].strip()\n",
    "        else:\n",
    "            answer = decoded.strip()\n",
    "        \n",
    "        # Store results\n",
    "        preds.append(answer)\n",
    "        refs.append(example[\"reference\"])\n",
    "        raw_outputs.append(decoded)\n",
    "        prompts_list.append(example[\"prompt\"])\n",
    "        \n",
    "        # Extract question and context for detailed analysis\n",
    "        if \"Question:\" in example[\"prompt\"]:\n",
    "            question_part = example[\"prompt\"].split(\"Question:\")[-1].split(\"[/INST]\")[0].strip()\n",
    "            questions.append(question_part)\n",
    "        if \"Context:\" in example[\"prompt\"]:\n",
    "            context_part = example[\"prompt\"].split(\"Context:\")[-1].split(\"Question:\")[0].strip()\n",
    "            contexts.append(context_part[:200] + \"...\" if len(context_part) > 200 else context_part)\n",
    "    \n",
    "    print(\"Predictions generated! Computing metrics...\")\n",
    "    \n",
    "    # Compute BERTScore using silent function from your notebook\n",
    "    print(\"Computing BERTScore...\")\n",
    "    try:\n",
    "        P, R, F1 = silent_bert_score(preds, refs, lang=\"en\")\n",
    "        bert_scores = {\n",
    "            \"precision\": P.mean().item(),\n",
    "            \"recall\": R.mean().item(),\n",
    "            \"f1\": F1.mean().item()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"BERTScore computation failed: {e}\")\n",
    "        bert_scores = {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "        P = R = F1 = [0.0] * len(preds)\n",
    "    \n",
    "    # Compute exact match accuracy\n",
    "    exact_matches = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        if ref != \"No answer\" and ref.lower().strip() in pred.lower().strip():\n",
    "            exact_matches.append(1)\n",
    "        else:\n",
    "            exact_matches.append(0)\n",
    "    \n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    \n",
    "    # Compute F1 score (token overlap)\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = set(pred.lower().split())\n",
    "        ref_tokens = set(ref.lower().split())\n",
    "        \n",
    "        if len(pred_tokens) == 0 and len(ref_tokens) == 0:\n",
    "            f1_scores.append(1.0)\n",
    "        elif len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "        else:\n",
    "            common = len(pred_tokens & ref_tokens)\n",
    "            precision = common / len(pred_tokens)\n",
    "            recall = common / len(ref_tokens)\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            f1_scores.append(f1)\n",
    "    \n",
    "    f1_score = np.mean(f1_scores)\n",
    "    \n",
    "    # Compute semantic similarity (simple word overlap)\n",
    "    semantic_similarities = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_words = set(pred.lower().split())\n",
    "        ref_words = set(ref.lower().split())\n",
    "        if len(pred_words) == 0 and len(ref_words) == 0:\n",
    "            semantic_similarities.append(1.0)\n",
    "        elif len(pred_words | ref_words) == 0:\n",
    "            semantic_similarities.append(0.0)\n",
    "        else:\n",
    "            jaccard = len(pred_words & ref_words) / len(pred_words | ref_words)\n",
    "            semantic_similarities.append(jaccard)\n",
    "    \n",
    "    semantic_similarity = np.mean(semantic_similarities)\n",
    "    \n",
    "    # Compute answer length statistics\n",
    "    pred_lengths = [len(pred.split()) for pred in preds]\n",
    "    ref_lengths = [len(ref.split()) for ref in refs]\n",
    "    \n",
    "    # Print comprehensive results\n",
    "    print(\"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Test Set Size: {len(preds)}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"BERTScore Metrics:\")\n",
    "    print(f\"  Precision: {bert_scores['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {bert_scores['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {bert_scores['f1']:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Other Metrics:\")\n",
    "    print(f\"  Exact Match: {exact_match_score:.4f}\")\n",
    "    print(f\"  F1 Score:    {f1_score:.4f}\")\n",
    "    print(f\"  Semantic Similarity: {semantic_similarity:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Answer Length Statistics:\")\n",
    "    print(f\"  Avg Prediction Length: {np.mean(pred_lengths):.2f} words\")\n",
    "    print(f\"  Avg Reference Length:  {np.mean(ref_lengths):.2f} words\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show detailed examples\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED PREDICTION EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select diverse examples: best, worst, and random\n",
    "    bert_f1_scores = [f.item() if hasattr(f, 'item') else f for f in F1]\n",
    "    \n",
    "    # Get indices for different categories\n",
    "    sorted_indices = sorted(range(len(bert_f1_scores)), key=lambda i: bert_f1_scores[i], reverse=True)\n",
    "    \n",
    "    best_indices = sorted_indices[:3]  # Top 3\n",
    "    worst_indices = sorted_indices[-3:]  # Bottom 3\n",
    "    random_indices = random.sample(range(len(preds)), min(4, len(preds)))  # Random 4\n",
    "    \n",
    "    example_categories = [\n",
    "        (\"BEST PREDICTIONS\", best_indices),\n",
    "        (\"WORST PREDICTIONS\", worst_indices),\n",
    "        (\"RANDOM PREDICTIONS\", random_indices)\n",
    "    ]\n",
    "    \n",
    "    for category_name, indices in example_categories:\n",
    "        print(f\"\\n{category_name}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "            print(f\"BERTScore F1: {bert_f1_scores[idx]:.4f}\")\n",
    "            print(f\"Token F1: {f1_scores[idx]:.4f}\")\n",
    "            print(f\"Exact Match: {'‚úì' if exact_matches[idx] else '‚úó'}\")\n",
    "            \n",
    "            if idx < len(questions):\n",
    "                print(f\"Question: {questions[idx]}\")\n",
    "            if idx < len(contexts):\n",
    "                print(f\"Context: {contexts[idx]}\")\n",
    "            \n",
    "            print(f\"Reference Answer: {refs[idx]}\")\n",
    "            print(f\"Model Prediction: {preds[idx]}\")\n",
    "            \n",
    "            # Analysis\n",
    "            pred_words = len(preds[idx].split())\n",
    "            ref_words = len(refs[idx].split())\n",
    "            print(f\"Length: Pred={pred_words} words, Ref={ref_words} words\")\n",
    "            \n",
    "            # Simple similarity check\n",
    "            pred_lower = preds[idx].lower()\n",
    "            ref_lower = refs[idx].lower()\n",
    "            common_words = set(pred_lower.split()) & set(ref_lower.split())\n",
    "            print(f\"Common words: {len(common_words)}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Create results dictionary matching your expected format\n",
    "    results = {\n",
    "        \"test_size\": len(preds),\n",
    "        \"exact_match\": exact_match_score,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"bert_score_f1\": bert_scores[\"f1\"],\n",
    "        \"semantic_similarity\": semantic_similarity,\n",
    "        \"avg_prediction_length\": np.mean(pred_lengths),\n",
    "        \"avg_reference_length\": np.mean(ref_lengths),\n",
    "        \"predictions\": preds,\n",
    "        \"references\": refs,\n",
    "        \"questions\": questions,\n",
    "        \"individual_scores\": {\n",
    "            \"bert_f1\": bert_f1_scores,\n",
    "            \"token_f1\": f1_scores,\n",
    "            \"exact_match\": exact_matches,\n",
    "            \"semantic_similarity\": semantic_similarities\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save detailed results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save detailed examples\n",
    "    with open(\"detailed_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"DETAILED TEST SET PREDICTIONS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, (prompt, pred, ref, f1_score, em) in enumerate(zip(prompts_list, preds, refs, f1_scores, exact_matches)):\n",
    "            f.write(f\"Example {i+1}:\\n\")\n",
    "            f.write(f\"BERTScore F1: {f1_score:.4f}\\n\")\n",
    "            f.write(f\"Exact Match: {'‚úì' if em else '‚úó'}\\n\")\n",
    "            \n",
    "            if \"Question:\" in prompt:\n",
    "                question = prompt.split(\"Question:\")[-1].split(\"Answer:\")[0].strip()\n",
    "                f.write(f\"Question: {question}\\n\")\n",
    "            else:\n",
    "                f.write(f\"Prompt: {prompt}\\n\")\n",
    "            \n",
    "            f.write(f\"Reference: {ref}\\n\")\n",
    "            f.write(f\"Prediction: {pred}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    print(\"Results saved to:\")\n",
    "    print(\"  - test_evaluation_results.json\")\n",
    "    print(\"  - detailed_predictions.txt\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e592cc",
   "metadata": {},
   "source": [
    "## Iterative training and dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e47ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_answers(model, tokenizer, formatted_dataset, device, generation_num=1):\n",
    "    \"\"\"\n",
    "    Generate synthetic answers using the fine-tuned causal language model.\n",
    "    Takes formatted_dataset with prompts as input.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Generating synthetic answers (Generation {generation_num})...\")\n",
    "    \n",
    "    synthetic_data = []\n",
    "    \n",
    "    # Use the train split from formatted_dataset\n",
    "    train_dataset = formatted_dataset['train']\n",
    "    \n",
    "    for example in tqdm(train_dataset, desc=\"Generating answers\"):\n",
    "        # Get the prompt that was created by make_prompt function\n",
    "        prompt = example['prompt']\n",
    "        \n",
    "        # Find where the prompt ends to extract the incomplete part\n",
    "        # Assuming the prompt format ends with something like \"[/INST]\" or \"### Response:\"\n",
    "        if '[/INST]' in prompt:\n",
    "            # For instruction format, generate after [/INST]\n",
    "            generation_prompt = prompt\n",
    "            stop_sequence = '[/INST]'\n",
    "        elif '### Response:' in prompt:\n",
    "            # For alpaca format, generate after ### Response:\n",
    "            generation_prompt = prompt\n",
    "            stop_sequence = '### Response:'\n",
    "        else:\n",
    "            # Fallback: use the full prompt\n",
    "            generation_prompt = prompt\n",
    "            stop_sequence = None\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            generation_prompt,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate answer using causal LM\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,  # Increased for better answers\n",
    "                do_sample=True,      # Changed to True for diversity\n",
    "                temperature=0.7,     # Added temperature for controlled randomness\n",
    "                top_p=0.9,          # Added nucleus sampling\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the new generated part (answer)\n",
    "        if stop_sequence and stop_sequence in generated_text:\n",
    "            # Split by the stop sequence and take everything after it\n",
    "            parts = generated_text.split(stop_sequence)\n",
    "            if len(parts) > 1:\n",
    "                synthetic_answer = parts[-1].strip()\n",
    "            else:\n",
    "                synthetic_answer = \"No answer found\"\n",
    "        else:\n",
    "            # If no stop sequence, take everything after the original prompt\n",
    "            if generated_text.startswith(generation_prompt):\n",
    "                synthetic_answer = generated_text[len(generation_prompt):].strip()\n",
    "            else:\n",
    "                synthetic_answer = generated_text.strip()\n",
    "        \n",
    "        # Clean up the answer\n",
    "        if not synthetic_answer or synthetic_answer == generation_prompt:\n",
    "            synthetic_answer = \"No answer found\"\n",
    "        \n",
    "        # Create new example with synthetic answer\n",
    "        new_example = example.copy()\n",
    "        \n",
    "        # Update the prompt to include the generated answer\n",
    "        if stop_sequence:\n",
    "            new_example['prompt'] = generation_prompt + synthetic_answer\n",
    "        else:\n",
    "            new_example['prompt'] = generation_prompt + \" \" + synthetic_answer\n",
    "        \n",
    "        # If original data has structured fields, preserve them and update answers\n",
    "        if 'answers' in example:\n",
    "            if synthetic_answer != \"No answer found\":\n",
    "                # Try to find answer in context if context exists\n",
    "                context = example.get('context', '')\n",
    "                answer_start = context.find(synthetic_answer) if context else 0\n",
    "                if answer_start == -1:\n",
    "                    answer_start = 0\n",
    "                \n",
    "                new_example['answers'] = {\n",
    "                    'text': [synthetic_answer],\n",
    "                    'answer_start': [answer_start]\n",
    "                }\n",
    "            else:\n",
    "                new_example['answers'] = {\n",
    "                    'text': [],\n",
    "                    'answer_start': []\n",
    "                }\n",
    "        \n",
    "        # Add generation metadata\n",
    "        new_example['generation_num'] = generation_num\n",
    "        new_example['synthetic'] = True\n",
    "        \n",
    "        synthetic_data.append(new_example)\n",
    "    \n",
    "    # Create a new formatted dataset with the synthetic data\n",
    "    synthetic_dataset = Dataset.from_list(synthetic_data)\n",
    "    \n",
    "    # Return in the same format as input\n",
    "    return {\n",
    "        'train': synthetic_dataset\n",
    "    }\n",
    "\n",
    "\n",
    "def save_synthetic_dataset(dataset_dir, synthetic_formatted_dataset, generation_num):\n",
    "    \"\"\"\n",
    "    Save the synthetic formatted dataset to a new subdirectory.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Base dataset directory\n",
    "        synthetic_formatted_dataset: Formatted dataset with synthetic answers\n",
    "        generation_num: Generation number\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create new subdirectory\n",
    "    new_dir = os.path.join(dataset_dir, f\"generation_{generation_num}\")\n",
    "    os.makedirs(new_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the train split\n",
    "    synthetic_formatted_dataset['train'].save_to_disk(os.path.join(new_dir, \"train\"))\n",
    "    \n",
    "    # Also save as JSON for inspection\n",
    "    json_file = os.path.join(new_dir, \"synthetic_data.json\")\n",
    "    synthetic_formatted_dataset['train'].to_json(json_file)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"generation_number\": generation_num,\n",
    "        \"total_examples\": len(synthetic_formatted_dataset['train']),\n",
    "        \"generated_from\": \"fine_tuned_model\",\n",
    "        \"description\": f\"Synthetic answers generated using fine-tuned model (Generation {generation_num})\",\n",
    "        \"format\": \"formatted_dataset_with_prompts\"\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(new_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved synthetic formatted dataset to {new_dir}\")\n",
    "    return new_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388194b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_training_and_generation(\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    model_path = \"microsoft/phi-3-mini-128k-instruct\",\n",
    "    num_generations=3,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform iterative training and synthetic data generation.\n",
    "    \n",
    "    Args:\n",
    "        base_dataset_dir: Directory containing the original dataset\n",
    "        num_generations: Number of generations to create\n",
    "        device: Device for inference\n",
    "    \"\"\"\n",
    "    \n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer once at the beginning\n",
    "    base_model_name = \"microsoft/phi-3-mini-128k-instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Define tokenization function\n",
    "    def tokenize(example):\n",
    "        return tokenizer(\n",
    "            example[\"prompt\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256\n",
    "        )\n",
    "    \n",
    "    print(f\"Starting iterative training and generation for {num_generations} generations...\")\n",
    "    \n",
    "    # Load original dataset\n",
    "    dataset = None\n",
    "    formatted_dataset = None\n",
    "    \n",
    "    # Main progress bar for generations\n",
    "    generation_progress = tqdm(\n",
    "        range(1, num_generations + 1), \n",
    "        desc=\"üîÑ Overall Progress\", \n",
    "        unit=\"generation\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    for generation in generation_progress:\n",
    "        generation_progress.set_description(f\"üîÑ Generation {generation}/{num_generations}\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"GENERATION {generation}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Step 1: Fine-tune model with current training dataset\n",
    "        print(\"Step 1: Fine-tuning model...\")\n",
    "        \n",
    "        # Prepare training arguments for this generation\n",
    "        train_config = {\n",
    "            \"bf16\": True,\n",
    "            \"do_eval\": False,  # Disable evaluation completely\n",
    "            \"learning_rate\": 1.0e-05,\n",
    "            \"log_level\": \"info\",\n",
    "            \"logging_steps\": 10,\n",
    "            \"logging_strategy\": \"steps\",\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"num_train_epochs\": 3,\n",
    "            \"max_steps\": -1,\n",
    "            \"output_dir\": f\"./phi3-squad2-gen{generation}\",  # Update for each generation\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"per_device_train_batch_size\": 4,\n",
    "            \"remove_unused_columns\": True,\n",
    "            \"save_steps\": 50,\n",
    "            \"save_total_limit\": 2,\n",
    "            \"seed\": 42,\n",
    "            \"gradient_checkpointing\": True,\n",
    "            \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "            \"warmup_ratio\": 0.05,\n",
    "            \"save_strategy\": \"steps\",\n",
    "            \"load_best_model_at_end\": False,  # No evaluation, so no \"best\" model\n",
    "            \"disable_tqdm\": False,  # Enable tqdm progress bars for training\n",
    "        }\n",
    "\n",
    "        train_args = TrainingArguments(**train_config)\n",
    "        \n",
    "        # Load model for this generation\n",
    "        print(\"üì• Loading model...\")\n",
    "        with tqdm(total=1, desc=\"ü§ñ Model Loading\", position=1, leave=False) as model_pbar:\n",
    "            if generation == 1:\n",
    "                # First generation: load base model with quantization\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    base_model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    quantization_config=bnb_config\n",
    "                )\n",
    "                # Prepare dataset for first generation\n",
    "                dataset = load_squad_subset()\n",
    "                formatted_dataset = {\n",
    "                    split: dataset[split].map(make_prompt)\n",
    "                    for split in dataset.keys()\n",
    "                }\n",
    "            else:\n",
    "                # Subsequent generations: load previous model (already fine-tuned, no quantization needed)\n",
    "                model_path = f\"./phi3-squad2-gen{generation-1}-final\"\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "                # Prepare dataset for subsequent generations\n",
    "                dataset = load_squad_subset(f\"train_{generation-1}_gen\")\n",
    "                formatted_dataset = {\n",
    "                    split: dataset[split].map(make_prompt)\n",
    "                    for split in formatted_dataset.keys()\n",
    "                }\n",
    "\n",
    "            model_pbar.update(1)\n",
    "        \n",
    "        # Apply PEFT configuration\n",
    "        print(\"üîß Applying PEFT configuration...\")\n",
    "        with tqdm(total=1, desc=\"‚öôÔ∏è PEFT Setup\", position=1, leave=False) as peft_pbar:\n",
    "            peft_config = {\n",
    "                \"r\": 8,  # Reduced from 16 to 8 (fewer parameters)\n",
    "                \"lora_alpha\": 16,  # Reduced from 32 to 16\n",
    "                \"lora_dropout\": 0.1,  # Slightly increased dropout\n",
    "                \"bias\": \"none\",\n",
    "                \"task_type\": \"CAUSAL_LM\",\n",
    "                \"target_modules\": \"all-linear\",\n",
    "                \"modules_to_save\": None,\n",
    "            }\n",
    "            lora_config = LoraConfig(**peft_config)\n",
    "            model = get_peft_model(model, lora_config)\n",
    "            peft_pbar.update(1)\n",
    "        \n",
    "        # Tokenize current training dataset\n",
    "        print(\"üî§ Tokenizing dataset...\")\n",
    "        tokenized = {\n",
    "            split: formatted_dataset[split].map(tokenize, batched=True)\n",
    "            for split in formatted_dataset.keys()\n",
    "        }\n",
    "        \n",
    "        # Fine-tune the model\n",
    "        print(\"üöÄ Starting training...\")\n",
    "        trainer = train_model(model, tokenized, tokenizer, train_args)\n",
    "        \n",
    "        # Save the fine-tuned model for this generation\n",
    "        print(\"üíæ Saving model...\")\n",
    "        with tqdm(total=1, desc=\"üíæ Saving Model\", position=1, leave=False) as save_pbar:\n",
    "            final_model_path = f\"./phi3-squad2-gen{generation}-final\"\n",
    "            trainer.save_model(final_model_path)\n",
    "            save_pbar.update(1)\n",
    "        print(f\"‚úÖ Generation {generation} model saved to {final_model_path}\")\n",
    "        \n",
    "        # Step 2: Generate synthetic answers using the fine-tuned model\n",
    "        print(\"Step 2: Generating synthetic answers...\")\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        synthetic_dataset = generate_synthetic_answers(\n",
    "            model, tokenizer, formatted_dataset, device, generation\n",
    "        )\n",
    "        \n",
    "        # Step 3: Save synthetic dataset\n",
    "        print(\"Step 3: Saving synthetic dataset...\")\n",
    "        with tqdm(total=1, desc=\"üíæ Saving Dataset\", position=1, leave=False) as dataset_save_pbar:\n",
    "            new_dir = save_synthetic_dataset(base_dataset_dir, synthetic_dataset, generation)\n",
    "            dataset_save_pbar.update(1)\n",
    "        \n",
    "        # Step 4: Evaluate on test set\n",
    "        print(\"Step 4: Evaluating on test set...\")\n",
    "        test_dataset = dataset['test']\n",
    "        \n",
    "        # Use the existing evaluation function\n",
    "        with tqdm(total=1, desc=\"üìä Evaluating\", position=1, leave=False) as eval_pbar:\n",
    "            evaluation_results = evaluate_model(model, tokenizer, test_dataset, device)\n",
    "            eval_pbar.update(1)\n",
    "        \n",
    "        print(f\"üìä Generation {generation} Evaluation Results:\")\n",
    "        print(f\"   Exact Match: {evaluation_results['exact_match']:.3f}\")\n",
    "        print(f\"   F1 Score: {evaluation_results['f1_score']:.3f}\")\n",
    "        print(f\"   BERTScore F1: {evaluation_results['bert_score_f1']:.3f}\")\n",
    "        print(f\"   Semantic Similarity: {evaluation_results['semantic_similarity']:.3f}\")\n",
    "        \n",
    "        # Update current training dataset for next iteration\n",
    "        current_train_dataset = synthetic_dataset\n",
    "        \n",
    "        print(f\"Generation {generation} completed!\")\n",
    "        print(f\"Synthetic dataset saved to: {new_dir}\")\n",
    "        print(f\"Model saved to: {final_model_path}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Update generation progress\n",
    "        generation_progress.set_postfix({\n",
    "            'EM': f\"{evaluation_results['exact_match']:.3f}\",\n",
    "            'F1': f\"{evaluation_results['f1_score']:.3f}\"\n",
    "        })\n",
    "    \n",
    "    generation_progress.close()\n",
    "    print(f\"\\nüéâ All {num_generations} generations completed!\")\n",
    "    print(\"Final models and datasets are ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50ee1940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iterative training and generation for 3 generations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generation 1/3:   0%|          | 0/3 [00:00<?, ?generation/s]  PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GENERATION 1\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n",
      "üì• Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b490ad722d94a07a4ef8939c1b7571b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.norm.weight, model.embed_tokens.weight, model.layers.*.post_attention_layernorm.weight, lm_head.weight, model.layers.*.input_layernorm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from squad_v2_05percent...\n",
      "Dataset metadata:\n",
      "  original_train_size: 130319\n",
      "  original_validation_size: 11873\n",
      "  extracted_train_size: 651\n",
      "  extracted_test_size: 59\n",
      "  extraction_percentage: 0.5\n",
      "\n",
      "Loaded dataset splits:\n",
      "  train: 651 examples\n",
      "  test: 59 examples\n",
      "üîß Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenizing dataset...\n",
      "üöÄ Starting training...\n",
      "‚úÖ Set use_cache=False for gradient checkpointing compatibility\n",
      "‚úÖ Enabled gradient checkpointing for memory efficiency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 651\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 246\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/246 18:59 < 18:22, 0.11 it/s, Epoch 1.53/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.868100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.809500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.008000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen1\\checkpoint-50\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen1\\checkpoint-100\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "The following layers were not sharded: pooler.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.output.LayerNorm.bias, pooler.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias\n",
      "üîÑ Generation 1/3:   0%|          | 0/3 [19:14<?, ?generation/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43miterative_training_and_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_dataset_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquad_v2_05percent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_generations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 149\u001b[0m, in \u001b[0;36miterative_training_and_generation\u001b[1;34m(base_dataset_dir, num_generations, device)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Starting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 149\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model for this generation\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müíæ Saving model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 202\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, tokenized_data, tokenizer, train_args)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping based on training loss improvement\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCache disabled for gradient checkpointing compatibility\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 202\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Re-enable cache for inference after training\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2553\u001b[0m )\n\u001b[0;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2561\u001b[0m ):\n\u001b[0;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\trainer.py:3745\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3744\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3745\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3747\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3750\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3751\u001b[0m ):\n",
      "Cell \u001b[1;32mIn[9], line 76\u001b[0m, in \u001b[0;36mBERTScoreTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     generated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Decode predictions and references\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     pred_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class\u001b[38;5;241m.\u001b[39mbatch_decode(generated, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\peft\\peft_model.py:1875\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1873\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1874\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1875\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1877\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\generation\\utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2594\u001b[0m     )\n\u001b[0;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2598\u001b[0m         input_ids,\n\u001b[0;32m   2599\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2600\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2601\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2602\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2603\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2604\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2605\u001b[0m     )\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2614\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\generation\\utils.py:3560\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3560\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3562\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3563\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3564\u001b[0m     outputs,\n\u001b[0;32m   3565\u001b[0m     model_kwargs,\n\u001b[0;32m   3566\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3567\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:745\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    740\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    741\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    742\u001b[0m )\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    746\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    747\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    748\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    749\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    750\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    751\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    752\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    753\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    754\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    756\u001b[0m )\n\u001b[0;32m    758\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    759\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:478\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    476\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 478\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    479\u001b[0m     hidden_states,\n\u001b[0;32m    480\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    481\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    482\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    483\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    484\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    485\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    486\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    488\u001b[0m )\n\u001b[0;32m    490\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\modeling_layers.py:47\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\_dynamo\\external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\utils\\checkpoint.py:489\u001b[0m, in \u001b[0;36mcheckpoint\u001b[1;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# Runs pre-forward logic\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mnext\u001b[39m(gen)\n\u001b[1;32m--> 489\u001b[0m ret \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# Runs post-forward logic\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:299\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    300\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    301\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    302\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    303\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    304\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    305\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    306\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    307\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    309\u001b[0m )\n\u001b[0;32m    310\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(hidden_states)  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[0;32m    312\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:182\u001b[0m, in \u001b[0;36mPhi3Attention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    180\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m--> 182\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m query_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_attention_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[0;32m    184\u001b[0m query_states \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :query_pos]\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:494\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:496\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    492\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m    494\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m--> 496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bnb\u001b[38;5;241m.\u001b[39mmatmul_4bit(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, bias\u001b[38;5;241m=\u001b[39mbias, quant_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mquant_state)\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:298\u001b[0m, in \u001b[0;36mParams4bit.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterative_training_and_generation(\n",
    "        base_dataset_dir=\"squad_v2_05percent\",\n",
    "        num_generations=3,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dfefd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
