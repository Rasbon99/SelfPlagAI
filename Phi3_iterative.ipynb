{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a919dc6",
   "metadata": {},
   "source": [
    "# Phi-3 Iterative Try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4babf",
   "metadata": {},
   "source": [
    "This notebook is used to try the iterative pipeline without the integration of MongoDB to store the versioning of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844840d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbc3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from bert_score import score\n",
    "from datasets import Dataset, load_from_disk, DatasetDict\n",
    "from dotenv import load_dotenv\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb166e",
   "metadata": {},
   "source": [
    "### Dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdc10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad_subset(dataset_dir=\"squad_v2_05percent\"):\n",
    "    \"\"\"\n",
    "    Load the saved SQuAD v2 0.5% dataset from disk.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir (str): Directory where the dataset was saved\n",
    "        \n",
    "    Returns:\n",
    "        DatasetDict: The loaded dataset with train and test splits\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{dataset_dir}' not found. Please run the extraction script first.\")\n",
    "    \n",
    "    print(f\"Loading dataset from {dataset_dir}...\")\n",
    "    \n",
    "    # Load the dataset using Hugging Face datasets\n",
    "    dataset = load_from_disk(dataset_dir)\n",
    "    \n",
    "    # Load and display metadata\n",
    "    metadata_path = os.path.join(dataset_dir, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"Dataset metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"\\nLoaded dataset splits:\")\n",
    "    for split_name, split_data in dataset.items():\n",
    "        print(f\"  {split_name}: {len(split_data)} examples\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ce1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_only(dataset_dir):\n",
    "    \"\"\"\n",
    "    Load only the train split from a saved dataset directory.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir (str): Directory where the dataset was saved\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: The loaded train dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{dataset_dir}' not found.\")\n",
    "    \n",
    "    print(f\"Loading train dataset from {dataset_dir}...\")\n",
    "    \n",
    "    # Try to load the full dataset first\n",
    "    try:\n",
    "        full_dataset = load_from_disk(dataset_dir)\n",
    "        \n",
    "        # Check if it's a DatasetDict with train split\n",
    "        if isinstance(full_dataset, dict) and 'train' in full_dataset:\n",
    "            train_dataset = full_dataset['train']\n",
    "        elif hasattr(full_dataset, 'column_names'):\n",
    "            # It's already a single Dataset\n",
    "            train_dataset = full_dataset\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected dataset format in {dataset_dir}\")\n",
    "            \n",
    "    except Exception:\n",
    "        # Try to load from train subdirectory if full dataset fails\n",
    "        train_dir = os.path.join(dataset_dir, \"train\")\n",
    "        if os.path.exists(train_dir):\n",
    "            train_dataset = load_from_disk(train_dir)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No train dataset found in {dataset_dir}\")\n",
    "    \n",
    "    # Load and display metadata if available\n",
    "    metadata_path = os.path.join(dataset_dir, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"Dataset metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Display train dataset info\n",
    "    print(f\"\\nLoaded train dataset:\")\n",
    "    print(f\"  Examples: {len(train_dataset)}\")\n",
    "    print(f\"  Columns: {train_dataset.column_names}\")\n",
    "    \n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fad3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from squad_v2_01percent...\n",
      "Dataset metadata:\n",
      "  original_train_size: 130319\n",
      "  original_validation_size: 11873\n",
      "  extracted_train_size: 130\n",
      "  extracted_test_size: 11\n",
      "  extraction_percentage: 0.1\n",
      "  sampling_method: random\n",
      "  seed: 42\n",
      "  train_answerable: 94\n",
      "  test_answerable: 3\n",
      "  train_answerable_percentage: 72.3076923076923\n",
      "  test_answerable_percentage: 27.27272727272727\n",
      "  dataset_format: squad_v2\n",
      "  splits: ['train', 'test']\n",
      "\n",
      "Loaded dataset splits:\n",
      "  train: 130 examples\n",
      "  test: 11 examples\n",
      "\n",
      "Example from each split:\n",
      "Train: What century did Nasser rule in?\n",
      "Test: How many State of California University campuses are there?\n",
      "\n",
      "Dataset features: {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_squad_subset(\"squad_v2_01percent\")\n",
    "    \n",
    "# Show examples\n",
    "print(f\"\\nExample from each split:\")\n",
    "print(f\"Train: {dataset['train'][0]['question']}\")\n",
    "print(f\"Test: {dataset['test'][0]['question']}\")\n",
    "\n",
    "# Access specific fields\n",
    "print(f\"\\nDataset features: {dataset['train'].features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c67d23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to format the dataset examples into a prompt\n",
    "#The prompt will include the context, question, and answer\n",
    "def make_prompt(example):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"No answer\"\n",
    "\n",
    "    prompt = f\"[INST] Given the context, answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question} [/INST] {answer}\"\n",
    "    return {\"prompt\": prompt, \"reference\": answer}\n",
    "\n",
    "formatted_dataset = {\n",
    "    split: dataset[split].map(make_prompt)\n",
    "    for split in dataset.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5fdf7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " [INST] Given the context, answer the question.\n",
      "\n",
      "Context: Nasser remains an iconic figure in the Arab world, particularly for his strides towards social justice and Arab unity, modernization policies, and anti-imperialist efforts. His presidency also encouraged and coincided with an Egyptian cultural boom, and launched large industrial projects, including the Aswan Dam and Helwan City. Nasser's detractors criticize his authoritarianism, his government's human rights violations, his populist relationship with the citizenry, and his failure to establish civil institutions, blaming his legacy for future dictatorial governance in Egypt. Historians describe Nasser as a towering political figure of the Middle East in the 20th century.\n",
      "\n",
      "Question: What century did Nasser rule in? [/INST] 20th\n",
      "\n",
      "Reference Answer:\n",
      " 20th\n"
     ]
    }
   ],
   "source": [
    "# Print the first formatted prompt and its reference answer\n",
    "print(\"Prompt:\\n\", formatted_dataset[\"train\"][0][\"prompt\"])\n",
    "print(\"\\nReference Answer:\\n\", formatted_dataset[\"train\"][0][\"reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad29c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"key.env\")\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "889ec8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae51a83",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3828e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced logging suppression - including BERTScore sharding messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate.utils.modeling\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f52b2384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERTScore silently...\n",
      "BERTScore initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERTScore silently\n",
    "print(\"Initializing BERTScore silently...\")\n",
    "with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "    from bert_score import score\n",
    "    _ = score([\"test\"], [\"test\"], lang=\"en\", verbose=False)\n",
    "print(\"BERTScore initialized successfully!\")\n",
    "\n",
    "# Modified BERTScore function with complete output suppression\n",
    "def silent_bert_score(cands, refs, lang=\"en\"):\n",
    "    \"\"\"BERTScore calculation with all output suppressed\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    old_stderr = sys.stderr\n",
    "    \n",
    "    sys.stdout = io.StringIO()\n",
    "    sys.stderr = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        P, R, F1 = score(cands, refs, lang=lang, verbose=False)\n",
    "        return P, R, F1\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stderr = old_stderr\n",
    "\n",
    "# Custom Early Stopping based on Training Loss\n",
    "class TrainingLossEarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait_count = 0\n",
    "        \n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        if logs is not None and 'train_loss' in logs:\n",
    "            current_loss = logs['train_loss']\n",
    "            \n",
    "            if current_loss < self.best_loss - self.min_delta:\n",
    "                self.best_loss = current_loss\n",
    "                self.wait_count = 0\n",
    "                print(f\"📈 Training loss improved to {current_loss:.4f}\")\n",
    "            else:\n",
    "                self.wait_count += 1\n",
    "                print(f\"📊 No improvement in training loss ({self.wait_count}/{self.patience})\")\n",
    "                \n",
    "                if self.wait_count >= self.patience:\n",
    "                    print(f\"🛑 Early stopping triggered! Best loss: {self.best_loss:.4f}\")\n",
    "                    control.should_training_stop = True\n",
    "\n",
    "# Fixed Custom Trainer class with BERTScore loss\n",
    "class BERTScoreTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.label_names = [\"labels\"]\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Custom loss function using BERTScore - completely silent\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Temporarily disable cache for forward pass\n",
    "        model.config.use_cache = False\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Generate predictions for BERTScore\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Re-enable cache for generation\n",
    "            model.config.use_cache = True\n",
    "            \n",
    "            # Generate text\n",
    "            try:\n",
    "                generated = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.processing_class.eos_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "                \n",
    "                # Decode predictions and references\n",
    "                pred_texts = self.processing_class.batch_decode(generated, skip_special_tokens=True)\n",
    "                ref_texts = self.processing_class.batch_decode(labels, skip_special_tokens=True)\n",
    "                \n",
    "                # Calculate BERTScore with completely silent function\n",
    "                P, R, F1 = silent_bert_score(pred_texts, ref_texts, lang=\"en\")\n",
    "                bert_f1 = F1.mean().item()\n",
    "                \n",
    "                # Convert BERTScore to loss\n",
    "                bert_loss = torch.tensor(1.0 - bert_f1, requires_grad=True, device=input_ids.device)\n",
    "            except Exception as e:\n",
    "                # Fallback to standard loss if BERTScore fails\n",
    "                bert_loss = outputs.loss\n",
    "            finally:\n",
    "                # Disable cache again for gradient checkpointing compatibility\n",
    "                model.config.use_cache = False\n",
    "        \n",
    "        # Combine with standard language modeling loss\n",
    "        standard_loss = outputs.loss\n",
    "        combined_loss = 0.7 * standard_loss + 0.3 * bert_loss\n",
    "        \n",
    "        return (combined_loss, outputs) if return_outputs else combined_loss\n",
    "\n",
    "# Data preparation function - removed tokenizer parameter since it's not used\n",
    "def prepare_training_data(tokenized_dataset):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    \n",
    "    def add_labels(example):\n",
    "        example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "        return example\n",
    "\n",
    "    # Only prepare train split\n",
    "    train_dataset = tokenized_dataset[\"train\"].map(add_labels)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    keep_keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    train_dataset = train_dataset.remove_columns(\n",
    "        [col for col in train_dataset.column_names if col not in keep_keys]\n",
    "    )\n",
    "    \n",
    "    return {\"train\": train_dataset}\n",
    "\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    \"\"\"Remove checkpoint directories and files\"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        # Find all checkpoint directories\n",
    "        checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith('checkpoint-')]\n",
    "        \n",
    "        for checkpoint_dir in checkpoint_dirs:\n",
    "            checkpoint_path = os.path.join(output_dir, checkpoint_dir)\n",
    "            if os.path.isdir(checkpoint_path):\n",
    "                print(f\"🗑️ Removing checkpoint: {checkpoint_path}\")\n",
    "                shutil.rmtree(checkpoint_path)\n",
    "        \n",
    "        # Remove any other checkpoint-related files\n",
    "        checkpoint_files = [f for f in os.listdir(output_dir) if 'checkpoint' in f.lower()]\n",
    "        for checkpoint_file in checkpoint_files:\n",
    "            file_path = os.path.join(output_dir, checkpoint_file)\n",
    "            if os.path.isfile(file_path):\n",
    "                print(f\"🗑️ Removing checkpoint file: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        print(\"✅ Checkpoint cleanup completed!\")\n",
    "\n",
    "def configure_model_for_training(model):\n",
    "    \"\"\"Configure model for training with proper cache settings\"\"\"\n",
    "    \n",
    "    # Disable use_cache for training compatibility with gradient checkpointing\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = False\n",
    "        print(\"✅ Set use_cache=False for gradient checkpointing compatibility\")\n",
    "    \n",
    "    # Enable gradient checkpointing if available\n",
    "    if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"✅ Enabled gradient checkpointing for memory efficiency\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main training function\n",
    "def train_model(model, tokenized_data, tokenizer, train_args):\n",
    "    \"\"\"Training function with BERTScore and early stopping\"\"\"\n",
    "    \n",
    "    # Configure model for training\n",
    "    model = configure_model_for_training(model)\n",
    "    \n",
    "    # Prepare data\n",
    "    prepared_data = prepare_training_data(tokenized_data)\n",
    "    \n",
    "    # Setup data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Custom early stopping based on training loss\n",
    "    early_stopping_callback = TrainingLossEarlyStoppingCallback(\n",
    "        patience=5,\n",
    "        min_delta=0.01\n",
    "    )\n",
    "    \n",
    "    # Initialize BERTScore Trainer\n",
    "    trainer = BERTScoreTrainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=prepared_data[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[early_stopping_callback],\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training with BERTScore optimization...\")\n",
    "    print(\"Early stopping based on training loss improvement\")\n",
    "    print(\"Cache disabled for gradient checkpointing compatibility\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Re-enable cache for inference after training\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = True\n",
    "        print(\"✅ Re-enabled use_cache for inference\")\n",
    "    \n",
    "    # Save model\n",
    "    final_model_path = \"./phi3-squad2-final\"\n",
    "    trainer.save_model(final_model_path)\n",
    "    print(f\"✅ Model saved to {final_model_path}\")\n",
    "    \n",
    "    # Clean up checkpoints after saving the final model\n",
    "    print(\"\\n🧹 Cleaning up checkpoints...\")\n",
    "    if hasattr(train_args, 'output_dir') and train_args.output_dir:\n",
    "        cleanup_checkpoints(train_args.output_dir)\n",
    "    \n",
    "    # Also clean up from the final model directory if it has checkpoints\n",
    "    cleanup_checkpoints(final_model_path)\n",
    "    \n",
    "    # Clean up any checkpoint directories in the current working directory\n",
    "    current_dir_checkpoints = [d for d in os.listdir('.') if d.startswith('checkpoint-')]\n",
    "    for checkpoint_dir in current_dir_checkpoints:\n",
    "        if os.path.isdir(checkpoint_dir):\n",
    "            print(f\"🗑️ Removing checkpoint: {checkpoint_dir}\")\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    print(\"🎉 Training completed and checkpoints cleaned up!\")\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f992b",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604acdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", num_examples=None):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation on test set with detailed prediction examples\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING TEST SET EVALUATION WITH EXAMPLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare test data using make_prompt function\n",
    "    print(\"Preparing test prompts...\")\n",
    "    test_prompts = dataset.map(make_prompt)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists for predictions and references\n",
    "    preds = []\n",
    "    refs = []\n",
    "    raw_outputs = []\n",
    "    prompts_list = []\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    \n",
    "    # Limit examples if specified\n",
    "    if num_examples:\n",
    "        test_prompts = test_prompts.select(range(min(num_examples, len(test_prompts))))\n",
    "    \n",
    "    print(f\"Generating predictions for {len(test_prompts)} test examples...\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    for example in tqdm(test_prompts, desc=\"Evaluating\"):\n",
    "        # Get prompt without answer (remove answer part from make_prompt output)\n",
    "        full_prompt = example[\"prompt\"]\n",
    "        if '[/INST]' in full_prompt:\n",
    "            prompt_without_answer = full_prompt.split('[/INST]')[0] + '[/INST]'\n",
    "        else:\n",
    "            prompt_without_answer = full_prompt\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            prompt_without_answer,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=50,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer (everything after [/INST])\n",
    "        if '[/INST]' in decoded:\n",
    "            answer = decoded.split('[/INST]')[-1].strip()\n",
    "        else:\n",
    "            answer = decoded.strip()\n",
    "        \n",
    "        # Store results\n",
    "        preds.append(answer)\n",
    "        refs.append(example[\"reference\"])\n",
    "        raw_outputs.append(decoded)\n",
    "        prompts_list.append(example[\"prompt\"])\n",
    "        \n",
    "        # Extract question and context for detailed analysis\n",
    "        if \"Question:\" in example[\"prompt\"]:\n",
    "            question_part = example[\"prompt\"].split(\"Question:\")[-1].split(\"[/INST]\")[0].strip()\n",
    "            questions.append(question_part)\n",
    "        if \"Context:\" in example[\"prompt\"]:\n",
    "            context_part = example[\"prompt\"].split(\"Context:\")[-1].split(\"Question:\")[0].strip()\n",
    "            contexts.append(context_part[:200] + \"...\" if len(context_part) > 200 else context_part)\n",
    "    \n",
    "    print(\"Predictions generated! Computing metrics...\")\n",
    "    \n",
    "    # Compute BERTScore using silent function from your notebook\n",
    "    print(\"Computing BERTScore...\")\n",
    "    try:\n",
    "        P, R, F1 = silent_bert_score(preds, refs, lang=\"en\")\n",
    "        bert_scores = {\n",
    "            \"precision\": P.mean().item(),\n",
    "            \"recall\": R.mean().item(),\n",
    "            \"f1\": F1.mean().item()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"BERTScore computation failed: {e}\")\n",
    "        bert_scores = {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "        P = R = F1 = [0.0] * len(preds)\n",
    "    \n",
    "    # Compute exact match accuracy\n",
    "    exact_matches = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        if ref != \"No answer\" and ref.lower().strip() in pred.lower().strip():\n",
    "            exact_matches.append(1)\n",
    "        else:\n",
    "            exact_matches.append(0)\n",
    "    \n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    \n",
    "    # Compute F1 score (token overlap)\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = set(pred.lower().split())\n",
    "        ref_tokens = set(ref.lower().split())\n",
    "        \n",
    "        if len(pred_tokens) == 0 and len(ref_tokens) == 0:\n",
    "            f1_scores.append(1.0)\n",
    "        elif len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "        else:\n",
    "            common = len(pred_tokens & ref_tokens)\n",
    "            precision = common / len(pred_tokens)\n",
    "            recall = common / len(ref_tokens)\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            f1_scores.append(f1)\n",
    "    \n",
    "    f1_score = np.mean(f1_scores)\n",
    "    \n",
    "    # Compute semantic similarity (simple word overlap)\n",
    "    semantic_similarities = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_words = set(pred.lower().split())\n",
    "        ref_words = set(ref.lower().split())\n",
    "        if len(pred_words) == 0 and len(ref_words) == 0:\n",
    "            semantic_similarities.append(1.0)\n",
    "        elif len(pred_words | ref_words) == 0:\n",
    "            semantic_similarities.append(0.0)\n",
    "        else:\n",
    "            jaccard = len(pred_words & ref_words) / len(pred_words | ref_words)\n",
    "            semantic_similarities.append(jaccard)\n",
    "    \n",
    "    semantic_similarity = np.mean(semantic_similarities)\n",
    "    \n",
    "    # Compute answer length statistics\n",
    "    pred_lengths = [len(pred.split()) for pred in preds]\n",
    "    ref_lengths = [len(ref.split()) for ref in refs]\n",
    "    \n",
    "    # Print comprehensive results\n",
    "    print(\"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Test Set Size: {len(preds)}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"BERTScore Metrics:\")\n",
    "    print(f\"  Precision: {bert_scores['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {bert_scores['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {bert_scores['f1']:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Other Metrics:\")\n",
    "    print(f\"  Exact Match: {exact_match_score:.4f}\")\n",
    "    print(f\"  F1 Score:    {f1_score:.4f}\")\n",
    "    print(f\"  Semantic Similarity: {semantic_similarity:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Answer Length Statistics:\")\n",
    "    print(f\"  Avg Prediction Length: {np.mean(pred_lengths):.2f} words\")\n",
    "    print(f\"  Avg Reference Length:  {np.mean(ref_lengths):.2f} words\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show detailed examples\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED PREDICTION EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select diverse examples: best, worst, and random\n",
    "    bert_f1_scores = [f.item() if hasattr(f, 'item') else f for f in F1]\n",
    "    \n",
    "    # Get indices for different categories\n",
    "    sorted_indices = sorted(range(len(bert_f1_scores)), key=lambda i: bert_f1_scores[i], reverse=True)\n",
    "    \n",
    "    best_indices = sorted_indices[:3]  # Top 3\n",
    "    worst_indices = sorted_indices[-3:]  # Bottom 3\n",
    "    random_indices = random.sample(range(len(preds)), min(4, len(preds)))  # Random 4\n",
    "    \n",
    "    example_categories = [\n",
    "        (\"BEST PREDICTIONS\", best_indices),\n",
    "        (\"WORST PREDICTIONS\", worst_indices),\n",
    "        (\"RANDOM PREDICTIONS\", random_indices)\n",
    "    ]\n",
    "    \n",
    "    for category_name, indices in example_categories:\n",
    "        print(f\"\\n{category_name}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "            print(f\"BERTScore F1: {bert_f1_scores[idx]:.4f}\")\n",
    "            print(f\"Token F1: {f1_scores[idx]:.4f}\")\n",
    "            print(f\"Exact Match: {'✓' if exact_matches[idx] else '✗'}\")\n",
    "            \n",
    "            if idx < len(questions):\n",
    "                print(f\"Question: {questions[idx]}\")\n",
    "            if idx < len(contexts):\n",
    "                print(f\"Context: {contexts[idx]}\")\n",
    "            \n",
    "            print(f\"Reference Answer: {refs[idx]}\")\n",
    "            print(f\"Model Prediction: {preds[idx]}\")\n",
    "            \n",
    "            # Analysis\n",
    "            pred_words = len(preds[idx].split())\n",
    "            ref_words = len(refs[idx].split())\n",
    "            print(f\"Length: Pred={pred_words} words, Ref={ref_words} words\")\n",
    "            \n",
    "            # Simple similarity check\n",
    "            pred_lower = preds[idx].lower()\n",
    "            ref_lower = refs[idx].lower()\n",
    "            common_words = set(pred_lower.split()) & set(ref_lower.split())\n",
    "            print(f\"Common words: {len(common_words)}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Create results dictionary matching your expected format\n",
    "    results = {\n",
    "        \"test_size\": len(preds),\n",
    "        \"exact_match\": exact_match_score,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"bert_score_f1\": bert_scores[\"f1\"],\n",
    "        \"semantic_similarity\": semantic_similarity,\n",
    "        \"avg_prediction_length\": np.mean(pred_lengths),\n",
    "        \"avg_reference_length\": np.mean(ref_lengths),\n",
    "        \"predictions\": preds,\n",
    "        \"references\": refs,\n",
    "        \"questions\": questions,\n",
    "        \"individual_scores\": {\n",
    "            \"bert_f1\": bert_f1_scores,\n",
    "            \"token_f1\": f1_scores,\n",
    "            \"exact_match\": exact_matches,\n",
    "            \"semantic_similarity\": semantic_similarities\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save detailed results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save detailed examples\n",
    "    with open(\"detailed_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"DETAILED TEST SET PREDICTIONS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, (prompt, pred, ref, f1_score, em) in enumerate(zip(prompts_list, preds, refs, f1_scores, exact_matches)):\n",
    "            f.write(f\"Example {i+1}:\\n\")\n",
    "            f.write(f\"BERTScore F1: {f1_score:.4f}\\n\")\n",
    "            f.write(f\"Exact Match: {'✓' if em else '✗'}\\n\")\n",
    "            \n",
    "            if \"Question:\" in prompt:\n",
    "                question = prompt.split(\"Question:\")[-1].split(\"Answer:\")[0].strip()\n",
    "                f.write(f\"Question: {question}\\n\")\n",
    "            else:\n",
    "                f.write(f\"Prompt: {prompt}\\n\")\n",
    "            \n",
    "            f.write(f\"Reference: {ref}\\n\")\n",
    "            f.write(f\"Prediction: {pred}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    print(\"Results saved to:\")\n",
    "    print(\"  - test_evaluation_results.json\")\n",
    "    print(\"  - detailed_predictions.txt\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e592cc",
   "metadata": {},
   "source": [
    "## Iterative training and dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e47ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_answers(model, tokenizer, formatted_dataset, device, generation_num=1):\n",
    "    \"\"\"\n",
    "    Generate synthetic answers using the fine-tuned causal language model.\n",
    "    Takes formatted_dataset with prompts as input.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Generating synthetic answers (Generation {generation_num})...\")\n",
    "    \n",
    "    synthetic_data = []\n",
    "    \n",
    "    # Use the train split from formatted_dataset\n",
    "    train_dataset = formatted_dataset['train']\n",
    "    \n",
    "    # Enhanced progress bar with statistics\n",
    "    progress_bar = tqdm(\n",
    "        train_dataset, \n",
    "        desc=f\"🤖 Gen {generation_num} - Generating answers\",\n",
    "        unit=\"examples\",\n",
    "        position=1,\n",
    "        leave=False,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "    \n",
    "    # Statistics tracking\n",
    "    successful_generations = 0\n",
    "    failed_generations = 0\n",
    "    total_examples = len(train_dataset)\n",
    "    \n",
    "    for idx, example in enumerate(progress_bar):\n",
    "        # Get the prompt that was created by make_prompt function\n",
    "        prompt = example['prompt']\n",
    "        \n",
    "        # Find where the prompt ends to extract the incomplete part\n",
    "        # Assuming the prompt format ends with something like \"[/INST]\" or \"### Response:\"\n",
    "        if '[/INST]' in prompt:\n",
    "            # For instruction format, generate after [/INST]\n",
    "            generation_prompt = prompt\n",
    "            stop_sequence = '[/INST]'\n",
    "        elif '### Response:' in prompt:\n",
    "            # For alpaca format, generate after ### Response:\n",
    "            generation_prompt = prompt\n",
    "            stop_sequence = '### Response:'\n",
    "        else:\n",
    "            # Fallback: use the full prompt\n",
    "            generation_prompt = prompt\n",
    "            stop_sequence = None\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                generation_prompt,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Generate answer using causal LM\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,  # Increased for better answers\n",
    "                    do_sample=True,      # Changed to True for diversity\n",
    "                    temperature=0.7,     # Added temperature for controlled randomness\n",
    "                    top_p=0.9,          # Added nucleus sampling\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract the new generated part (answer)\n",
    "            if stop_sequence and stop_sequence in generated_text:\n",
    "                # Split by the stop sequence and take everything after it\n",
    "                parts = generated_text.split(stop_sequence)\n",
    "                if len(parts) > 1:\n",
    "                    synthetic_answer = parts[-1].strip()\n",
    "                else:\n",
    "                    synthetic_answer = \"No answer found\"\n",
    "            else:\n",
    "                # If no stop sequence, take everything after the original prompt\n",
    "                if generated_text.startswith(generation_prompt):\n",
    "                    synthetic_answer = generated_text[len(generation_prompt):].strip()\n",
    "                else:\n",
    "                    synthetic_answer = generated_text.strip()\n",
    "            \n",
    "            # Clean up the answer\n",
    "            if not synthetic_answer or synthetic_answer == generation_prompt:\n",
    "                synthetic_answer = \"No answer found\"\n",
    "                failed_generations += 1\n",
    "            else:\n",
    "                successful_generations += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any generation errors\n",
    "            synthetic_answer = \"No answer found\"\n",
    "            failed_generations += 1\n",
    "            print(f\"\\nWarning: Generation failed for example {idx}: {str(e)}\")\n",
    "        \n",
    "        # Create new example with synthetic answer\n",
    "        new_example = example.copy()\n",
    "        \n",
    "        # Update the prompt to include the generated answer\n",
    "        if stop_sequence:\n",
    "            new_example['prompt'] = generation_prompt + synthetic_answer\n",
    "        else:\n",
    "            new_example['prompt'] = generation_prompt + \" \" + synthetic_answer\n",
    "        \n",
    "        # If original data has structured fields, preserve them and update answers\n",
    "        if 'answers' in example:\n",
    "            if synthetic_answer != \"No answer found\":\n",
    "                # Try to find answer in context if context exists\n",
    "                context = example.get('context', '')\n",
    "                answer_start = context.find(synthetic_answer) if context else 0\n",
    "                if answer_start == -1:\n",
    "                    answer_start = 0\n",
    "                \n",
    "                new_example['answers'] = {\n",
    "                    'text': [synthetic_answer],\n",
    "                    'answer_start': [answer_start]\n",
    "                }\n",
    "            else:\n",
    "                new_example['answers'] = {\n",
    "                    'text': [],\n",
    "                    'answer_start': []\n",
    "                }\n",
    "        \n",
    "        # Add generation metadata\n",
    "        new_example['generation_num'] = generation_num\n",
    "        new_example['synthetic'] = True\n",
    "        \n",
    "        synthetic_data.append(new_example)\n",
    "        \n",
    "        # Update progress bar with statistics\n",
    "        success_rate = (successful_generations / (idx + 1)) * 100\n",
    "        progress_bar.set_postfix({\n",
    "            'Success': f'{successful_generations}/{idx + 1}',\n",
    "            'Rate': f'{success_rate:.1f}%',\n",
    "            'Failed': failed_generations\n",
    "        })\n",
    "        \n",
    "        # Update description every 100 examples\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            progress_bar.set_description(\n",
    "                f\"🤖 Gen {generation_num} - Generated {idx + 1}/{total_examples}\"\n",
    "            )\n",
    "    \n",
    "    # Close progress bar\n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f\"✅ Generation {generation_num} completed:\")\n",
    "    print(f\"   📊 Total examples processed: {total_examples}\")\n",
    "    print(f\"   ✅ Successful generations: {successful_generations}\")\n",
    "    print(f\"   ❌ Failed generations: {failed_generations}\")\n",
    "    print(f\"   📈 Success rate: {(successful_generations/total_examples)*100:.1f}%\")\n",
    "    \n",
    "    # Create a new formatted dataset with the synthetic data\n",
    "    print(\"📦 Creating synthetic dataset...\")\n",
    "    with tqdm(total=1, desc=\"📦 Building Dataset\", position=1, leave=False) as dataset_pbar:\n",
    "        synthetic_dataset = Dataset.from_list(synthetic_data)\n",
    "        dataset_pbar.update(1)\n",
    "    \n",
    "    # Return in the same format as input\n",
    "    return {\n",
    "        'train': synthetic_dataset\n",
    "    }\n",
    "\n",
    "\n",
    "def save_synthetic_dataset(dataset_dir, synthetic_formatted_dataset, generation_num):\n",
    "    \"\"\"\n",
    "    Save the synthetic formatted dataset to a new subdirectory.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Base dataset directory\n",
    "        synthetic_formatted_dataset: Formatted dataset with synthetic answers\n",
    "        generation_num: Generation number\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create new subdirectory\n",
    "    new_dir = os.path.join(dataset_dir, f\"generation_{generation_num}\")\n",
    "    os.makedirs(new_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the train split\n",
    "    synthetic_formatted_dataset['train'].save_to_disk(os.path.join(new_dir, \"train\"))\n",
    "    \n",
    "    # Also save as JSON for inspection\n",
    "    json_file = os.path.join(new_dir, \"synthetic_data.json\")\n",
    "    synthetic_formatted_dataset['train'].to_json(json_file)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"generation_number\": generation_num,\n",
    "        \"total_examples\": len(synthetic_formatted_dataset['train']),\n",
    "        \"generated_from\": \"fine_tuned_model\",\n",
    "        \"description\": f\"Synthetic answers generated using fine-tuned model (Generation {generation_num})\",\n",
    "        \"format\": \"formatted_dataset_with_prompts\"\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(new_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved synthetic formatted dataset to {new_dir}\")\n",
    "    return new_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "653fe91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_nick(model_path):\n",
    "    # Extract the part after the first \"/\"\n",
    "    model_name = model_path.split(\"/\")[1]\n",
    "    \n",
    "    # Match common patterns like \"phi-3\" or \"Mistral-7B\"\n",
    "    match = re.match(r\"([A-Za-z0-9\\-]+?)(?=-\\d|-[a-zA-Z])\", model_name)\n",
    "    \n",
    "    return match.group(1) if match else model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "388194b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_training_and_generation(\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    model_path = \"microsoft/phi-3-mini-128k-instruct\",\n",
    "    num_generations=3,\n",
    "    start_generation=1,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform iterative training and synthetic data generation.\n",
    "    \n",
    "    Args:\n",
    "        base_dataset_dir: Directory containing the original dataset\n",
    "        num_generations: Number of generations to create\n",
    "        device: Device for inference\n",
    "    \"\"\"\n",
    "    \n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    # Determine if starting from base model or fine-tuned model\n",
    "    is_base_model = start_generation == 1\n",
    "    \n",
    "    # Load tokenizer once at the beginning\n",
    "    if is_base_model:\n",
    "        base_model_name = model_path\n",
    "        model_nick = extract_model_nick(base_model_name)\n",
    "    else:\n",
    "        # For fine-tuned models, extract base model name for tokenizer\n",
    "        # Assume model_path format: \"./phi3-squad2-gen{X}-final\"\n",
    "        base_model_name = model_path  # Default fallback\n",
    "        model_nick = extract_model_nick(base_model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Define tokenization function\n",
    "    def tokenize(example):\n",
    "        return tokenizer(\n",
    "            example[\"prompt\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256\n",
    "        )\n",
    "    \n",
    "    print(f\"Starting iterative training and generation for {num_generations} generations...\")\n",
    "    \n",
    "    # Used to load dataset\n",
    "    dataset = None\n",
    "    formatted_dataset = None\n",
    "    \n",
    "    # Main progress bar for generations\n",
    "    generation_progress = tqdm(\n",
    "        range(start_generation, start_generation + num_generations), \n",
    "        desc=\"🔄 Overall Progress\", \n",
    "        unit=\"generation\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    for generation in generation_progress:\n",
    "        torch.cuda.empty_cache()\n",
    "        generation_progress.set_description(f\"🔄 Generation {generation}/{start_generation+num_generations}\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"GENERATION {generation}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Step 1: Fine-tune model with current training dataset\n",
    "        print(\"Step 1: Fine-tuning model...\")\n",
    "        \n",
    "        # Prepare training arguments for this generation\n",
    "        train_config = {\n",
    "            \"bf16\": True,\n",
    "            \"do_eval\": False,  # Disable evaluation completely\n",
    "            \"learning_rate\": 1.0e-05,\n",
    "            \"log_level\": \"info\",\n",
    "            \"logging_steps\": 10,\n",
    "            \"logging_strategy\": \"steps\",\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"num_train_epochs\": 3,\n",
    "            \"max_steps\": -1,\n",
    "            \"output_dir\": f\"./phi3-squad2-gen{generation}\",  # Update for each generation\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"per_device_train_batch_size\": 4,\n",
    "            \"remove_unused_columns\": True,\n",
    "            \"save_steps\": 50,\n",
    "            \"save_total_limit\": 2,\n",
    "            \"seed\": 42,\n",
    "            \"gradient_checkpointing\": True,\n",
    "            \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "            \"warmup_ratio\": 0.05,\n",
    "            \"save_strategy\": \"steps\",\n",
    "            \"load_best_model_at_end\": False,  # No evaluation, so no \"best\" model\n",
    "            \"disable_tqdm\": False,  # Enable tqdm progress bars for training\n",
    "        }\n",
    "\n",
    "        train_args = TrainingArguments(**train_config)\n",
    "\n",
    "        offload_cache_dir = \"./offload_cache\"\n",
    "        os.makedirs(offload_cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Load model for this generation\n",
    "        print(\"📥 Loading model...\")\n",
    "        with tqdm(total=1, desc=\"🤖 Model Loading\", position=1, leave=False) as model_pbar:\n",
    "            if generation == start_generation and is_base_model:\n",
    "                # First generation: load base model with quantization\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    base_model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    quantization_config=bnb_config\n",
    "                )\n",
    "                # Prepare dataset for first generation\n",
    "                dataset = load_squad_subset(base_dataset_dir)\n",
    "                formatted_dataset = {\n",
    "                    split: dataset[split].map(make_prompt)\n",
    "                    for split in dataset.keys()\n",
    "                }\n",
    "            elif generation == start_generation and not is_base_model:\n",
    "                # Starting from fine-tuned model: load without quantization\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    device_map=\"auto\",\n",
    "                    offload_folder=\"./offload_cache\",  # Add offload directory\n",
    "                    torch_dtype=torch.float16,        # Use float16 to save memory\n",
    "                    low_cpu_mem_usage=True           # Enable low CPU memory usage\n",
    "                )\n",
    "                # Prepare dataset based on start generation\n",
    "                if start_generation == 1:\n",
    "                    dataset = load_squad_subset(base_dataset_dir)\n",
    "                    formatted_dataset = {\n",
    "                        split: dataset[split].map(make_prompt)\n",
    "                        for split in dataset.keys()\n",
    "                    }\n",
    "                else:\n",
    "                    # Load synthetic data from previous generation\n",
    "                    synthetic_train = load_train_only(f\"{base_dataset_dir}/generation_{start_generation - 1}\")\n",
    "                    original_dataset = load_from_disk(base_dataset_dir)\n",
    "                    dataset = DatasetDict({\n",
    "                        'train': synthetic_train,\n",
    "                        'test': original_dataset['test']\n",
    "                    })\n",
    "                    formatted_dataset = {\n",
    "                        split: dataset[split].map(make_prompt)\n",
    "                        for split in dataset.keys()\n",
    "                    }\n",
    "                            \n",
    "            else:\n",
    "                # Subsequent generations: load previous model\n",
    "                previous_model_path = f\"./{model_nick}-squad2-gen{generation-1}-final\"\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    previous_model_path,\n",
    "                    device_map=\"auto\",\n",
    "                    offload_folder=\"./offload_cache\",  # Add offload directory\n",
    "                    torch_dtype=torch.float16,        # Use float16 to save memory\n",
    "                    low_cpu_mem_usage=True           # Enable low CPU memory usage\n",
    "                )\n",
    "                \n",
    "                # Load synthetic data and create dataset\n",
    "                synthetic_train = load_train_only(f\"{base_dataset_dir}/generation_{generation - 1}\")\n",
    "                original_dataset = load_from_disk(base_dataset_dir)\n",
    "                dataset = DatasetDict({\n",
    "                    'train': synthetic_train,\n",
    "                    'test': original_dataset['test']\n",
    "                })\n",
    "                formatted_dataset = {\n",
    "                    split: dataset[split].map(make_prompt)\n",
    "                    for split in dataset.keys()\n",
    "                }\n",
    "\n",
    "            model_pbar.update(1)\n",
    "        \n",
    "        # Apply PEFT configuration\n",
    "        print(\"🔧 Applying PEFT configuration...\")\n",
    "        with tqdm(total=1, desc=\"⚙️ PEFT Setup\", position=1, leave=False) as peft_pbar:\n",
    "            peft_config = {\n",
    "                \"r\": 8,  # Reduced from 16 to 8 (fewer parameters)\n",
    "                \"lora_alpha\": 16,  # Reduced from 32 to 16\n",
    "                \"lora_dropout\": 0.1,  # Slightly increased dropout\n",
    "                \"bias\": \"none\",\n",
    "                \"task_type\": \"CAUSAL_LM\",\n",
    "                \"target_modules\": \"all-linear\",\n",
    "                \"modules_to_save\": None,\n",
    "            }\n",
    "            lora_config = LoraConfig(**peft_config)\n",
    "            model = get_peft_model(model, lora_config)\n",
    "            peft_pbar.update(1)\n",
    "        \n",
    "        # Tokenize current training dataset\n",
    "        print(\"🔤 Tokenizing dataset...\")\n",
    "        tokenized = {\n",
    "            split: formatted_dataset[split].map(tokenize, batched=True)\n",
    "            for split in formatted_dataset.keys()\n",
    "        }\n",
    "        \n",
    "        # Fine-tune the model\n",
    "        print(\"🚀 Starting training...\")\n",
    "        trainer = train_model(model, tokenized, tokenizer, train_args)\n",
    "        \n",
    "        # Save the fine-tuned model for this generation\n",
    "        print(\"💾 Saving model...\")\n",
    "        with tqdm(total=1, desc=\"💾 Saving Model\", position=1, leave=False) as save_pbar:\n",
    "            final_model_path = f\"./{model_nick}-squad2-gen{generation}-final\"\n",
    "            trainer.save_model(final_model_path)\n",
    "            save_pbar.update(1)\n",
    "        print(f\"✅ Generation {generation} model saved to {final_model_path}\")\n",
    "        \n",
    "        # Step 2: Generate synthetic answers using the fine-tuned model\n",
    "        print(\"Step 2: Generating synthetic answers...\")\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        synthetic_dataset = generate_synthetic_answers(\n",
    "            model, tokenizer, formatted_dataset, device, generation\n",
    "        )\n",
    "        \n",
    "        # Step 3: Save synthetic dataset\n",
    "        print(\"Step 3: Saving synthetic dataset...\")\n",
    "        with tqdm(total=1, desc=\"💾 Saving Dataset\", position=1, leave=False) as dataset_save_pbar:\n",
    "            new_dir = save_synthetic_dataset(base_dataset_dir, synthetic_dataset, generation)\n",
    "            dataset_save_pbar.update(1)\n",
    "        \n",
    "        # Update current training dataset for next iteration\n",
    "        current_train_dataset = synthetic_dataset\n",
    "        \n",
    "        print(f\"Generation {generation} completed!\")\n",
    "        print(f\"Synthetic dataset saved to: {new_dir}\")\n",
    "        print(f\"Model saved to: {final_model_path}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    generation_progress.close()\n",
    "    print(f\"\\n🎉 All {num_generations} generations completed!\")\n",
    "    print(\"Final models and datasets are ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50ee1940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iterative training and generation for 2 generations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Generation 3/5:   0%|          | 0/2 [00:00<?, ?generation/s]  PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GENERATION 3\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n",
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914df00a32d34b978559035c74a797b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from squad_v2_05percent/generation_2...\n",
      "Dataset metadata:\n",
      "  generation_number: 2\n",
      "  total_examples: 651\n",
      "  generated_from: fine_tuned_model\n",
      "  description: Synthetic answers generated using fine-tuned model (Generation 2)\n",
      "  format: formatted_dataset_with_prompts\n",
      "\n",
      "Loaded train dataset:\n",
      "  Examples: 651\n",
      "  Columns: ['id', 'title', 'context', 'question', 'answers', 'prompt', 'reference', 'generation_num', 'synthetic']\n",
      "🔧 Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Tokenizing dataset...\n",
      "🚀 Starting training...\n",
      "✅ Set use_cache=False for gradient checkpointing compatibility\n",
      "✅ Enabled gradient checkpointing for memory efficiency\n",
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 651\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 246\n",
      "  Number of trainable parameters = 12,582,912\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [246/246 47:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.651100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.718300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.730800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.536600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.310700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.160300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.237400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-50\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-100\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-150\n",
      "Deleting older checkpoint [phi3-squad2-gen3\\checkpoint-50] due to args.save_total_limit\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-200\n",
      "Deleting older checkpoint [phi3-squad2-gen3\\checkpoint-100] due to args.save_total_limit\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-246\n",
      "Deleting older checkpoint [phi3-squad2-gen3\\checkpoint-150] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Training loss improved to 2.3892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Re-enabled use_cache for inference\n",
      "✅ Model saved to ./phi3-squad2-final\n",
      "\n",
      "🧹 Cleaning up checkpoints...\n",
      "🗑️ Removing checkpoint: ./phi3-squad2-gen3\\checkpoint-200\n",
      "🗑️ Removing checkpoint: ./phi3-squad2-gen3\\checkpoint-246\n",
      "✅ Checkpoint cleanup completed!\n",
      "✅ Checkpoint cleanup completed!\n",
      "🎉 Training completed and checkpoints cleaned up!\n",
      "💾 Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi-squad2-gen3-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generation 3 model saved to ./phi-squad2-gen3-final\n",
      "Step 2: Generating synthetic answers...\n",
      "Generating synthetic answers (Generation 3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generation 3 completed:\n",
      "   📊 Total examples processed: 651\n",
      "   ✅ Successful generations: 650\n",
      "   ❌ Failed generations: 1\n",
      "   📈 Success rate: 99.8%\n",
      "📦 Creating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Saving synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c548f9f2a9c4af48cde3887b9e18809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7814f43c298c4fe7afe0ed25b5aac2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Generation 4/5:  50%|█████     | 1/2 [1:46:43<1:46:43, 6403.36s/generation]PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic formatted dataset to squad_v2_05percent\\generation_3\n",
      "Generation 3 completed!\n",
      "Synthetic dataset saved to: squad_v2_05percent\\generation_3\n",
      "Model saved to: ./phi-squad2-gen3-final\n",
      "\n",
      "==================================================\n",
      "GENERATION 4\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d6e1290c06400181d7a0d3b2515239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from squad_v2_05percent/generation_3...\n",
      "Dataset metadata:\n",
      "  generation_number: 3\n",
      "  total_examples: 651\n",
      "  generated_from: fine_tuned_model\n",
      "  description: Synthetic answers generated using fine-tuned model (Generation 3)\n",
      "  format: formatted_dataset_with_prompts\n",
      "\n",
      "Loaded train dataset:\n",
      "  Examples: 651\n",
      "  Columns: ['id', 'title', 'context', 'question', 'answers', 'prompt', 'reference', 'generation_num', 'synthetic']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026f3735ff8f449a800e7cf9092241f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a566eb512e4ce19e984fa6fe795c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training...\n",
      "✅ Set use_cache=False for gradient checkpointing compatibility\n",
      "✅ Enabled gradient checkpointing for memory efficiency\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54be93170154ebd95869c6439dcb75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "***** Running training *****\n",
      "  Num examples = 651\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 246\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [246/246 35:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.544700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.809500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.223400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.337900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.321700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.074300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.186100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.176800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.291100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-50\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-100\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-150\n",
      "Deleting older checkpoint [phi3-squad2-gen4\\checkpoint-50] due to args.save_total_limit\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-200\n",
      "Deleting older checkpoint [phi3-squad2-gen4\\checkpoint-100] due to args.save_total_limit\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n",
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-246\n",
      "Deleting older checkpoint [phi3-squad2-gen4\\checkpoint-150] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Training loss improved to 2.3175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Re-enabled use_cache for inference\n",
      "✅ Model saved to ./phi3-squad2-final\n",
      "\n",
      "🧹 Cleaning up checkpoints...\n",
      "🗑️ Removing checkpoint: ./phi3-squad2-gen4\\checkpoint-200\n",
      "🗑️ Removing checkpoint: ./phi3-squad2-gen4\\checkpoint-246\n",
      "✅ Checkpoint cleanup completed!\n",
      "✅ Checkpoint cleanup completed!\n",
      "🎉 Training completed and checkpoints cleaned up!\n",
      "💾 Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi-squad2-gen4-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generation 4 model saved to ./phi-squad2-gen4-final\n",
      "Step 2: Generating synthetic answers...\n",
      "Generating synthetic answers (Generation 4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generation 4 completed:\n",
      "   📊 Total examples processed: 651\n",
      "   ✅ Successful generations: 651\n",
      "   ❌ Failed generations: 0\n",
      "   📈 Success rate: 100.0%\n",
      "📦 Creating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Saving synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4c635d41e04335a564b0192c644765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a6fc9e1b11464ea0f267a13ae419b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Generation 4/5: 100%|██████████| 2/2 [3:20:31<00:00, 6015.62s/generation]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic formatted dataset to squad_v2_05percent\\generation_4\n",
      "Generation 4 completed!\n",
      "Synthetic dataset saved to: squad_v2_05percent\\generation_4\n",
      "Model saved to: ./phi-squad2-gen4-final\n",
      "\n",
      "🎉 All 2 generations completed!\n",
      "Final models and datasets are ready for use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iterative_training_and_generation(\n",
    "        base_dataset_dir=\"squad_v2_05percent\",\n",
    "        model_path=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "        start_generation=3,\n",
    "        num_generations=2,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94bcca3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de14b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation(base_model_name, generation_num, base_dataset_dir, device, \n",
    "                       save_results=True, results_dir=\"evaluation_results\", show_examples=True, num_examples=5):\n",
    "    \"\"\"\n",
    "    Evaluate a specific generation model on the test set with optional result saving.\n",
    "    Loads the model and tokenizer internally based on generation number.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Base model name (e.g., \"microsoft/phi-3-mini-128k-instruct\")\n",
    "        generation_num: Generation number to evaluate\n",
    "        base_dataset_dir: Directory containing the original test dataset\n",
    "        device: Device for inference\n",
    "        save_results: Whether to save results to JSON file\n",
    "        results_dir: Directory to save evaluation results\n",
    "        show_examples: Whether to display example predictions\n",
    "        num_examples: Number of examples to show\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    # Determine if starting from base model or fine-tuned model\n",
    "    is_base_model = generation_num == 0\n",
    "    \n",
    "    # Extract model nickname for path construction\n",
    "    model_nick = extract_model_nick(base_model_name)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"📥 Loading tokenizer...\")\n",
    "    with tqdm(total=1, desc=\"🔤 Tokenizer Loading\", position=1, leave=False) as tokenizer_pbar:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer_pbar.update(1)\n",
    "    \n",
    "    # Load model based on generation number\n",
    "    print(\"📥 Loading model...\")\n",
    "    with tqdm(total=1, desc=\"🤖 Model Loading\", position=1, leave=False) as model_pbar:\n",
    "        if generation_num == 0:\n",
    "            # Load base model\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            # Load fine-tuned model from specific generation\n",
    "            model_path = f\"./{model_nick}-squad2-gen{generation_num}-final\"\n",
    "            # Create offload directory\n",
    "            offload_cache_dir = \"./offload_cache\"\n",
    "            os.makedirs(offload_cache_dir, exist_ok=True)\n",
    "            \n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model for generation {generation_num} not found at {model_path}\")\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                offload_folder=offload_cache_dir,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        model_pbar.update(1)\n",
    "    \n",
    "    # Load original test dataset\n",
    "    with tqdm(total=1, desc=\"📂 Loading Test Data\", position=1, leave=False) as load_pbar:\n",
    "        original_dataset = load_from_disk(base_dataset_dir)\n",
    "        test_dataset = original_dataset['test']\n",
    "        load_pbar.update(1)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Run evaluation\n",
    "    with tqdm(total=1, desc=\"📊 Evaluating\", position=1, leave=False) as eval_pbar:\n",
    "        evaluation_results = evaluate_model(model, tokenizer, test_dataset, device)\n",
    "        eval_pbar.update(1)\n",
    "    \n",
    "    # Add metadata to results\n",
    "    evaluation_results.update({\n",
    "        'generation': generation_num,\n",
    "        'test_dataset_size': len(test_dataset),\n",
    "        'base_dataset_dir': base_dataset_dir,\n",
    "        'base_model_name': base_model_name,\n",
    "        'model_nick': model_nick,\n",
    "        'model_path': f\"./{model_nick}-squad2-gen{generation_num}-final\" if generation_num > 0 else base_model_name,\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"📊 Generation {generation_num} Evaluation Results:\")\n",
    "    print(f\"   Exact Match: {evaluation_results['exact_match']:.3f}\")\n",
    "    print(f\"   F1 Score: {evaluation_results['f1_score']:.3f}\")\n",
    "    print(f\"   BERTScore F1: {evaluation_results['bert_score_f1']:.3f}\")\n",
    "    print(f\"   Semantic Similarity: {evaluation_results['semantic_similarity']:.3f}\")\n",
    "    \n",
    "    # Show example predictions if requested\n",
    "    if show_examples and 'predictions' in evaluation_results:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📋 EXAMPLE PREDICTIONS - GENERATION {generation_num}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        predictions = evaluation_results['predictions']\n",
    "        references = evaluation_results['references']\n",
    "        questions = evaluation_results.get('questions', [])\n",
    "        individual_scores = evaluation_results.get('individual_scores', {})\n",
    "        bert_f1_scores = individual_scores.get('bert_f1', [])\n",
    "        exact_matches = individual_scores.get('exact_match', [])\n",
    "        \n",
    "        # Select diverse examples: best, worst, and random\n",
    "        num_total = len(predictions)\n",
    "        num_to_show = min(num_examples, num_total)\n",
    "        \n",
    "        if bert_f1_scores and len(bert_f1_scores) > 0:\n",
    "            # Sort by BERTScore F1 for best/worst examples\n",
    "            sorted_indices = sorted(range(len(bert_f1_scores)), \n",
    "                                  key=lambda i: bert_f1_scores[i], reverse=True)\n",
    "            \n",
    "            # Get best, worst, and random examples\n",
    "            best_idx = sorted_indices[0] if sorted_indices else 0\n",
    "            worst_idx = sorted_indices[-1] if sorted_indices else 0\n",
    "            \n",
    "            # Get some random examples (avoid duplicates with best/worst)\n",
    "            available_indices = [i for i in range(num_total) if i not in [best_idx, worst_idx]]\n",
    "            num_random = min(num_to_show - 2, len(available_indices))\n",
    "            random_indices = random.sample(available_indices, num_random) if num_random > 0 else []\n",
    "            \n",
    "            example_indices = [best_idx, worst_idx] + random_indices\n",
    "        else:\n",
    "            # If no scores available, just show random examples\n",
    "            example_indices = random.sample(range(num_total), min(num_to_show, num_total))\n",
    "        \n",
    "        # Display selected examples\n",
    "        for i, idx in enumerate(example_indices):\n",
    "            if idx >= len(predictions):\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n{'─'*60}\")\n",
    "            print(f\"📝 EXAMPLE {i+1} (Index {idx})\")\n",
    "            print(f\"{'─'*60}\")\n",
    "            \n",
    "            # Show scores if available\n",
    "            if bert_f1_scores and idx < len(bert_f1_scores):\n",
    "                print(f\"🎯 BERTScore F1: {bert_f1_scores[idx]:.3f}\")\n",
    "            if exact_matches and idx < len(exact_matches):\n",
    "                match_status = \"✅ EXACT MATCH\" if exact_matches[idx] else \"❌ NO MATCH\"\n",
    "                print(f\"🔍 Exact Match: {match_status}\")\n",
    "            \n",
    "            # Show question if available\n",
    "            if questions and idx < len(questions):\n",
    "                print(f\"❓ Question: {questions[idx]}\")\n",
    "            \n",
    "            # Show context from test dataset\n",
    "            if idx < len(test_dataset):\n",
    "                context = test_dataset[idx]['context']\n",
    "                # Truncate context if too long\n",
    "                context_display = context[:300] + \"...\" if len(context) > 300 else context\n",
    "                print(f\"📄 Context: {context_display}\")\n",
    "            \n",
    "            # Show reference and prediction\n",
    "            print(f\"✅ Reference Answer: {references[idx]}\")\n",
    "            print(f\"🤖 Model Prediction: {predictions[idx]}\")\n",
    "            \n",
    "            # Simple analysis\n",
    "            ref_words = len(references[idx].split())\n",
    "            pred_words = len(predictions[idx].split())\n",
    "            print(f\"📊 Length: Reference={ref_words} words, Prediction={pred_words} words\")\n",
    "            \n",
    "            # Check if prediction contains reference keywords\n",
    "            ref_lower = references[idx].lower()\n",
    "            pred_lower = predictions[idx].lower()\n",
    "            common_words = set(ref_lower.split()) & set(pred_lower.split())\n",
    "            print(f\"🔗 Common words: {len(common_words)} ({', '.join(list(common_words)[:5])}{'...' if len(common_words) > 5 else ''})\")\n",
    "            \n",
    "            # Additional analysis: Check semantic overlap\n",
    "            if len(common_words) > 0:\n",
    "                overlap_percentage = len(common_words) / max(len(set(ref_lower.split())), 1) * 100\n",
    "                print(f\"🔍 Reference overlap: {overlap_percentage:.1f}%\")\n",
    "            \n",
    "            # Check if answer is contained in context\n",
    "            if idx < len(test_dataset):\n",
    "                context_lower = test_dataset[idx]['context'].lower()\n",
    "                ref_in_context = references[idx].lower() in context_lower\n",
    "                pred_in_context = predictions[idx].lower() in context_lower\n",
    "                print(f\"📖 Reference in context: {'✅' if ref_in_context else '❌'}\")\n",
    "                print(f\"📖 Prediction in context: {'✅' if pred_in_context else '❌'}\")\n",
    "        \n",
    "        # Enhanced summary section\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📈 DETAILED SUMMARY FOR GENERATION {generation_num}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"📊 Total examples evaluated: {num_total}\")\n",
    "        print(f\"📋 Examples shown above: {len(example_indices)}\")\n",
    "        \n",
    "        if exact_matches:\n",
    "            exact_match_count = sum(exact_matches)\n",
    "            print(f\"✅ Exact matches: {exact_match_count}/{num_total} ({exact_match_count/num_total*100:.1f}%)\")\n",
    "        \n",
    "        if bert_f1_scores:\n",
    "            avg_bert_f1 = sum(bert_f1_scores) / len(bert_f1_scores)\n",
    "            max_bert_f1 = max(bert_f1_scores)\n",
    "            min_bert_f1 = min(bert_f1_scores)\n",
    "            print(f\"🎯 BERTScore F1 - Avg: {avg_bert_f1:.3f}, Max: {max_bert_f1:.3f}, Min: {min_bert_f1:.3f}\")\n",
    "            \n",
    "            # Distribution analysis\n",
    "            high_scores = sum(1 for score in bert_f1_scores if score > 0.8)\n",
    "            medium_scores = sum(1 for score in bert_f1_scores if 0.5 <= score <= 0.8)\n",
    "            low_scores = sum(1 for score in bert_f1_scores if score < 0.5)\n",
    "            \n",
    "            print(f\"📊 Score Distribution:\")\n",
    "            print(f\"   🟢 High (>0.8): {high_scores} ({high_scores/num_total*100:.1f}%)\")\n",
    "            print(f\"   🟡 Medium (0.5-0.8): {medium_scores} ({medium_scores/num_total*100:.1f}%)\")\n",
    "            print(f\"   🔴 Low (<0.5): {low_scores} ({low_scores/num_total*100:.1f}%)\")\n",
    "        \n",
    "        # Answer length analysis\n",
    "        if 'predictions' in evaluation_results and 'references' in evaluation_results:\n",
    "            pred_lengths = [len(pred.split()) for pred in predictions]\n",
    "            ref_lengths = [len(ref.split()) for ref in references]\n",
    "            \n",
    "            avg_pred_len = np.mean(pred_lengths)\n",
    "            avg_ref_len = np.mean(ref_lengths)\n",
    "            \n",
    "            print(f\"📏 Answer Length Analysis:\")\n",
    "            print(f\"   📝 Avg Prediction Length: {avg_pred_len:.1f} words\")\n",
    "            print(f\"   📖 Avg Reference Length: {avg_ref_len:.1f} words\")\n",
    "            print(f\"   📊 Length Ratio: {avg_pred_len/avg_ref_len:.2f}\")\n",
    "            \n",
    "            # Length distribution\n",
    "            short_preds = sum(1 for length in pred_lengths if length <= 3)\n",
    "            medium_preds = sum(1 for length in pred_lengths if 4 <= length <= 10)\n",
    "            long_preds = sum(1 for length in pred_lengths if length > 10)\n",
    "            \n",
    "            print(f\"   📊 Prediction Length Distribution:\")\n",
    "            print(f\"      Short (≤3 words): {short_preds} ({short_preds/num_total*100:.1f}%)\")\n",
    "            print(f\"      Medium (4-10 words): {medium_preds} ({medium_preds/num_total*100:.1f}%)\")\n",
    "            print(f\"      Long (>10 words): {long_preds} ({long_preds/num_total*100:.1f}%)\")\n",
    "    \n",
    "    # Save results if requested\n",
    "    if save_results:\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        results_file = os.path.join(results_dir, f\"generation_{generation_num}_results.json\")\n",
    "        \n",
    "        # Create a serializable version of results for JSON\n",
    "        json_results = {}\n",
    "        for key, value in evaluation_results.items():\n",
    "            if isinstance(value, (list, dict, str, int, float, bool, type(None))):\n",
    "                json_results[key] = value\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                json_results[key] = value.tolist()\n",
    "            else:\n",
    "                json_results[key] = str(value)\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n💾 Results saved to: {results_file}\")\n",
    "        \n",
    "        # Also save a summary file\n",
    "        summary_file = os.path.join(results_dir, f\"generation_{generation_num}_summary.txt\")\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"EVALUATION SUMMARY - GENERATION {generation_num}\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            f.write(f\"Model: {base_model_name}\\n\")\n",
    "            f.write(f\"Generation: {generation_num}\\n\")\n",
    "            f.write(f\"Test Dataset: {base_dataset_dir}\\n\")\n",
    "            f.write(f\"Test Set Size: {evaluation_results['test_size']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"METRICS:\\n\")\n",
    "            f.write(f\"Exact Match: {evaluation_results['exact_match']:.4f}\\n\")\n",
    "            f.write(f\"F1 Score: {evaluation_results['f1_score']:.4f}\\n\")\n",
    "            f.write(f\"BERTScore F1: {evaluation_results['bert_score_f1']:.4f}\\n\")\n",
    "            f.write(f\"Semantic Similarity: {evaluation_results['semantic_similarity']:.4f}\\n\")\n",
    "            f.write(f\"Avg Prediction Length: {evaluation_results['avg_prediction_length']:.2f} words\\n\")\n",
    "            f.write(f\"Avg Reference Length: {evaluation_results['avg_reference_length']:.2f} words\\n\")\n",
    "        \n",
    "        print(f\"📄 Summary saved to: {summary_file}\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n🎉 Evaluation completed for Generation {generation_num}!\")\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8ceb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation(base_model_name, generation_num, base_dataset_dir, device, \n",
    "                       save_results=True, results_dir=\"evaluation_results\", show_examples=True, num_examples=5):\n",
    "    \"\"\"\n",
    "    Evaluate a specific generation model on the test set with optional result saving.\n",
    "    Loads the model and tokenizer internally based on generation number.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Base model name (e.g., \"microsoft/phi-3-mini-128k-instruct\")\n",
    "        generation_num: Generation number to evaluate\n",
    "        base_dataset_dir: Directory containing the original test dataset\n",
    "        device: Device for inference\n",
    "        save_results: Whether to save results to JSON file\n",
    "        results_dir: Directory to save evaluation results\n",
    "        show_examples: Whether to display example predictions\n",
    "        num_examples: Number of examples to show\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    # Determine if starting from base model or fine-tuned model\n",
    "    is_base_model = generation_num == 0\n",
    "    \n",
    "    # Extract model nickname for path construction\n",
    "    model_nick = extract_model_nick(base_model_name)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"📥 Loading tokenizer...\")\n",
    "    with tqdm(total=1, desc=\"🔤 Tokenizer Loading\", position=1, leave=False) as tokenizer_pbar:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer_pbar.update(1)\n",
    "    \n",
    "    # Load model based on generation number\n",
    "    print(\"📥 Loading model...\")\n",
    "    with tqdm(total=1, desc=\"🤖 Model Loading\", position=1, leave=False) as model_pbar:\n",
    "        if generation_num == 0:\n",
    "            # Load base model\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            # Load fine-tuned model from specific generation\n",
    "            model_path = f\"./{model_nick}-squad2-gen{generation_num}-final\"\n",
    "            # Create offload directory\n",
    "            offload_cache_dir = \"./offload_cache\"\n",
    "            os.makedirs(offload_cache_dir, exist_ok=True)\n",
    "            \n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model for generation {generation_num} not found at {model_path}\")\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                offload_folder=offload_cache_dir,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        model_pbar.update(1)\n",
    "    \n",
    "    # Load original test dataset\n",
    "    with tqdm(total=1, desc=\"📂 Loading Test Data\", position=1, leave=False) as load_pbar:\n",
    "        original_dataset = load_from_disk(base_dataset_dir)\n",
    "        test_dataset = original_dataset['test']\n",
    "        load_pbar.update(1)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Run evaluation\n",
    "    with tqdm(total=1, desc=\"📊 Evaluating\", position=1, leave=False) as eval_pbar:\n",
    "        evaluation_results = evaluate_model(model, tokenizer, test_dataset, device)\n",
    "        eval_pbar.update(1)\n",
    "    \n",
    "    # Add metadata to results\n",
    "    evaluation_results.update({\n",
    "        'generation': generation_num,\n",
    "        'test_dataset_size': len(test_dataset),\n",
    "        'base_dataset_dir': base_dataset_dir,\n",
    "        'base_model_name': base_model_name,\n",
    "        'model_nick': model_nick,\n",
    "        'model_path': f\"./{model_nick}-squad2-gen{generation_num}-final\" if generation_num > 0 else base_model_name,\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"📊 Generation {generation_num} Evaluation Results:\")\n",
    "    print(f\"   Exact Match: {evaluation_results['exact_match']:.3f}\")\n",
    "    print(f\"   F1 Score: {evaluation_results['f1_score']:.3f}\")\n",
    "    print(f\"   BERTScore F1: {evaluation_results['bert_score_f1']:.3f}\")\n",
    "    print(f\"   Semantic Similarity: {evaluation_results['semantic_similarity']:.3f}\")\n",
    "    \n",
    "    # Show example predictions if requested\n",
    "    if show_examples and 'predictions' in evaluation_results:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📋 EXAMPLE PREDICTIONS - GENERATION {generation_num}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        predictions = evaluation_results['predictions']\n",
    "        references = evaluation_results['references']\n",
    "        questions = evaluation_results.get('questions', [])\n",
    "        individual_scores = evaluation_results.get('individual_scores', {})\n",
    "        bert_f1_scores = individual_scores.get('bert_f1', [])\n",
    "        exact_matches = individual_scores.get('exact_match', [])\n",
    "        \n",
    "        # Select diverse examples: best, worst, and random\n",
    "        num_total = len(predictions)\n",
    "        num_to_show = min(num_examples, num_total)\n",
    "        \n",
    "        if bert_f1_scores and len(bert_f1_scores) > 0:\n",
    "            # Sort by BERTScore F1 for best/worst examples\n",
    "            sorted_indices = sorted(range(len(bert_f1_scores)), \n",
    "                                  key=lambda i: bert_f1_scores[i], reverse=True)\n",
    "            \n",
    "            # Get best, worst, and random examples\n",
    "            best_idx = sorted_indices[0] if sorted_indices else 0\n",
    "            worst_idx = sorted_indices[-1] if sorted_indices else 0\n",
    "            \n",
    "            # Get some random examples (avoid duplicates with best/worst)\n",
    "            available_indices = [i for i in range(num_total) if i not in [best_idx, worst_idx]]\n",
    "            num_random = min(num_to_show - 2, len(available_indices))\n",
    "            random_indices = random.sample(available_indices, num_random) if num_random > 0 else []\n",
    "            \n",
    "            example_indices = [best_idx, worst_idx] + random_indices\n",
    "        else:\n",
    "            # If no scores available, just show random examples\n",
    "            example_indices = random.sample(range(num_total), min(num_to_show, num_total))\n",
    "        \n",
    "        # Display selected examples\n",
    "        for i, idx in enumerate(example_indices):\n",
    "            if idx >= len(predictions):\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n{'─'*60}\")\n",
    "            print(f\"📝 EXAMPLE {i+1} (Index {idx})\")\n",
    "            print(f\"{'─'*60}\")\n",
    "            \n",
    "            # Show scores if available\n",
    "            if bert_f1_scores and idx < len(bert_f1_scores):\n",
    "                print(f\"🎯 BERTScore F1: {bert_f1_scores[idx]:.3f}\")\n",
    "            if exact_matches and idx < len(exact_matches):\n",
    "                match_status = \"✅ EXACT MATCH\" if exact_matches[idx] else \"❌ NO MATCH\"\n",
    "                print(f\"🔍 Exact Match: {match_status}\")\n",
    "            \n",
    "            # Show question if available\n",
    "            if questions and idx < len(questions):\n",
    "                print(f\"❓ Question: {questions[idx]}\")\n",
    "            \n",
    "            # Show context from test dataset\n",
    "            if idx < len(test_dataset):\n",
    "                context = test_dataset[idx]['context']\n",
    "                # Truncate context if too long\n",
    "                context_display = context[:300] + \"...\" if len(context) > 300 else context\n",
    "                print(f\"📄 Context: {context_display}\")\n",
    "            \n",
    "            # Show reference and prediction\n",
    "            print(f\"✅ Reference Answer: {references[idx]}\")\n",
    "            print(f\"🤖 Model Prediction: {predictions[idx]}\")\n",
    "            \n",
    "            # Simple analysis\n",
    "            ref_words = len(references[idx].split())\n",
    "            pred_words = len(predictions[idx].split())\n",
    "            print(f\"📊 Length: Reference={ref_words} words, Prediction={pred_words} words\")\n",
    "            \n",
    "            # Check if prediction contains reference keywords\n",
    "            ref_lower = references[idx].lower()\n",
    "            pred_lower = predictions[idx].lower()\n",
    "            common_words = set(ref_lower.split()) & set(pred_lower.split())\n",
    "            print(f\"🔗 Common words: {len(common_words)} ({', '.join(list(common_words)[:5])}{'...' if len(common_words) > 5 else ''})\")\n",
    "            \n",
    "            # Additional analysis: Check semantic overlap\n",
    "            if len(common_words) > 0:\n",
    "                overlap_percentage = len(common_words) / max(len(set(ref_lower.split())), 1) * 100\n",
    "                print(f\"🔍 Reference overlap: {overlap_percentage:.1f}%\")\n",
    "            \n",
    "            # Check if answer is contained in context\n",
    "            if idx < len(test_dataset):\n",
    "                context_lower = test_dataset[idx]['context'].lower()\n",
    "                ref_in_context = references[idx].lower() in context_lower\n",
    "                pred_in_context = predictions[idx].lower() in context_lower\n",
    "                print(f\"📖 Reference in context: {'✅' if ref_in_context else '❌'}\")\n",
    "                print(f\"📖 Prediction in context: {'✅' if pred_in_context else '❌'}\")\n",
    "        \n",
    "        # Enhanced summary section\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📈 DETAILED SUMMARY FOR GENERATION {generation_num}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"📊 Total examples evaluated: {num_total}\")\n",
    "        print(f\"📋 Examples shown above: {len(example_indices)}\")\n",
    "        \n",
    "        if exact_matches:\n",
    "            exact_match_count = sum(exact_matches)\n",
    "            print(f\"✅ Exact matches: {exact_match_count}/{num_total} ({exact_match_count/num_total*100:.1f}%)\")\n",
    "        \n",
    "        if bert_f1_scores:\n",
    "            avg_bert_f1 = sum(bert_f1_scores) / len(bert_f1_scores)\n",
    "            max_bert_f1 = max(bert_f1_scores)\n",
    "            min_bert_f1 = min(bert_f1_scores)\n",
    "            print(f\"🎯 BERTScore F1 - Avg: {avg_bert_f1:.3f}, Max: {max_bert_f1:.3f}, Min: {min_bert_f1:.3f}\")\n",
    "            \n",
    "            # Distribution analysis\n",
    "            high_scores = sum(1 for score in bert_f1_scores if score > 0.8)\n",
    "            medium_scores = sum(1 for score in bert_f1_scores if 0.5 <= score <= 0.8)\n",
    "            low_scores = sum(1 for score in bert_f1_scores if score < 0.5)\n",
    "            \n",
    "            print(f\"📊 Score Distribution:\")\n",
    "            print(f\"   🟢 High (>0.8): {high_scores} ({high_scores/num_total*100:.1f}%)\")\n",
    "            print(f\"   🟡 Medium (0.5-0.8): {medium_scores} ({medium_scores/num_total*100:.1f}%)\")\n",
    "            print(f\"   🔴 Low (<0.5): {low_scores} ({low_scores/num_total*100:.1f}%)\")\n",
    "        \n",
    "        # Answer length analysis\n",
    "        if 'predictions' in evaluation_results and 'references' in evaluation_results:\n",
    "            pred_lengths = [len(pred.split()) for pred in predictions]\n",
    "            ref_lengths = [len(ref.split()) for ref in references]\n",
    "            \n",
    "            avg_pred_len = np.mean(pred_lengths)\n",
    "            avg_ref_len = np.mean(ref_lengths)\n",
    "            \n",
    "            print(f\"📏 Answer Length Analysis:\")\n",
    "            print(f\"   📝 Avg Prediction Length: {avg_pred_len:.1f} words\")\n",
    "            print(f\"   📖 Avg Reference Length: {avg_ref_len:.1f} words\")\n",
    "            print(f\"   📊 Length Ratio: {avg_pred_len/avg_ref_len:.2f}\")\n",
    "            \n",
    "            # Length distribution\n",
    "            short_preds = sum(1 for length in pred_lengths if length <= 3)\n",
    "            medium_preds = sum(1 for length in pred_lengths if 4 <= length <= 10)\n",
    "            long_preds = sum(1 for length in pred_lengths if length > 10)\n",
    "            \n",
    "            print(f\"   📊 Prediction Length Distribution:\")\n",
    "            print(f\"      Short (≤3 words): {short_preds} ({short_preds/num_total*100:.1f}%)\")\n",
    "            print(f\"      Medium (4-10 words): {medium_preds} ({medium_preds/num_total*100:.1f}%)\")\n",
    "            print(f\"      Long (>10 words): {long_preds} ({long_preds/num_total*100:.1f}%)\")\n",
    "    \n",
    "    # Save results if requested\n",
    "    if save_results:\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        results_file = os.path.join(results_dir, f\"generation_{generation_num}_results.json\")\n",
    "        \n",
    "        # Create a serializable version of results for JSON\n",
    "        json_results = {}\n",
    "        for key, value in evaluation_results.items():\n",
    "            if isinstance(value, (list, dict, str, int, float, bool, type(None))):\n",
    "                json_results[key] = value\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                json_results[key] = value.tolist()\n",
    "            else:\n",
    "                json_results[key] = str(value)\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n💾 Results saved to: {results_file}\")\n",
    "        \n",
    "        # Also save a summary file\n",
    "        summary_file = os.path.join(results_dir, f\"generation_{generation_num}_summary.txt\")\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"EVALUATION SUMMARY - GENERATION {generation_num}\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            f.write(f\"Model: {base_model_name}\\n\")\n",
    "            f.write(f\"Generation: {generation_num}\\n\")\n",
    "            f.write(f\"Test Dataset: {base_dataset_dir}\\n\")\n",
    "            f.write(f\"Test Set Size: {evaluation_results['test_size']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"METRICS:\\n\")\n",
    "            f.write(f\"Exact Match: {evaluation_results['exact_match']:.4f}\\n\")\n",
    "            f.write(f\"F1 Score: {evaluation_results['f1_score']:.4f}\\n\")\n",
    "            f.write(f\"BERTScore F1: {evaluation_results['bert_score_f1']:.4f}\\n\")\n",
    "            f.write(f\"Semantic Similarity: {evaluation_results['semantic_similarity']:.4f}\\n\")\n",
    "            f.write(f\"Avg Prediction Length: {evaluation_results['avg_prediction_length']:.2f} words\\n\")\n",
    "            f.write(f\"Avg Reference Length: {evaluation_results['avg_reference_length']:.2f} words\\n\")\n",
    "        \n",
    "        print(f\"📄 Summary saved to: {summary_file}\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n🎉 Evaluation completed for Generation {generation_num}!\")\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "464dcfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a779e89a0ef04a2fabab4b74d4b3d3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TEST SET EVALUATION WITH EXAMPLES\n",
      "============================================================\n",
      "Preparing test prompts...\n",
      "Generating predictions for 59 test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 59/59 [01:33<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated! Computing metrics...\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Test Set Size: 59\n",
      "------------------------------------------------------------\n",
      "BERTScore Metrics:\n",
      "  Precision: 0.7729\n",
      "  Recall:    0.8187\n",
      "  F1 Score:  0.7949\n",
      "------------------------------------------------------------\n",
      "Other Metrics:\n",
      "  Exact Match: 0.3051\n",
      "  F1 Score:    0.0364\n",
      "  Semantic Similarity: 0.0195\n",
      "------------------------------------------------------------\n",
      "Answer Length Statistics:\n",
      "  Avg Prediction Length: 30.24 words\n",
      "  Avg Reference Length:  1.88 words\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED PREDICTION EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "BEST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 34):\n",
      "BERTScore F1: 0.8574\n",
      "Token F1: 0.2222\n",
      "Exact Match: ✗\n",
      "Question: What did maternal Old Norse traditions merge with?\n",
      "Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, ...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: ### Instruction: [INST] Given the context, answer the\n",
      "Length: Pred=8 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 1):\n",
      "BERTScore F1: 0.8518\n",
      "Token F1: 0.2000\n",
      "Exact Match: ✓\n",
      "Question: When were the Normans in Normandy?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: 10th and 11th centuries\n",
      "Model Prediction: The Normans were in Normandy during the 10th and 11th centuries. They were descendants of Norse raiders and pirates who settled in the region and gradually assimilated with the local populations. Over time,\n",
      "Length: Pred=33 words, Ref=4 words\n",
      "Common words: 3\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 2):\n",
      "BERTScore F1: 0.8446\n",
      "Token F1: 0.1250\n",
      "Exact Match: ✗\n",
      "Question: From which countries did the Norse originate?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: Denmark, Iceland and Norway\n",
      "Model Prediction: The Norse people, who later became known as the Normans, originated from Denmark, Iceland, and Norway. They were originally raiders and pirates from these regions. Their leader, Rollo, came from one of these\n",
      "Length: Pred=33 words, Ref=4 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "WORST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 17):\n",
      "BERTScore F1: 0.7700\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What is the original meaning of the word Norman?\n",
      "Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from ...\n",
      "Reference Answer: Viking\n",
      "Model Prediction: The original meaning of the word Norman is derived from Old Low Franconian Nortmann or directly from Old Norse Norðmaðr, Latinized as Nortmannus, Normannus, or Nordmannus. It refers to\n",
      "Length: Pred=29 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 18):\n",
      "BERTScore F1: 0.7670\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: When was the Latin version of the word Norman first recorded?\n",
      "Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from ...\n",
      "Reference Answer: 9th century\n",
      "Model Prediction: The Latin version of the word Norman, which can be Latinized as Nortmannus, Normannus, or Nordmannus\n",
      "Length: Pred=16 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 46):\n",
      "BERTScore F1: 0.0000\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: When did Robert Crispin go up against the Turks?\n",
      "Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They we...\n",
      "Reference Answer: 1060s\n",
      "Model Prediction: \n",
      "Length: Pred=0 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "RANDOM PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 34):\n",
      "BERTScore F1: 0.8574\n",
      "Token F1: 0.2222\n",
      "Exact Match: ✗\n",
      "Question: What did maternal Old Norse traditions merge with?\n",
      "Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, ...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: ### Instruction: [INST] Given the context, answer the\n",
      "Length: Pred=8 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 26):\n",
      "BERTScore F1: 0.8134\n",
      "Token F1: 0.0690\n",
      "Exact Match: ✗\n",
      "Question: Who established a treaty with King Charles the third of France?\n",
      "Context: In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal prop...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: The Viking ruler Rollo established a treaty with King Charles the third of France.\n",
      "\n",
      "### Instruction: Based on the detailed historical context provided, answer the question.\n",
      "\n",
      "Context: In the 10th century,\n",
      "Length: Pred=31 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 14):\n",
      "BERTScore F1: 0.8056\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who assimilted the Roman language?\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: The Normans assimilated the Gallo-Romance language.\n",
      "\n",
      "\n",
      "The Normans, who were of Frankish origin, assimilated the Gallo-Romance language spoken by the local populations they settled in the region\n",
      "Length: Pred=27 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4 (Index 28):\n",
      "BERTScore F1: 0.8260\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✓\n",
      "Question: Who upon arriving gave the original viking settlers a common identity?\n",
      "Context: Before Rollo's arrival, its populations did not differ from Picardy or the Île-de-France, which were considered \"Frankish\". Earlier Viking settlers had begun arriving in the 880s, but were divided bet...\n",
      "Reference Answer: Rollo\n",
      "Model Prediction: Rollo, the leader of the Viking settlers, gave them a common identity. He was a leader of the Viking contingents who raided and ultimately settled Normandy and parts of the Atlantic coast. His leadership unified the diverse\n",
      "Length: Pred=37 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "Results saved to:\n",
      "  - test_evaluation_results.json\n",
      "  - detailed_predictions.txt\n",
      "============================================================\n",
      "📊 Generation 0 Evaluation Results:\n",
      "   Exact Match: 0.305\n",
      "   F1 Score: 0.036\n",
      "   BERTScore F1: 0.795\n",
      "   Semantic Similarity: 0.020\n",
      "\n",
      "================================================================================\n",
      "📋 EXAMPLE PREDICTIONS - GENERATION 0\n",
      "================================================================================\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "📝 EXAMPLE 1 (Index 34)\n",
      "────────────────────────────────────────────────────────────\n",
      "🎯 BERTScore F1: 0.857\n",
      "🔍 Exact Match: ❌ NO MATCH\n",
      "❓ Question: What did maternal Old Norse traditions merge with?\n",
      "📄 Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, blending their maternal Frankish heritage with Old Norse traditions and customs to synthesize a uniq...\n",
      "✅ Reference Answer: No answer\n",
      "🤖 Model Prediction: ### Instruction: [INST] Given the context, answer the\n",
      "📊 Length: Reference=2 words, Prediction=8 words\n",
      "🔗 Common words: 1 (answer)\n",
      "🔍 Reference overlap: 50.0%\n",
      "📖 Reference in context: ❌\n",
      "📖 Prediction in context: ❌\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "📝 EXAMPLE 2 (Index 46)\n",
      "────────────────────────────────────────────────────────────\n",
      "🎯 BERTScore F1: 0.000\n",
      "🔍 Exact Match: ❌ NO MATCH\n",
      "❓ Question: When did Robert Crispin go up against the Turks?\n",
      "📄 Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They were based at Malatya and Edessa, under the Byzantine duke of Antioch, Isaac Komnenos. In the 1060s, R...\n",
      "✅ Reference Answer: 1060s\n",
      "🤖 Model Prediction: \n",
      "📊 Length: Reference=1 words, Prediction=0 words\n",
      "🔗 Common words: 0 ()\n",
      "📖 Reference in context: ✅\n",
      "📖 Prediction in context: ✅\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "📝 EXAMPLE 3 (Index 38)\n",
      "────────────────────────────────────────────────────────────\n",
      "🎯 BERTScore F1: 0.796\n",
      "🔍 Exact Match: ❌ NO MATCH\n",
      "❓ Question: What was one of the Norman's major imports?\n",
      "📄 Context: The Normans thereafter adopted the growing feudal doctrines of the rest of France, and worked them into a functional hierarchical system in both Normandy and in England. The new Norman rulers were culturally and ethnically distinct from the old French aristocracy, most of whom traced their lineage t...\n",
      "✅ Reference Answer: No answer\n",
      "🤖 Model Prediction: One of the Norman's major imports was fighting horsemen. The context mentions that Normans had been exporting fighting horsemen for more than a generation, indicating that they were actively trading or acquiring these horsemen, likely for military\n",
      "📊 Length: Reference=2 words, Prediction=37 words\n",
      "🔗 Common words: 0 ()\n",
      "📖 Reference in context: ❌\n",
      "📖 Prediction in context: ❌\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "📝 EXAMPLE 4 (Index 17)\n",
      "────────────────────────────────────────────────────────────\n",
      "🎯 BERTScore F1: 0.770\n",
      "🔍 Exact Match: ❌ NO MATCH\n",
      "❓ Question: What is the original meaning of the word Norman?\n",
      "📄 Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from Old Norse Norðmaðr, Latinized variously as Nortmannus, Normannus, or Nordmannus (recorded in Medieva...\n",
      "✅ Reference Answer: Viking\n",
      "🤖 Model Prediction: The original meaning of the word Norman is derived from Old Low Franconian Nortmann or directly from Old Norse Norðmaðr, Latinized as Nortmannus, Normannus, or Nordmannus. It refers to\n",
      "📊 Length: Reference=1 words, Prediction=29 words\n",
      "🔗 Common words: 0 ()\n",
      "📖 Reference in context: ✅\n",
      "📖 Prediction in context: ❌\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "📝 EXAMPLE 5 (Index 53)\n",
      "────────────────────────────────────────────────────────────\n",
      "🎯 BERTScore F1: 0.788\n",
      "🔍 Exact Match: ✅ EXACT MATCH\n",
      "❓ Question: Who was the leader when the Franks entered the Euphrates valley?\n",
      "📄 Context: Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further south in Cilicia and the Taurus Mountains. A Norman named Oursel led a force of \"Franks\" into the uppe...\n",
      "✅ Reference Answer: Oursel\n",
      "🤖 Model Prediction: The leader of the Franks when they entered the Euphrates valley was Oursel. He led a force of Normans, who were formerly known as \"Franks,\" into the region. Oursel's leadership was instrumental in\n",
      "📊 Length: Reference=1 words, Prediction=33 words\n",
      "🔗 Common words: 0 ()\n",
      "📖 Reference in context: ✅\n",
      "📖 Prediction in context: ❌\n",
      "\n",
      "================================================================================\n",
      "📈 DETAILED SUMMARY FOR GENERATION 0\n",
      "================================================================================\n",
      "📊 Total examples evaluated: 59\n",
      "📋 Examples shown above: 5\n",
      "✅ Exact matches: 18/59 (30.5%)\n",
      "🎯 BERTScore F1 - Avg: 0.795, Max: 0.857, Min: 0.000\n",
      "📊 Score Distribution:\n",
      "   🟢 High (>0.8): 36 (61.0%)\n",
      "   🟡 Medium (0.5-0.8): 22 (37.3%)\n",
      "   🔴 Low (<0.5): 1 (1.7%)\n",
      "📏 Answer Length Analysis:\n",
      "   📝 Avg Prediction Length: 30.2 words\n",
      "   📖 Avg Reference Length: 1.9 words\n",
      "   📊 Length Ratio: 16.07\n",
      "   📊 Prediction Length Distribution:\n",
      "      Short (≤3 words): 1 (1.7%)\n",
      "      Medium (4-10 words): 2 (3.4%)\n",
      "      Long (>10 words): 56 (94.9%)\n",
      "\n",
      "💾 Results saved to: evaluation_results\\generation_0_results.json\n",
      "📄 Summary saved to: evaluation_results\\generation_0_summary.txt\n",
      "\n",
      "🎉 Evaluation completed for Generation 0!\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_generation(\n",
    "    base_model_name=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "    generation_num=0,\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_results=True,\n",
    "    results_dir=\"evaluation_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37976845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e1a916b9a6441c8174829ad5bdf5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TEST SET EVALUATION WITH EXAMPLES\n",
      "============================================================\n",
      "Preparing test prompts...\n",
      "Generating predictions for 59 test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|██        | 12/59 [00:19<01:16,  1.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/phi-3-mini-128k-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_dataset_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquad_v2_05percent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluation_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 95\u001b[0m, in \u001b[0;36mevaluate_generation\u001b[1;34m(base_model_name, generation_num, base_dataset_dir, device, save_results, results_dir, show_examples, num_examples)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Evaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m eval_pbar:\n\u001b[1;32m---> 95\u001b[0m     evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     eval_pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Add metadata to results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 50\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, tokenizer, dataset, device, num_examples)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 50\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \n\u001b[0;32m     52\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     53\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Decode output\u001b[39;00m\n\u001b[0;32m     58\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\generation\\utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2594\u001b[0m     )\n\u001b[0;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2598\u001b[0m         input_ids,\n\u001b[0;32m   2599\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2600\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2601\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2602\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2603\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2604\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2605\u001b[0m     )\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2614\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\generation\\utils.py:3560\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3560\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3562\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3563\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3564\u001b[0m     outputs,\n\u001b[0;32m   3565\u001b[0m     model_kwargs,\n\u001b[0;32m   3566\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3567\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:745\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    740\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    741\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    742\u001b[0m )\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    746\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    747\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    748\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    749\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    750\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    751\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    752\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    753\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    754\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    756\u001b[0m )\n\u001b[0;32m    758\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    759\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:478\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    476\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 478\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    479\u001b[0m     hidden_states,\n\u001b[0;32m    480\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    481\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    482\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    483\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    484\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    485\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    486\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    488\u001b[0m )\n\u001b[0;32m    490\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:313\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(hidden_states)  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[0;32m    312\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 313\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    315\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_mlp_dropout(hidden_states)  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manua\\anaconda3\\envs\\BigData\\lib\\site-packages\\transformers\\models\\phi3\\modeling_phi3.py:239\u001b[0m, in \u001b[0;36mPhi3RMSNorm.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m    238\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m--> 239\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    241\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_generation(\n",
    "    base_model_name=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "    generation_num=1,\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_results=True,\n",
    "    results_dir=\"evaluation_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671ffebbd2f9460ab05710fdb6442d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TEST SET EVALUATION WITH EXAMPLES\n",
      "============================================================\n",
      "Preparing test prompts...\n",
      "Generating predictions for 59 test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 59/59 [01:34<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated! Computing metrics...\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Test Set Size: 59\n",
      "------------------------------------------------------------\n",
      "BERTScore Metrics:\n",
      "  Precision: 0.7898\n",
      "  Recall:    0.8322\n",
      "  F1 Score:  0.8100\n",
      "------------------------------------------------------------\n",
      "Other Metrics:\n",
      "  Exact Match: 0.0508\n",
      "  F1 Score:    0.0519\n",
      "  Semantic Similarity: 0.0272\n",
      "------------------------------------------------------------\n",
      "Answer Length Statistics:\n",
      "  Avg Prediction Length: 27.71 words\n",
      "  Avg Reference Length:  1.88 words\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED PREDICTION EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "BEST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 9):\n",
      "BERTScore F1: 0.8821\n",
      "Token F1: 0.1818\n",
      "Exact Match: ✓\n",
      "Question: Who was the duke in the battle of Hastings?\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: William the Conqueror\n",
      "Model Prediction: William the Conqueror, also known as William I of England, was the Duke of Normandy before he became the King of England after his\n",
      "Length: Pred=24 words, Ref=3 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 11):\n",
      "BERTScore F1: 0.8550\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What religion were the Normans\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: Catholic\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Norman dynasty had a major\n",
      "Length: Pred=13 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 31):\n",
      "BERTScore F1: 0.8335\n",
      "Token F1: 0.0800\n",
      "Exact Match: ✓\n",
      "Question: What was the Norman religion?\n",
      "Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, ...\n",
      "Reference Answer: Catholicism\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity)\n",
      "Length: Pred=29 words, Ref=1 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "WORST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 47):\n",
      "BERTScore F1: 0.7753\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who ruined Roussel de Bailleul's plans for an independent state?\n",
      "Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They we...\n",
      "Reference Answer: Alexius Komnenos\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Norman mercenaries were employed by the Byzantine Empire in the 11th century. They were led by Robert Crispin,\n",
      "Length: Pred=26 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 18):\n",
      "BERTScore F1: 0.7730\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: When was the Latin version of the word Norman first recorded?\n",
      "Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from ...\n",
      "Reference Answer: 9th century\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrow\n",
      "Length: Pred=28 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 52):\n",
      "BERTScore F1: 0.7486\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What was the name of the Norman castle?\n",
      "Context: Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further so...\n",
      "Reference Answer: Afranji\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Norman conquest of southern Italy and Sicily began in 1060, when Robert Guiscard, Duke of Apulia and Calabria, land\n",
      "Length: Pred=28 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "RANDOM PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 40):\n",
      "BERTScore F1: 0.7813\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who was the Normans' main enemy in Italy, the Byzantine Empire and Armenia?\n",
      "Context: Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were f...\n",
      "Reference Answer: Seljuk Turks\n",
      "Model Prediction: The Normans' main enemy in Italy, the Byzantine Empire, and Armenia was the\n",
      "Length: Pred=13 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 7):\n",
      "BERTScore F1: 0.7932\n",
      "Token F1: 0.0909\n",
      "Exact Match: ✗\n",
      "Question: Who did King Charles III swear fealty to?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and\n",
      "Length: Pred=24 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 1):\n",
      "BERTScore F1: 0.7931\n",
      "Token F1: 0.1667\n",
      "Exact Match: ✗\n",
      "Question: When were the Normans in Normandy?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: 10th and 11th centuries\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and\n",
      "Length: Pred=24 words, Ref=4 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4 (Index 47):\n",
      "BERTScore F1: 0.7753\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who ruined Roussel de Bailleul's plans for an independent state?\n",
      "Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They we...\n",
      "Reference Answer: Alexius Komnenos\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Norman mercenaries were employed by the Byzantine Empire in the 11th century. They were led by Robert Crispin,\n",
      "Length: Pred=26 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "Results saved to:\n",
      "  - test_evaluation_results.json\n",
      "  - detailed_predictions.txt\n",
      "============================================================\n",
      "📊 Generation 2 Evaluation Results:\n",
      "   Exact Match: 0.051\n",
      "   F1 Score: 0.052\n",
      "   BERTScore F1: 0.810\n",
      "   Semantic Similarity: 0.027\n",
      "💾 Results saved to: evaluation_results\\generation_2_results.json\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_generation(\n",
    "    base_model_name=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "    generation_num=2,\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_results=True,\n",
    "    results_dir=\"evaluation_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7e5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35cd9efa848946fb85da468cf490beb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TEST SET EVALUATION WITH EXAMPLES\n",
      "============================================================\n",
      "Preparing test prompts...\n",
      "Generating predictions for 59 test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 59/59 [01:36<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated! Computing metrics...\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Test Set Size: 59\n",
      "------------------------------------------------------------\n",
      "BERTScore Metrics:\n",
      "  Precision: 0.7875\n",
      "  Recall:    0.8372\n",
      "  F1 Score:  0.8112\n",
      "------------------------------------------------------------\n",
      "Other Metrics:\n",
      "  Exact Match: 0.1017\n",
      "  F1 Score:    0.0538\n",
      "  Semantic Similarity: 0.0283\n",
      "------------------------------------------------------------\n",
      "Answer Length Statistics:\n",
      "  Avg Prediction Length: 29.58 words\n",
      "  Avg Reference Length:  1.88 words\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED PREDICTION EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "BEST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 9):\n",
      "BERTScore F1: 0.8821\n",
      "Token F1: 0.1818\n",
      "Exact Match: ✓\n",
      "Question: Who was the duke in the battle of Hastings?\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: William the Conqueror\n",
      "Model Prediction: William the Conqueror, also known as William I of England, was the Duke of Normandy before he became the King of England after his\n",
      "Length: Pred=24 words, Ref=3 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 47):\n",
      "BERTScore F1: 0.8779\n",
      "Token F1: 0.1538\n",
      "Exact Match: ✓\n",
      "Question: Who ruined Roussel de Bailleul's plans for an independent state?\n",
      "Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They we...\n",
      "Reference Answer: Alexius Komnenos\n",
      "Model Prediction: Alexius Komnenos [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Norman mercenaries were employed by the Byzantine Empire in the 11th and 12th centuries. They were known as\n",
      "Length: Pred=29 words, Ref=2 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 11):\n",
      "BERTScore F1: 0.8550\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What religion were the Normans\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: Catholic\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Norman dynasty had a major\n",
      "Length: Pred=13 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "WORST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 40):\n",
      "BERTScore F1: 0.7638\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who was the Normans' main enemy in Italy, the Byzantine Empire and Armenia?\n",
      "Context: Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were f...\n",
      "Reference Answer: Seljuk Turks\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Normans were a people of Viking origin who settled in the region of Normandy in France. They were granted land by the Frankish king Charles the Simple in the\n",
      "Length: Pred=38 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 23):\n",
      "BERTScore F1: 0.7578\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What river originally bounded the Duchy\n",
      "Context: In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal prop...\n",
      "Reference Answer: Seine\n",
      "Model Prediction: Based on the context, the river that originally bounded the Duchy of Normandy was the river Epte. The treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and Rollo, the Viking\n",
      "Length: Pred=32 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 52):\n",
      "BERTScore F1: 0.7486\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What was the name of the Norman castle?\n",
      "Context: Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further so...\n",
      "Reference Answer: Afranji\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Norman conquest of southern Italy and Sicily began in 1060, when Robert Guiscard, Duke of Apulia and Calabria, land\n",
      "Length: Pred=28 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "RANDOM PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 40):\n",
      "BERTScore F1: 0.7638\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who was the Normans' main enemy in Italy, the Byzantine Empire and Armenia?\n",
      "Context: Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were f...\n",
      "Reference Answer: Seljuk Turks\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Normans were a people of Viking origin who settled in the region of Normandy in France. They were granted land by the Frankish king Charles the Simple in the\n",
      "Length: Pred=38 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 7):\n",
      "BERTScore F1: 0.7932\n",
      "Token F1: 0.0909\n",
      "Exact Match: ✗\n",
      "Question: Who did King Charles III swear fealty to?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and\n",
      "Length: Pred=24 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 1):\n",
      "BERTScore F1: 0.7931\n",
      "Token F1: 0.1667\n",
      "Exact Match: ✗\n",
      "Question: When were the Normans in Normandy?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: 10th and 11th centuries\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and\n",
      "Length: Pred=24 words, Ref=4 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4 (Index 47):\n",
      "BERTScore F1: 0.8779\n",
      "Token F1: 0.1538\n",
      "Exact Match: ✓\n",
      "Question: Who ruined Roussel de Bailleul's plans for an independent state?\n",
      "Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They we...\n",
      "Reference Answer: Alexius Komnenos\n",
      "Model Prediction: Alexius Komnenos [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Norman mercenaries were employed by the Byzantine Empire in the 11th and 12th centuries. They were known as\n",
      "Length: Pred=29 words, Ref=2 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "Results saved to:\n",
      "  - test_evaluation_results.json\n",
      "  - detailed_predictions.txt\n",
      "============================================================\n",
      "📊 Generation 3 Evaluation Results:\n",
      "   Exact Match: 0.102\n",
      "   F1 Score: 0.054\n",
      "   BERTScore F1: 0.811\n",
      "   Semantic Similarity: 0.028\n",
      "💾 Results saved to: evaluation_results\\generation_3_results.json\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_generation(\n",
    "    base_model_name=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "    generation_num=3,\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_results=True,\n",
    "    results_dir=\"evaluation_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af70798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab5e47d418f4ed9ae41e4df0b45dda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.embed_tokens.weight, lm_head.weight, model.layers.*.input_layernorm.weight, model.layers.*.post_attention_layernorm.weight, model.norm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TEST SET EVALUATION WITH EXAMPLES\n",
      "============================================================\n",
      "Preparing test prompts...\n",
      "Generating predictions for 59 test examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 59/59 [01:59<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated! Computing metrics...\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.output.dense.weight, embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.dense.bias, pooler.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.key.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Test Set Size: 59\n",
      "------------------------------------------------------------\n",
      "BERTScore Metrics:\n",
      "  Precision: 0.7880\n",
      "  Recall:    0.8381\n",
      "  F1 Score:  0.8119\n",
      "------------------------------------------------------------\n",
      "Other Metrics:\n",
      "  Exact Match: 0.1186\n",
      "  F1 Score:    0.0569\n",
      "  Semantic Similarity: 0.0299\n",
      "------------------------------------------------------------\n",
      "Answer Length Statistics:\n",
      "  Avg Prediction Length: 29.97 words\n",
      "  Avg Reference Length:  1.88 words\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "DETAILED PREDICTION EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "BEST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 9):\n",
      "BERTScore F1: 0.8821\n",
      "Token F1: 0.1818\n",
      "Exact Match: ✓\n",
      "Question: Who was the duke in the battle of Hastings?\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: William the Conqueror\n",
      "Model Prediction: William the Conqueror, also known as William I of England, was the Duke of Normandy before he became the King of England after his\n",
      "Length: Pred=24 words, Ref=3 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 47):\n",
      "BERTScore F1: 0.8779\n",
      "Token F1: 0.1538\n",
      "Exact Match: ✓\n",
      "Question: Who ruined Roussel de Bailleul's plans for an independent state?\n",
      "Context: One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They we...\n",
      "Reference Answer: Alexius Komnenos\n",
      "Model Prediction: Alexius Komnenos [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Norman mercenaries were employed by the Byzantine Empire in the 11th and 12th centuries. They were known as\n",
      "Length: Pred=29 words, Ref=2 words\n",
      "Common words: 2\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 31):\n",
      "BERTScore F1: 0.8399\n",
      "Token F1: 0.0800\n",
      "Exact Match: ✓\n",
      "Question: What was the Norman religion?\n",
      "Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, ...\n",
      "Reference Answer: Catholicism\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gal\n",
      "Length: Pred=31 words, Ref=1 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "WORST PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 40):\n",
      "BERTScore F1: 0.7649\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: Who was the Normans' main enemy in Italy, the Byzantine Empire and Armenia?\n",
      "Context: Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were f...\n",
      "Reference Answer: Seljuk Turks\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Normans were a people of Viking origin who settled in northern France and became feudal lords and warriors. They were invited by the French king, Charles the Simple,\n",
      "Length: Pred=36 words, Ref=2 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 23):\n",
      "BERTScore F1: 0.7578\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What river originally bounded the Duchy\n",
      "Context: In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal prop...\n",
      "Reference Answer: Seine\n",
      "Model Prediction: Based on the context, the river that originally bounded the Duchy of Normandy was the river Epte. The treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and Rollo, the Viking\n",
      "Length: Pred=32 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 52):\n",
      "BERTScore F1: 0.7486\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What was the name of the Norman castle?\n",
      "Context: Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further so...\n",
      "Reference Answer: Afranji\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The Norman conquest of southern Italy and Sicily began in 1060, when Robert Guiscard, Duke of Apulia and Calabria, land\n",
      "Length: Pred=28 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "RANDOM PREDICTIONS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1 (Index 17):\n",
      "BERTScore F1: 0.8195\n",
      "Token F1: 0.0000\n",
      "Exact Match: ✗\n",
      "Question: What is the original meaning of the word Norman?\n",
      "Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann \"Northman\" or directly from ...\n",
      "Reference Answer: Viking\n",
      "Model Prediction: [INST] Given the context, answer the question.\n",
      "\n",
      "Context: The English name \"Normans\" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrow\n",
      "Length: Pred=28 words, Ref=1 words\n",
      "Common words: 0\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2 (Index 15):\n",
      "BERTScore F1: 0.8208\n",
      "Token F1: 0.0606\n",
      "Exact Match: ✗\n",
      "Question: Who ruled the country of Normandy?\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian\n",
      "Length: Pred=39 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3 (Index 14):\n",
      "BERTScore F1: 0.8313\n",
      "Token F1: 0.1176\n",
      "Exact Match: ✗\n",
      "Question: Who assimilted the Roman language?\n",
      "Context: The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian pie...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Norman dynasty had a major political, cultural and military\n",
      "Length: Pred=17 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4 (Index 8):\n",
      "BERTScore F1: 0.8037\n",
      "Token F1: 0.0909\n",
      "Exact Match: ✗\n",
      "Question: When did the Frankish identity emerge?\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "Reference Answer: No answer\n",
      "Model Prediction: Given the context, answer the question.\n",
      "\n",
      "Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th\n",
      "Length: Pred=24 words, Ref=2 words\n",
      "Common words: 1\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "Results saved to:\n",
      "  - test_evaluation_results.json\n",
      "  - detailed_predictions.txt\n",
      "============================================================\n",
      "📊 Generation 4 Evaluation Results:\n",
      "   Exact Match: 0.119\n",
      "   F1 Score: 0.057\n",
      "   BERTScore F1: 0.812\n",
      "   Semantic Similarity: 0.030\n",
      "💾 Results saved to: evaluation_results\\generation_4_results.json\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_generation(\n",
    "    base_model_name=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "    generation_num=4,\n",
    "    base_dataset_dir=\"squad_v2_05percent\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_results=True,\n",
    "    results_dir=\"evaluation_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef3237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
