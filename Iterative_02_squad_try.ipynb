{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecb8a69",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d4eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from bert_score import score\n",
    "from datasets import Dataset, load_from_disk, DatasetDict\n",
    "from dotenv import load_dotenv\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88e747",
   "metadata": {},
   "source": [
    "## Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3548d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad_subset(dataset_dir=\"squad_v2_05percent\"):\n",
    "    \"\"\"\n",
    "    Load the saved SQuAD v2 0.5% dataset from disk.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir (str): Directory where the dataset was saved\n",
    "        \n",
    "    Returns:\n",
    "        DatasetDict: The loaded dataset with train and test splits\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{dataset_dir}' not found. Please run the extraction script first.\")\n",
    "    \n",
    "    print(f\"Loading dataset from {dataset_dir}...\")\n",
    "    \n",
    "    # Load the dataset using Hugging Face datasets\n",
    "    dataset = load_from_disk(dataset_dir)\n",
    "    \n",
    "    # Load and display metadata\n",
    "    metadata_path = os.path.join(dataset_dir, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"Dataset metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"\\nLoaded dataset splits:\")\n",
    "    for split_name, split_data in dataset.items():\n",
    "        print(f\"  {split_name}: {len(split_data)} examples\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_synthetic_train(base_dataset_dir, train_dataset_dir):\n",
    "    \"\"\"\n",
    "    Load the saved synthetic train dataset from disk, merged with the test dataset if available.\n",
    "    \n",
    "    Args:\n",
    "        base_dataset_dir (str): Base directory where the dataset was saved, in order to retrieve the test\n",
    "        train_dataset_dir (str): Directory where the previous generation train dataset was saved\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: The loaded train dataset\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{train_dataset_dir}' not found.\")\n",
    "    if not os.path.exists(train_dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory '{train_dataset_dir}' not found.\")\n",
    "    \n",
    "    print(f\"Loading train dataset from {train_dataset_dir}...\")\n",
    "    \n",
    "    # Try to load the full dataset first\n",
    "    try:\n",
    "        full_dataset = load_from_disk(base_dataset_dir)\n",
    "        train_dataset = load_from_disk(train_dataset_dir)\n",
    "        \n",
    "        # Check if it's a DatasetDict with train split\n",
    "        if isinstance(full_dataset, dict) and 'train' in full_dataset:\n",
    "            train_dataset = full_dataset['train']\n",
    "        elif hasattr(full_dataset, 'column_names'):\n",
    "            # It's already a single Dataset\n",
    "            train_dataset = full_dataset\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected dataset format in {train_dataset_dir}\")\n",
    "            \n",
    "    except Exception:\n",
    "        # Try to load from train subdirectory if full dataset fails\n",
    "        train_dir = os.path.join(train_dataset_dir, \"train\")\n",
    "        if os.path.exists(train_dir):\n",
    "            train_dataset = load_from_disk(train_dir)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No train dataset found in {train_dataset_dir}\")\n",
    "    \n",
    "    # Load and display metadata if available\n",
    "    metadata_path = os.path.join(train_dataset_dir, \"metadata.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"Dataset metadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Display train dataset info\n",
    "    #print(f\"\\nLoaded train dataset:\")\n",
    "    #print(f\"  Examples: {len(train_dataset)}\")\n",
    "    #print(f\"  Columns: {train_dataset.column_names}\")\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': full_dataset['test']\n",
    "    })\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259e321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from squad_v2_05percent...\n",
      "Dataset metadata:\n",
      "  original_train_size: 130319\n",
      "  original_validation_size: 11873\n",
      "  extracted_train_size: 651\n",
      "  extracted_test_size: 59\n",
      "  extraction_percentage: 0.5\n",
      "\n",
      "Loaded dataset splits:\n",
      "  train: 651 examples\n",
      "  test: 59 examples\n",
      "\n",
      "Example from each split:\n",
      "Train: When did Beyonce start becoming popular?\n",
      "Test: In what country is Normandy located?\n",
      "\n",
      "Dataset features: {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_squad_subset(\"squad_v2_05percent\")\n",
    "    \n",
    "# Show examples\n",
    "print(f\"\\nExample from each split:\")\n",
    "print(f\"Train: {dataset['train'][0]['question']}\")\n",
    "print(f\"Test: {dataset['test'][0]['question']}\")\n",
    "\n",
    "# Access specific fields\n",
    "print(f\"\\nDataset features: {dataset['train'].features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774e7922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " [INST] Given the context, answer the question.\n",
      "\n",
      "Context: Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc√©'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "\n",
      "Question: When did Beyonce start becoming popular? [/INST] Answer: in the late 1990s\n",
      "\n",
      "Reference Answer:\n",
      " in the late 1990s\n"
     ]
    }
   ],
   "source": [
    "def make_prompt(example):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"No answer\"\n",
    "\n",
    "    prompt = f\"[INST] Given the context, answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question} [/INST] Answer: {answer}\"\n",
    "    return {\"prompt\": prompt, \"reference\": answer}\n",
    "\n",
    "formatted_dataset = {\n",
    "    split: dataset[split].map(make_prompt)\n",
    "    for split in dataset.keys()\n",
    "}\n",
    "\n",
    "# Print the first formatted prompt and its reference answer\n",
    "print(\"Prompt:\\n\", formatted_dataset[\"train\"][0][\"prompt\"])\n",
    "print(\"\\nReference Answer:\\n\", formatted_dataset[\"train\"][0][\"reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79667a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"key.env\")\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=token)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b104edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced logging suppression - including BERTScore sharding messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"accelerate.utils.modeling\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395fcac",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0073640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERTScore silently...\n",
      "BERTScore initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERTScore silently\n",
    "print(\"Initializing BERTScore silently...\")\n",
    "with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "    from bert_score import score\n",
    "    _ = score([\"test\"], [\"test\"], lang=\"en\", verbose=False)\n",
    "print(\"BERTScore initialized successfully!\")\n",
    "\n",
    "# Modified BERTScore function with complete output suppression\n",
    "def silent_bert_score(cands, refs, lang=\"en\"):\n",
    "    \"\"\"BERTScore calculation with all output suppressed\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    old_stderr = sys.stderr\n",
    "    \n",
    "    sys.stdout = io.StringIO()\n",
    "    sys.stderr = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        P, R, F1 = score(cands, refs, lang=lang, verbose=False)\n",
    "        return P, R, F1\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stderr = old_stderr\n",
    "\n",
    "# Custom Early Stopping based on Training Loss\n",
    "class TrainingLossEarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait_count = 0\n",
    "        \n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        if logs is not None and 'train_loss' in logs:\n",
    "            current_loss = logs['train_loss']\n",
    "            \n",
    "            if current_loss < self.best_loss - self.min_delta:\n",
    "                self.best_loss = current_loss\n",
    "                self.wait_count = 0\n",
    "                print(f\"üìà Training loss improved to {current_loss:.4f}\")\n",
    "            else:\n",
    "                self.wait_count += 1\n",
    "                print(f\"üìä No improvement in training loss ({self.wait_count}/{self.patience})\")\n",
    "                \n",
    "                if self.wait_count >= self.patience:\n",
    "                    print(f\"üõë Early stopping triggered! Best loss: {self.best_loss:.4f}\")\n",
    "                    control.should_training_stop = True\n",
    "\n",
    "# Fixed Custom Trainer class with BERTScore loss\n",
    "class BERTScoreTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.label_names = [\"labels\"]\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Custom loss function using BERTScore - completely silent\n",
    "        Extracts only the answer part after \"Answer: \" for BERTScore computation\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Temporarily disable cache for forward pass\n",
    "        model.config.use_cache = False\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Generate predictions for BERTScore\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Re-enable cache for generation\n",
    "            model.config.use_cache = True\n",
    "            \n",
    "            # Generate text\n",
    "            try:\n",
    "                generated = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.processing_class.eos_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "                \n",
    "                # Decode predictions and references\n",
    "                pred_texts = self.processing_class.batch_decode(generated, skip_special_tokens=True)\n",
    "                ref_texts = self.processing_class.batch_decode(labels, skip_special_tokens=True)\n",
    "                \n",
    "                # Extract only the answer part from both predictions and references\n",
    "                extracted_preds = []\n",
    "                extracted_refs = []\n",
    "                \n",
    "                for pred_text in pred_texts:\n",
    "                    # Extract answer after \"Answer: \" in prediction\n",
    "                    if \"Answer: \" in pred_text:\n",
    "                        answer_part = pred_text.split(\"Answer: \")[-1].strip()\n",
    "                    elif \"[/INST]\" in pred_text:\n",
    "                        # Fallback: extract after [/INST] if \"Answer: \" not found\n",
    "                        answer_part = pred_text.split(\"[/INST]\")[-1].strip()\n",
    "                        # Remove \"Answer: \" prefix if it exists\n",
    "                        if answer_part.startswith(\"Answer: \"):\n",
    "                            answer_part = answer_part[8:].strip()\n",
    "                    else:\n",
    "                        answer_part = pred_text.strip()\n",
    "                    \n",
    "                    extracted_preds.append(answer_part)\n",
    "                \n",
    "                for ref_text in ref_texts:\n",
    "                    # Extract answer after \"Answer: \" in reference\n",
    "                    if \"Answer: \" in ref_text:\n",
    "                        answer_part = ref_text.split(\"Answer: \")[-1].strip()\n",
    "                    elif \"[/INST]\" in ref_text:\n",
    "                        # Fallback: extract after [/INST] if \"Answer: \" not found\n",
    "                        answer_part = ref_text.split(\"[/INST]\")[-1].strip()\n",
    "                        # Remove \"Answer: \" prefix if it exists\n",
    "                        if answer_part.startswith(\"Answer: \"):\n",
    "                            answer_part = answer_part[8:].strip()\n",
    "                    else:\n",
    "                        answer_part = ref_text.strip()\n",
    "                    \n",
    "                    extracted_refs.append(answer_part)\n",
    "                \n",
    "                # Calculate BERTScore with completely silent function on extracted answers only\n",
    "                P, R, F1 = silent_bert_score(extracted_preds, extracted_refs, lang=\"en\")\n",
    "                bert_f1 = F1.mean().item()\n",
    "                \n",
    "                # Convert BERTScore to loss\n",
    "                bert_loss = torch.tensor(1.0 - bert_f1, requires_grad=True, device=input_ids.device)\n",
    "            except Exception as e:\n",
    "                # Fallback to standard loss if BERTScore fails\n",
    "                bert_loss = outputs.loss\n",
    "            finally:\n",
    "                # Disable cache again for gradient checkpointing compatibility\n",
    "                model.config.use_cache = False\n",
    "        \n",
    "        # Combine with standard language modeling loss\n",
    "        standard_loss = outputs.loss\n",
    "        combined_loss = 0.7 * standard_loss + 0.3 * bert_loss\n",
    "        \n",
    "        return (combined_loss, outputs) if return_outputs else combined_loss\n",
    "\n",
    "# Data preparation function - removed tokenizer parameter since it's not used\n",
    "def prepare_training_data(tokenized_dataset):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    \n",
    "    def add_labels(example):\n",
    "        example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "        return example\n",
    "\n",
    "    # Only prepare train split\n",
    "    train_dataset = tokenized_dataset[\"train\"].map(add_labels)\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    keep_keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    train_dataset = train_dataset.remove_columns(\n",
    "        [col for col in train_dataset.column_names if col not in keep_keys]\n",
    "    )\n",
    "    \n",
    "    return {\"train\": train_dataset}\n",
    "\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    \"\"\"Remove checkpoint directories and files\"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        # Find all checkpoint directories\n",
    "        checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith('checkpoint-')]\n",
    "        \n",
    "        for checkpoint_dir in checkpoint_dirs:\n",
    "            checkpoint_path = os.path.join(output_dir, checkpoint_dir)\n",
    "            if os.path.isdir(checkpoint_path):\n",
    "                print(f\"üóëÔ∏è Removing checkpoint: {checkpoint_path}\")\n",
    "                shutil.rmtree(checkpoint_path)\n",
    "        \n",
    "        # Remove any other checkpoint-related files\n",
    "        checkpoint_files = [f for f in os.listdir(output_dir) if 'checkpoint' in f.lower()]\n",
    "        for checkpoint_file in checkpoint_files:\n",
    "            file_path = os.path.join(output_dir, checkpoint_file)\n",
    "            if os.path.isfile(file_path):\n",
    "                print(f\"üóëÔ∏è Removing checkpoint file: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "        \n",
    "        print(\"‚úÖ Checkpoint cleanup completed!\")\n",
    "\n",
    "def configure_model_for_training(model):\n",
    "    \"\"\"Configure model for training with proper cache settings\"\"\"\n",
    "    \n",
    "    # Disable use_cache for training compatibility with gradient checkpointing\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = False\n",
    "        print(\"‚úÖ Set use_cache=False for gradient checkpointing compatibility\")\n",
    "    \n",
    "    # Enable gradient checkpointing if available\n",
    "    if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"‚úÖ Enabled gradient checkpointing for memory efficiency\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main training function\n",
    "def train_model(model, tokenized_data, tokenizer, train_args):\n",
    "    \"\"\"Training function with BERTScore and early stopping\"\"\"\n",
    "    \n",
    "    # Configure model for training\n",
    "    model = configure_model_for_training(model)\n",
    "    \n",
    "    # Prepare data\n",
    "    prepared_data = prepare_training_data(tokenized_data)\n",
    "    \n",
    "    # Setup data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Custom early stopping based on training loss\n",
    "    early_stopping_callback = TrainingLossEarlyStoppingCallback(\n",
    "        patience=5,\n",
    "        min_delta=0.01\n",
    "    )\n",
    "    \n",
    "    # Initialize BERTScore Trainer\n",
    "    trainer = BERTScoreTrainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=prepared_data[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[early_stopping_callback],\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training with BERTScore optimization...\")\n",
    "    print(\"Early stopping based on training loss improvement\")\n",
    "    print(\"Cache disabled for gradient checkpointing compatibility\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Re-enable cache for inference after training\n",
    "    if hasattr(model.config, 'use_cache'):\n",
    "        model.config.use_cache = True\n",
    "        print(\"‚úÖ Re-enabled use_cache for inference\")\n",
    "    \n",
    "    # Save model\n",
    "    final_model_path = \"./phi3-squad2-final\"\n",
    "    trainer.save_model(final_model_path)\n",
    "    print(f\"‚úÖ Model saved to {final_model_path}\")\n",
    "    \n",
    "    # Clean up checkpoints after saving the final model\n",
    "    print(\"\\nüßπ Cleaning up checkpoints...\")\n",
    "    if hasattr(train_args, 'output_dir') and train_args.output_dir:\n",
    "        cleanup_checkpoints(train_args.output_dir)\n",
    "    \n",
    "    # Also clean up from the final model directory if it has checkpoints\n",
    "    cleanup_checkpoints(final_model_path)\n",
    "    \n",
    "    # Clean up any checkpoint directories in the current working directory\n",
    "    current_dir_checkpoints = [d for d in os.listdir('.') if d.startswith('checkpoint-')]\n",
    "    for checkpoint_dir in current_dir_checkpoints:\n",
    "        if os.path.isdir(checkpoint_dir):\n",
    "            print(f\"üóëÔ∏è Removing checkpoint: {checkpoint_dir}\")\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    print(\"üéâ Training completed and checkpoints cleaned up!\")\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2977f59",
   "metadata": {},
   "source": [
    "## Synthetic Answer generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09d9cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_answers(model, tokenizer, formatted_dataset, device, generation_num=1):\n",
    "    \"\"\"\n",
    "    Generate synthetic answers using the fine-tuned causal language model.\n",
    "    Takes formatted_dataset with prompts as input.\n",
    "    Updated to handle \"Answer: \" format and write \"No answer\" when appropriate.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Generating synthetic answers (Generation {generation_num})...\")\n",
    "    \n",
    "    synthetic_data = []\n",
    "    \n",
    "    # Use the train split from formatted_dataset\n",
    "    train_dataset = formatted_dataset['train']\n",
    "    \n",
    "    # Enhanced progress bar with statistics\n",
    "    progress_bar = tqdm(\n",
    "        train_dataset, \n",
    "        desc=f\"ü§ñ Gen {generation_num} - Generating answers\",\n",
    "        unit=\"examples\",\n",
    "        position=1,\n",
    "        leave=False,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "    \n",
    "    # Statistics tracking\n",
    "    successful_generations = 0\n",
    "    failed_generations = 0\n",
    "    total_examples = len(train_dataset)\n",
    "    \n",
    "    for idx, example in enumerate(progress_bar):\n",
    "        # Get the prompt that was created by make_prompt function\n",
    "        prompt = example['prompt']\n",
    "        \n",
    "        # For your format: \"[INST] ... [/INST] Answer: {answer}\"\n",
    "        # We need to generate starting from just before \"Answer:\"\n",
    "        if '[/INST] Answer:' in prompt:\n",
    "            # Remove the existing answer to create generation prompt\n",
    "            generation_prompt = prompt.split(' Answer:')[0] + ' Answer:'\n",
    "        elif '[/INST]' in prompt:\n",
    "            # Fallback: if no \"Answer:\" found, add it\n",
    "            generation_prompt = prompt.split('[/INST]')[0] + '[/INST] Answer:'\n",
    "        else:\n",
    "            # Fallback: use the full prompt\n",
    "            generation_prompt = prompt\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input - increased max_length to handle full context\n",
    "            inputs = tokenizer(\n",
    "                generation_prompt,\n",
    "                max_length=512,  # Increased from 50 to handle full context\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Generate answer using causal LM\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,   # Reduced for concise answers\n",
    "                    do_sample=True,      # Keep diversity\n",
    "                    temperature=0.7,     # Controlled randomness\n",
    "                    top_p=0.9,          # Nucleus sampling\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract only the answer part after \"Answer: \"\n",
    "            if 'Answer:' in generated_text:\n",
    "                # Split by \"Answer:\" and take the last part (the generated answer)\n",
    "                answer_parts = generated_text.split('Answer:')\n",
    "                if len(answer_parts) > 1:\n",
    "                    synthetic_answer = answer_parts[-1].strip()\n",
    "                else:\n",
    "                    synthetic_answer = \"No answer\"\n",
    "            else:\n",
    "                # Fallback: extract after [/INST] if \"Answer:\" not found\n",
    "                if '[/INST]' in generated_text:\n",
    "                    synthetic_answer = generated_text.split('[/INST]')[-1].strip()\n",
    "                    # Remove \"Answer:\" prefix if it exists\n",
    "                    if synthetic_answer.startswith('Answer:'):\n",
    "                        synthetic_answer = synthetic_answer[7:].strip()\n",
    "                else:\n",
    "                    synthetic_answer = \"No answer\"\n",
    "            \n",
    "            # Additional cleanup and validation\n",
    "            if synthetic_answer:\n",
    "                # Stop at first sentence if answer is too long\n",
    "                sentences = synthetic_answer.split('.')\n",
    "                if len(sentences) > 1 and len(sentences[0]) > 5:\n",
    "                    synthetic_answer = sentences[0].strip()\n",
    "                \n",
    "                # Remove any remaining formatting artifacts\n",
    "                synthetic_answer = synthetic_answer.replace('\\n', ' ').strip()\n",
    "                \n",
    "                # Check if answer is reasonable (not empty, not too long)\n",
    "                if len(synthetic_answer) < 2 or len(synthetic_answer) > 200:\n",
    "                    synthetic_answer = \"No answer\"\n",
    "                    failed_generations += 1\n",
    "                else:\n",
    "                    # Check if the answer exists in the context (if context is available)\n",
    "                    context = example.get('context', '')\n",
    "                    if context and synthetic_answer.lower() not in context.lower():\n",
    "                        # Answer not found in context, mark as \"No answer\"\n",
    "                        synthetic_answer = \"No answer\"\n",
    "                        failed_generations += 1\n",
    "                    else:\n",
    "                        successful_generations += 1\n",
    "            else:\n",
    "                synthetic_answer = \"No answer\"\n",
    "                failed_generations += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any generation errors\n",
    "            synthetic_answer = \"No answer\"\n",
    "            failed_generations += 1\n",
    "            print(f\"\\nWarning: Generation failed for example {idx}: {str(e)}\")\n",
    "        \n",
    "        # Create new example with synthetic answer\n",
    "        new_example = example.copy()\n",
    "        \n",
    "        # Update the prompt to include the generated answer in correct format\n",
    "        # Format: \"[INST] ... [/INST] Answer: {synthetic_answer}\"\n",
    "        if '[/INST]' in prompt:\n",
    "            base_prompt = prompt.split('[/INST]')[0] + '[/INST] Answer:'\n",
    "            new_example['prompt'] = base_prompt + ' ' + synthetic_answer\n",
    "        else:\n",
    "            new_example['prompt'] = prompt + ' ' + synthetic_answer\n",
    "        \n",
    "        # Update the reference to the synthetic answer\n",
    "        new_example['reference'] = synthetic_answer\n",
    "        \n",
    "        # If original data has structured fields, preserve them and update answers\n",
    "        if 'answers' in example:\n",
    "            if synthetic_answer != \"No answer\":\n",
    "                # Try to find answer in context if context exists\n",
    "                context = example.get('context', '')\n",
    "                answer_start = context.find(synthetic_answer) if context else 0\n",
    "                if answer_start == -1:\n",
    "                    answer_start = 0\n",
    "                \n",
    "                new_example['answers'] = {\n",
    "                    'text': [synthetic_answer],\n",
    "                    'answer_start': [answer_start]\n",
    "                }\n",
    "            else:\n",
    "                # No answer case - follow SQuAD v2 format\n",
    "                new_example['answers'] = {\n",
    "                    'text': [],\n",
    "                    'answer_start': []\n",
    "                }\n",
    "        \n",
    "        # Add generation metadata\n",
    "        new_example['generation_num'] = generation_num\n",
    "        new_example['synthetic'] = True\n",
    "        \n",
    "        synthetic_data.append(new_example)\n",
    "        \n",
    "        # Update progress bar with statistics\n",
    "        success_rate = (successful_generations / (idx + 1)) * 100\n",
    "        progress_bar.set_postfix({\n",
    "            'Success': f'{successful_generations}/{idx + 1}',\n",
    "            'Rate': f'{success_rate:.1f}%',\n",
    "            'Failed': failed_generations,\n",
    "            'No Answer': failed_generations\n",
    "        })\n",
    "        \n",
    "        # Update description every 100 examples\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            progress_bar.set_description(\n",
    "                f\"ü§ñ Gen {generation_num} - Generated {idx + 1}/{total_examples}\"\n",
    "            )\n",
    "    \n",
    "    # Close progress bar\n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f\"‚úÖ Generation {generation_num} completed:\")\n",
    "    print(f\"   üìä Total examples processed: {total_examples}\")\n",
    "    print(f\"   ‚úÖ Successful generations: {successful_generations}\")\n",
    "    print(f\"   ‚ùå No answer cases: {failed_generations}\")\n",
    "    print(f\"   üìà Success rate: {(successful_generations/total_examples)*100:.1f}%\")\n",
    "    print(f\"   üìà No answer rate: {(failed_generations/total_examples)*100:.1f}%\")\n",
    "    \n",
    "    # Create a new formatted dataset with the synthetic data\n",
    "    print(\"üì¶ Creating synthetic dataset...\")\n",
    "    with tqdm(total=1, desc=\"üì¶ Building Dataset\", position=1, leave=False) as dataset_pbar:\n",
    "        synthetic_dataset = Dataset.from_list(synthetic_data)\n",
    "        dataset_pbar.update(1)\n",
    "    \n",
    "    # Return in the same format as input\n",
    "    return {\n",
    "        'train': synthetic_dataset\n",
    "    }\n",
    "\n",
    "\n",
    "def save_synthetic_dataset(dataset_dir, synthetic_formatted_dataset, generation_num):\n",
    "    \"\"\"\n",
    "    Save the synthetic formatted dataset to a new subdirectory.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Base dataset directory\n",
    "        synthetic_formatted_dataset: Formatted dataset with synthetic answers\n",
    "        generation_num: Generation number\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create new subdirectory\n",
    "    new_dir = os.path.join(dataset_dir, f\"generation_{generation_num}\")\n",
    "    os.makedirs(new_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the train split\n",
    "    synthetic_formatted_dataset['train'].save_to_disk(os.path.join(new_dir, \"train\"))\n",
    "    \n",
    "    # Also save as JSON for inspection\n",
    "    json_file = os.path.join(new_dir, \"synthetic_data.json\")\n",
    "    synthetic_formatted_dataset['train'].to_json(json_file)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"generation_number\": generation_num,\n",
    "        \"total_examples\": len(synthetic_formatted_dataset['train']),\n",
    "        \"generated_from\": \"fine_tuned_model\",\n",
    "        \"description\": f\"Synthetic answers generated using fine-tuned model (Generation {generation_num})\",\n",
    "        \"format\": \"formatted_dataset_with_prompts\"\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(new_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved synthetic formatted dataset to {new_dir}\")\n",
    "    return new_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d47390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_nick(model_path):\n",
    "    # Extract the part after the first \"/\"\n",
    "    model_name = model_path.split(\"/\")[1]\n",
    "    \n",
    "    # Match common patterns like \"phi-3\" or \"Mistral-7B\"\n",
    "    match = re.match(r\"([A-Za-z0-9\\-]+?)(?=-\\d|-[a-zA-Z])\", model_name)\n",
    "    \n",
    "    return match.group(1) if match else model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241ecd9",
   "metadata": {},
   "source": [
    "## Iterative training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d50fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_training_and_generation(\n",
    "    base_dataset_dir=\"squad_v2_01percent\",\n",
    "    model_path=\"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    num_generations=3,\n",
    "    start_generation=1,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform iterative training and synthetic data generation.\n",
    "    \n",
    "    Args:\n",
    "        base_dataset_dir: Directory containing the original dataset\n",
    "        model_path: Base model path or fine-tuned model path\n",
    "        num_generations: Number of generations to create\n",
    "        start_generation: Starting generation number (1 for base model)\n",
    "        device: Device for inference\n",
    "    \"\"\"\n",
    "    \n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    # Determine if starting from base model or fine-tuned model\n",
    "    is_base_model = start_generation == 1\n",
    "    \n",
    "    # Load tokenizer once at the beginning\n",
    "    if is_base_model:\n",
    "        base_model_name = model_path\n",
    "        model_nick = extract_model_nick(base_model_name)\n",
    "    else:\n",
    "        # For fine-tuned models, extract base model name for tokenizer\n",
    "        # Assume model_path format: \"./phi3-squad2-gen{X}-final\"\n",
    "        base_model_name = \"microsoft/Phi-3-mini-128k-instruct\"  # Default fallback\n",
    "        model_nick = extract_model_nick(base_model_name)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"Starting iterative training and generation for {num_generations} generations...\")\n",
    "    \n",
    "    # Used to load dataset\n",
    "    dataset = None\n",
    "    formatted_dataset = None\n",
    "    \n",
    "    # Main progress bar for generations\n",
    "    generation_progress = tqdm(\n",
    "        range(start_generation, start_generation + num_generations), \n",
    "        desc=\"üîÑ Overall Progress\", \n",
    "        unit=\"generation\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    for generation in generation_progress:\n",
    "        torch.cuda.empty_cache()\n",
    "        generation_progress.set_description(f\"üîÑ Generation {generation}/{start_generation+num_generations-1}\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"GENERATION {generation}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Step 1: Fine-tune model with current training dataset\n",
    "        print(\"Step 1: Fine-tuning model...\")\n",
    "        \n",
    "        # Prepare training arguments for this generation\n",
    "        train_config = {\n",
    "            \"bf16\": True,\n",
    "            \"do_eval\": False,  # Disable evaluation completely\n",
    "            \"learning_rate\": 1.0e-05,\n",
    "            \"log_level\": \"info\",\n",
    "            \"logging_steps\": 10,\n",
    "            \"logging_strategy\": \"steps\",\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"num_train_epochs\": 3,\n",
    "            \"max_steps\": -1,\n",
    "            \"output_dir\": f\"./phi3-squad2-gen{generation}\",  # Update for each generation\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"per_device_train_batch_size\": 4,\n",
    "            \"remove_unused_columns\": True,\n",
    "            \"save_steps\": 50,\n",
    "            \"save_total_limit\": 2,\n",
    "            \"seed\": 42,\n",
    "            \"gradient_checkpointing\": True,\n",
    "            \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "            \"warmup_ratio\": 0.05,\n",
    "            \"save_strategy\": \"steps\",\n",
    "            \"load_best_model_at_end\": False,  # No evaluation, so no \"best\" model\n",
    "            \"disable_tqdm\": False,  # Enable tqdm progress bars for training\n",
    "        }\n",
    "\n",
    "        train_args = TrainingArguments(**train_config)\n",
    "\n",
    "        offload_cache_dir = \"./offload_cache\"\n",
    "        os.makedirs(offload_cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Load model for this generation\n",
    "        print(\"üì• Loading model...\")\n",
    "        with tqdm(total=1, desc=\"ü§ñ Model Loading\", position=1, leave=False) as model_pbar:\n",
    "            if generation == start_generation and is_base_model:\n",
    "                # First generation: load base model with quantization\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    base_model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    quantization_config=bnb_config\n",
    "                )\n",
    "                # Prepare dataset for first generation using original function\n",
    "                dataset = load_squad_subset(base_dataset_dir)\n",
    "                formatted_dataset = {\n",
    "                    split: dataset[split].map(make_prompt)\n",
    "                    for split in dataset.keys()\n",
    "                }\n",
    "            elif generation == start_generation and not is_base_model:\n",
    "                # Starting from fine-tuned model: load without quantization\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16,        # Use float16 to save memory\n",
    "                    low_cpu_mem_usage=True           # Enable low CPU memory usage\n",
    "                )\n",
    "                # Prepare dataset based on start generation\n",
    "                if start_generation == 1:\n",
    "                    dataset = load_squad_subset(base_dataset_dir)\n",
    "                    formatted_dataset = {\n",
    "                        split: dataset[split].map(make_prompt)\n",
    "                        for split in dataset.keys()\n",
    "                    }\n",
    "                else:\n",
    "                    # Use the dedicated function to load synthetic data\n",
    "                    synthetic_data_dir = os.path.join(base_dataset_dir, f\"generation_{start_generation - 1}\", \"train\")\n",
    "                    dataset = load_synthetic_train(base_dataset_dir, synthetic_data_dir)\n",
    "                    formatted_dataset = dataset  # Already formatted\n",
    "                            \n",
    "            else:\n",
    "                # Subsequent generations: load previous model - FIXED VERSION\n",
    "                previous_model_path = f\"./phi3-squad2-gen{generation-1}-final\"\n",
    "                \n",
    "                # Load the model without offloading to avoid dispatch issues\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    previous_model_path,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "                \n",
    "                # Manually move to device after loading\n",
    "                if torch.cuda.is_available():\n",
    "                    model = model.to(\"cuda\")\n",
    "                \n",
    "                # Use the dedicated function to load synthetic data\n",
    "                synthetic_data_dir = os.path.join(base_dataset_dir, f\"generation_{generation - 1}\", \"train\")\n",
    "                dataset = load_synthetic_train(base_dataset_dir, synthetic_data_dir)\n",
    "                formatted_dataset = dataset  # Already formatted\n",
    "\n",
    "            model_pbar.update(1)\n",
    "        \n",
    "        # Apply PEFT configuration\n",
    "        print(\"üîß Applying PEFT configuration...\")\n",
    "        with tqdm(total=1, desc=\"‚öôÔ∏è PEFT Setup\", position=1, leave=False) as peft_pbar:\n",
    "            peft_config = {\n",
    "                \"r\": 8,  # Reduced from 16 to 8 (fewer parameters)\n",
    "                \"lora_alpha\": 16,  # Reduced from 32 to 16\n",
    "                \"lora_dropout\": 0.1,  # Slightly increased dropout\n",
    "                \"bias\": \"none\",\n",
    "                \"task_type\": \"CAUSAL_LM\",\n",
    "                \"target_modules\": \"all-linear\",\n",
    "                \"modules_to_save\": None,\n",
    "            }\n",
    "            lora_config = LoraConfig(**peft_config)\n",
    "            model = get_peft_model(model, lora_config)\n",
    "            peft_pbar.update(1)\n",
    "        \n",
    "        # Define tokenization function based on data structure\n",
    "        def tokenize(example):\n",
    "            # Check if example has 'prompt' key (formatted data) or needs to be created\n",
    "            if 'prompt' in example:\n",
    "                text_to_tokenize = example[\"prompt\"]\n",
    "            else:\n",
    "                # Create prompt from raw data using make_prompt logic\n",
    "                context = example[\"context\"]\n",
    "                question = example[\"question\"]\n",
    "                \n",
    "                # Handle different answer formats\n",
    "                if \"answers\" in example:\n",
    "                    answers = example[\"answers\"]\n",
    "                    # Check if answers is a dict with 'text' key (original format)\n",
    "                    if isinstance(answers, dict) and \"text\" in answers:\n",
    "                        answer = answers[\"text\"][0] if answers[\"text\"] else \"No answer\"\n",
    "                    # Check if answers is a list (some synthetic data format)\n",
    "                    elif isinstance(answers, list) and len(answers) > 0:\n",
    "                        # If it's a list of strings\n",
    "                        if isinstance(answers[0], str):\n",
    "                            answer = answers[0]\n",
    "                        # If it's a list of dicts with 'text' key\n",
    "                        elif isinstance(answers[0], dict) and \"text\" in answers[0]:\n",
    "                            answer = answers[0][\"text\"]\n",
    "                        else:\n",
    "                            answer = \"No answer\"\n",
    "                    else:\n",
    "                        answer = \"No answer\"\n",
    "                else:\n",
    "                    # Fallback: check if there's a 'reference' field\n",
    "                    answer = example.get(\"reference\", \"No answer\")\n",
    "                \n",
    "                text_to_tokenize = f\"[INST] Given the context, answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question} [/INST] Answer: {answer}\"\n",
    "            \n",
    "            return tokenizer(\n",
    "                text_to_tokenize,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512\n",
    "            )\n",
    "        \n",
    "        # Tokenize current training dataset\n",
    "        print(\"üî§ Tokenizing dataset...\")\n",
    "        print(f\"üîç Checking dataset structure...\")\n",
    "        \n",
    "        # Check the structure of the first example to debug\n",
    "        sample_example = formatted_dataset['train'][0]\n",
    "        print(f\"üìã Available columns in training data: {list(sample_example.keys())}\")\n",
    "        \n",
    "        # Debug: Check the structure of answers field\n",
    "        if 'answers' in sample_example:\n",
    "            print(f\"üîç Answers field type: {type(sample_example['answers'])}\")\n",
    "            print(f\"üîç Answers field content: {sample_example['answers']}\")\n",
    "        \n",
    "        if generation == 1 or (generation == start_generation and is_base_model):\n",
    "            # For first generation or starting from base model, tokenize normally\n",
    "            tokenized = {\n",
    "                split: formatted_dataset[split].map(tokenize, batched=False)\n",
    "                for split in formatted_dataset.keys()\n",
    "            }\n",
    "        else:\n",
    "            # For subsequent generations, data is already formatted with prompts\n",
    "            tokenized = {\n",
    "                split: formatted_dataset[split].map(tokenize, batched=False)\n",
    "                for split in formatted_dataset.keys()\n",
    "            }\n",
    "        \n",
    "        # Fine-tune the model\n",
    "        print(\"üöÄ Starting training...\")\n",
    "        trainer = train_model(model, tokenized, tokenizer, train_args)\n",
    "        \n",
    "        # Save the fine-tuned model for this generation\n",
    "        print(\"üíæ Saving model...\")\n",
    "        with tqdm(total=1, desc=\"üíæ Saving Model\", position=1, leave=False) as save_pbar:\n",
    "            final_model_path = f\"./phi3-squad2-gen{generation}-final\"\n",
    "            trainer.save_model(final_model_path)\n",
    "            save_pbar.update(1)\n",
    "        print(f\"‚úÖ Generation {generation} model saved to {final_model_path}\")\n",
    "        \n",
    "        # Step 2: Generate synthetic answers using the fine-tuned model\n",
    "        print(\"Step 2: Generating synthetic answers...\")\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # For synthetic generation, ensure we have proper formatted dataset structure\n",
    "        if generation == 1 or (generation == start_generation and is_base_model):\n",
    "            generation_dataset = formatted_dataset\n",
    "        else:\n",
    "            # Ensure synthetic data has the right format for generation\n",
    "            # If it doesn't have 'prompt' column, create it\n",
    "            if 'prompt' not in formatted_dataset['train'].column_names:\n",
    "                print(\"üîß Reformatting synthetic data for generation...\")\n",
    "                formatted_dataset['train'] = formatted_dataset['train'].map(make_prompt)\n",
    "            generation_dataset = formatted_dataset\n",
    "        \n",
    "        synthetic_dataset = generate_synthetic_answers(\n",
    "            model, tokenizer, generation_dataset, device, generation\n",
    "        )\n",
    "        \n",
    "        # Step 3: Save synthetic dataset\n",
    "        print(\"Step 3: Saving synthetic dataset...\")\n",
    "        with tqdm(total=1, desc=\"üíæ Saving Dataset\", position=1, leave=False) as dataset_save_pbar:\n",
    "            new_dir = save_synthetic_dataset(base_dataset_dir, synthetic_dataset, generation)\n",
    "            dataset_save_pbar.update(1)\n",
    "        \n",
    "        print(f\"Generation {generation} completed!\")\n",
    "        print(f\"Synthetic dataset saved to: {new_dir}\")\n",
    "        print(f\"Model saved to: {final_model_path}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    generation_progress.close()\n",
    "    print(f\"\\nüéâ All {num_generations} generations completed!\")\n",
    "    print(\"Final models and datasets are ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3216d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iterative training and generation for 5 generations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generation 1/5:   0%|          | 0/5 [00:00<?, ?generation/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GENERATION 1\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n",
      "üì• Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab7ba2c776243ceb1ff69b991530141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from squad_v2_05percent...\n",
      "Dataset metadata:\n",
      "  original_train_size: 130319\n",
      "  original_validation_size: 11873\n",
      "  extracted_train_size: 651\n",
      "  extracted_test_size: 59\n",
      "  extraction_percentage: 0.5\n",
      "\n",
      "Loaded dataset splits:\n",
      "  train: 651 examples\n",
      "  test: 59 examples\n",
      "üîß Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenizing dataset...\n",
      "üîç Checking dataset structure...\n",
      "üìã Available columns in training data: ['id', 'title', 'context', 'question', 'answers', 'prompt', 'reference']\n",
      "üîç Answers field type: <class 'dict'>\n",
      "üîç Answers field content: {'text': ['in the late 1990s'], 'answer_start': [269]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0976733e5b41f0a89cb751b4719ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb9dac8dd364a198114d7fe56c7d35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "‚úÖ Set use_cache=False for gradient checkpointing compatibility\n",
      "‚úÖ Enabled gradient checkpointing for memory efficiency\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89d47397afd4d1893b48451b0b706b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "***** Running training *****\n",
      "  Num examples = 651\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 246\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [246/246 1:02:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.766600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.770800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.742500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.902500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.731700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.809100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.973200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.970400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.900400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.884600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen1\\checkpoint-50\n",
      "Saving model checkpoint to ./phi3-squad2-gen1\\checkpoint-100\n",
      "Saving model checkpoint to ./phi3-squad2-gen1\\checkpoint-150\n",
      "Deleting older checkpoint [phi3-squad2-gen1\\checkpoint-50] due to args.save_total_limit\n",
      "Saving model checkpoint to ./phi3-squad2-gen1\\checkpoint-200\n",
      "Deleting older checkpoint [phi3-squad2-gen1\\checkpoint-100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./phi3-squad2-gen1\\checkpoint-246\n",
      "Deleting older checkpoint [phi3-squad2-gen1\\checkpoint-150] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Training loss improved to 3.1325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Re-enabled use_cache for inference\n",
      "‚úÖ Model saved to ./phi3-squad2-final\n",
      "\n",
      "üßπ Cleaning up checkpoints...\n",
      "üóëÔ∏è Removing checkpoint: ./phi3-squad2-gen1\\checkpoint-200\n",
      "üóëÔ∏è Removing checkpoint: ./phi3-squad2-gen1\\checkpoint-246\n",
      "‚úÖ Checkpoint cleanup completed!\n",
      "‚úÖ Checkpoint cleanup completed!\n",
      "üéâ Training completed and checkpoints cleaned up!\n",
      "üíæ Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen1-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation 1 model saved to ./phi3-squad2-gen1-final\n",
      "Step 2: Generating synthetic answers...\n",
      "Generating synthetic answers (Generation 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation 1 completed:\n",
      "   üìä Total examples processed: 651\n",
      "   ‚úÖ Successful generations: 109\n",
      "   ‚ùå No answer cases: 542\n",
      "   üìà Success rate: 16.7%\n",
      "   üìà No answer rate: 83.3%\n",
      "üì¶ Creating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Saving synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40083232963d4ab19fb3ac453cd9b66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e473a912d335422f99039389589f4800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generation 2/5:  20%|‚ñà‚ñà        | 1/5 [1:58:57<7:55:49, 7137.48s/generation]PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic formatted dataset to squad_v2_05percent\\generation_1\n",
      "Generation 1 completed!\n",
      "Synthetic dataset saved to: squad_v2_05percent\\generation_1\n",
      "Model saved to: ./phi3-squad2-gen1-final\n",
      "\n",
      "==================================================\n",
      "GENERATION 2\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b86f4212bb94d9a99aca626e7d412b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.norm.weight, model.layers.*.input_layernorm.weight, lm_head.weight, model.embed_tokens.weight, model.layers.*.post_attention_layernorm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from squad_v2_05percent\\generation_1\\train...\n",
      "üîß Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenizing dataset...\n",
      "üîç Checking dataset structure...\n",
      "üìã Available columns in training data: ['id', 'title', 'context', 'question', 'answers']\n",
      "üîç Answers field type: <class 'dict'>\n",
      "üîç Answers field content: {'text': ['in the late 1990s'], 'answer_start': [269]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdd7d60338e434381ecf02fad8c6225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244d911ba0604b0a8ef064187c08baa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "‚úÖ Set use_cache=False for gradient checkpointing compatibility\n",
      "‚úÖ Enabled gradient checkpointing for memory efficiency\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a929d96d4284befb1a9c99dab7b6e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "***** Running training *****\n",
      "  Num examples = 651\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 246\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [246/246 46:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.299700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.701900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.896500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.929400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.887300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.717700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.626600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.777900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.801800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.718300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.714300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen2\\checkpoint-50\n",
      "Saving model checkpoint to ./phi3-squad2-gen2\\checkpoint-100\n",
      "Saving model checkpoint to ./phi3-squad2-gen2\\checkpoint-150\n",
      "Deleting older checkpoint [phi3-squad2-gen2\\checkpoint-50] due to args.save_total_limit\n",
      "Saving model checkpoint to ./phi3-squad2-gen2\\checkpoint-200\n",
      "Deleting older checkpoint [phi3-squad2-gen2\\checkpoint-100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./phi3-squad2-gen2\\checkpoint-246\n",
      "Deleting older checkpoint [phi3-squad2-gen2\\checkpoint-150] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Training loss improved to 2.8864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Re-enabled use_cache for inference\n",
      "‚úÖ Model saved to ./phi3-squad2-final\n",
      "\n",
      "üßπ Cleaning up checkpoints...\n",
      "üóëÔ∏è Removing checkpoint: ./phi3-squad2-gen2\\checkpoint-200\n",
      "üóëÔ∏è Removing checkpoint: ./phi3-squad2-gen2\\checkpoint-246\n",
      "‚úÖ Checkpoint cleanup completed!\n",
      "‚úÖ Checkpoint cleanup completed!\n",
      "üéâ Training completed and checkpoints cleaned up!\n",
      "üíæ Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation 2 model saved to ./phi3-squad2-gen2-final\n",
      "Step 2: Generating synthetic answers...\n",
      "üîß Reformatting synthetic data for generation...\n",
      "Generating synthetic answers (Generation 2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation 2 completed:\n",
      "   üìä Total examples processed: 651\n",
      "   ‚úÖ Successful generations: 89\n",
      "   ‚ùå No answer cases: 562\n",
      "   üìà Success rate: 13.7%\n",
      "   üìà No answer rate: 86.3%\n",
      "üì¶ Creating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Saving synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c5caab1592465d9c14488ca4374515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b4b738feeb4ec7a9a15d41f69e3dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generation 3/5:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [3:29:58<5:07:33, 6151.10s/generation]PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic formatted dataset to squad_v2_05percent\\generation_2\n",
      "Generation 2 completed!\n",
      "Synthetic dataset saved to: squad_v2_05percent\\generation_2\n",
      "Model saved to: ./phi3-squad2-gen2-final\n",
      "\n",
      "==================================================\n",
      "GENERATION 3\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n",
      "üì• Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d85dc251dd49af8d592973e951d5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.norm.weight, model.layers.*.input_layernorm.weight, lm_head.weight, model.embed_tokens.weight, model.layers.*.post_attention_layernorm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from squad_v2_05percent\\generation_2\\train...\n",
      "üîß Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenizing dataset...\n",
      "üîç Checking dataset structure...\n",
      "üìã Available columns in training data: ['id', 'title', 'context', 'question', 'answers']\n",
      "üîç Answers field type: <class 'dict'>\n",
      "üîç Answers field content: {'text': ['in the late 1990s'], 'answer_start': [269]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32b6c75a6a642cca762f84dd93c2951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "‚úÖ Set use_cache=False for gradient checkpointing compatibility\n",
      "‚úÖ Enabled gradient checkpointing for memory efficiency\n",
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 651\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 246\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [246/246 51:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.295400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.299400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.948500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.897400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.719500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.664000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.797200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.546300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.779100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.803500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.716300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-50\n",
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-100\n",
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-150\n",
      "Deleting older checkpoint [phi3-squad2-gen3\\checkpoint-50] due to args.save_total_limit\n",
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-200\n",
      "Deleting older checkpoint [phi3-squad2-gen3\\checkpoint-100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./phi3-squad2-gen3\\checkpoint-246\n",
      "Deleting older checkpoint [phi3-squad2-gen3\\checkpoint-150] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Training loss improved to 2.8876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Re-enabled use_cache for inference\n",
      "‚úÖ Model saved to ./phi3-squad2-final\n",
      "\n",
      "üßπ Cleaning up checkpoints...\n",
      "üóëÔ∏è Removing checkpoint: ./phi3-squad2-gen3\\checkpoint-200\n",
      "üóëÔ∏è Removing checkpoint: ./phi3-squad2-gen3\\checkpoint-246\n",
      "‚úÖ Checkpoint cleanup completed!\n",
      "‚úÖ Checkpoint cleanup completed!\n",
      "üéâ Training completed and checkpoints cleaned up!\n",
      "üíæ Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen3-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation 3 model saved to ./phi3-squad2-gen3-final\n",
      "Step 2: Generating synthetic answers...\n",
      "üîß Reformatting synthetic data for generation...\n",
      "Generating synthetic answers (Generation 3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation 3 completed:\n",
      "   üìä Total examples processed: 651\n",
      "   ‚úÖ Successful generations: 94\n",
      "   ‚ùå No answer cases: 557\n",
      "   üìà Success rate: 14.4%\n",
      "   üìà No answer rate: 85.6%\n",
      "üì¶ Creating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Saving synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241aae64356f479f97d7cdf11e7a5567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7907d1ca635049788431add7bf1a2f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generation 4/5:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [5:10:30<3:23:14, 6097.04s/generation]PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic formatted dataset to squad_v2_05percent\\generation_3\n",
      "Generation 3 completed!\n",
      "Synthetic dataset saved to: squad_v2_05percent\\generation_3\n",
      "Model saved to: ./phi3-squad2-gen3-final\n",
      "\n",
      "==================================================\n",
      "GENERATION 4\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n",
      "üì• Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7149a2339c734bd8a702f5d5db37a847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.norm.weight, model.layers.*.input_layernorm.weight, lm_head.weight, model.embed_tokens.weight, model.layers.*.post_attention_layernorm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from squad_v2_05percent\\generation_3\\train...\n",
      "üîß Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenizing dataset...\n",
      "üîç Checking dataset structure...\n",
      "üìã Available columns in training data: ['id', 'title', 'context', 'question', 'answers']\n",
      "üîç Answers field type: <class 'dict'>\n",
      "üîç Answers field content: {'text': ['in the late 1990s'], 'answer_start': [269]}\n",
      "üöÄ Starting training...\n",
      "‚úÖ Set use_cache=False for gradient checkpointing compatibility\n",
      "‚úÖ Enabled gradient checkpointing for memory efficiency\n",
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 651\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 246\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [246/246 46:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.335400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.860300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.704400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.891200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.878700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.697600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.631500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.782400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.720300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-50\n",
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-100\n",
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-150\n",
      "Deleting older checkpoint [phi3-squad2-gen4\\checkpoint-50] due to args.save_total_limit\n",
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-200\n",
      "Deleting older checkpoint [phi3-squad2-gen4\\checkpoint-100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./phi3-squad2-gen4\\checkpoint-246\n",
      "Deleting older checkpoint [phi3-squad2-gen4\\checkpoint-150] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Training loss improved to 2.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Re-enabled use_cache for inference\n",
      "‚úÖ Model saved to ./phi3-squad2-final\n",
      "\n",
      "üßπ Cleaning up checkpoints...\n",
      "üóëÔ∏è Removing checkpoint: ./phi3-squad2-gen4\\checkpoint-200\n",
      "üóëÔ∏è Removing checkpoint: ./phi3-squad2-gen4\\checkpoint-246\n",
      "‚úÖ Checkpoint cleanup completed!\n",
      "‚úÖ Checkpoint cleanup completed!\n",
      "üéâ Training completed and checkpoints cleaned up!\n",
      "üíæ Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen4-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation 4 model saved to ./phi3-squad2-gen4-final\n",
      "Step 2: Generating synthetic answers...\n",
      "üîß Reformatting synthetic data for generation...\n",
      "Generating synthetic answers (Generation 4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation 4 completed:\n",
      "   üìä Total examples processed: 651\n",
      "   ‚úÖ Successful generations: 89\n",
      "   ‚ùå No answer cases: 562\n",
      "   üìà Success rate: 13.7%\n",
      "   üìà No answer rate: 86.3%\n",
      "üì¶ Creating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Saving synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32191b2d90de43b4901d7b5355374707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d194a515ea3457c8a879441b2cbc967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generation 5/5:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [6:41:25<1:37:23, 5843.48s/generation]PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic formatted dataset to squad_v2_05percent\\generation_4\n",
      "Generation 4 completed!\n",
      "Synthetic dataset saved to: squad_v2_05percent\\generation_4\n",
      "Model saved to: ./phi3-squad2-gen4-final\n",
      "\n",
      "==================================================\n",
      "GENERATION 5\n",
      "==================================================\n",
      "Step 1: Fine-tuning model...\n",
      "üì• Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f4ef9f936d4daf9ac032a35b65ae1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.norm.weight, model.layers.*.input_layernorm.weight, lm_head.weight, model.embed_tokens.weight, model.layers.*.post_attention_layernorm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from squad_v2_05percent\\generation_4\\train...\n",
      "üîß Applying PEFT configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenizing dataset...\n",
      "üîç Checking dataset structure...\n",
      "üìã Available columns in training data: ['id', 'title', 'context', 'question', 'answers']\n",
      "üîç Answers field type: <class 'dict'>\n",
      "üîç Answers field content: {'text': ['in the late 1990s'], 'answer_start': [269]}\n",
      "üöÄ Starting training...\n",
      "‚úÖ Set use_cache=False for gradient checkpointing compatibility\n",
      "‚úÖ Enabled gradient checkpointing for memory efficiency\n",
      "Starting training with BERTScore optimization...\n",
      "Early stopping based on training loss improvement\n",
      "Cache disabled for gradient checkpointing compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 651\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 246\n",
      "  Number of trainable parameters = 12,582,912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [246/246 52:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.947600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.896900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.888100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.797100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.544100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.626300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.715900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen5\\checkpoint-50\n",
      "Saving model checkpoint to ./phi3-squad2-gen5\\checkpoint-100\n",
      "Saving model checkpoint to ./phi3-squad2-gen5\\checkpoint-150\n",
      "Deleting older checkpoint [phi3-squad2-gen5\\checkpoint-50] due to args.save_total_limit\n",
      "Saving model checkpoint to ./phi3-squad2-gen5\\checkpoint-200\n",
      "Deleting older checkpoint [phi3-squad2-gen5\\checkpoint-100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./phi3-squad2-gen5\\checkpoint-246\n",
      "Deleting older checkpoint [phi3-squad2-gen5\\checkpoint-150] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Training loss improved to 2.8869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Re-enabled use_cache for inference\n",
      "‚úÖ Model saved to ./phi3-squad2-final\n",
      "\n",
      "üßπ Cleaning up checkpoints...\n",
      "üóëÔ∏è Removing checkpoint: ./phi3-squad2-gen5\\checkpoint-200\n",
      "üóëÔ∏è Removing checkpoint: ./phi3-squad2-gen5\\checkpoint-246\n",
      "‚úÖ Checkpoint cleanup completed!\n",
      "‚úÖ Checkpoint cleanup completed!\n",
      "üéâ Training completed and checkpoints cleaned up!\n",
      "üíæ Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./phi3-squad2-gen5-final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation 5 model saved to ./phi3-squad2-gen5-final\n",
      "Step 2: Generating synthetic answers...\n",
      "üîß Reformatting synthetic data for generation...\n",
      "Generating synthetic answers (Generation 5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation 5 completed:\n",
      "   üìä Total examples processed: 651\n",
      "   ‚úÖ Successful generations: 76\n",
      "   ‚ùå No answer cases: 575\n",
      "   üìà Success rate: 11.7%\n",
      "   üìà No answer rate: 88.3%\n",
      "üì¶ Creating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Saving synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a04608e5674fe1bccb09da1b7865b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/651 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61eba2d3f665443f88853600229ac7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Generation 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [8:24:10<00:00, 6050.08s/generation]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic formatted dataset to squad_v2_05percent\\generation_5\n",
      "Generation 5 completed!\n",
      "Synthetic dataset saved to: squad_v2_05percent\\generation_5\n",
      "Model saved to: ./phi3-squad2-gen5-final\n",
      "\n",
      "üéâ All 5 generations completed!\n",
      "Final models and datasets are ready for use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iterative_training_and_generation(\n",
    "        base_dataset_dir=\"squad_v2_05percent\",\n",
    "        model_path=\"microsoft/phi-3-mini-128k-instruct\",\n",
    "        start_generation=1,\n",
    "        num_generations=5,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a997995",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a9259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", num_examples=None):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation on test set with minimal console output.\n",
    "    All detailed analysis is written to files instead of console.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare test data using make_prompt function\n",
    "    test_prompts = dataset.map(make_prompt)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize lists for predictions and references\n",
    "    preds = []\n",
    "    refs = []\n",
    "    raw_outputs = []\n",
    "    prompts_list = []\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    \n",
    "    # Limit examples if specified\n",
    "    if num_examples:\n",
    "        test_prompts = test_prompts.select(range(min(num_examples, len(test_prompts))))\n",
    "    \n",
    "    # Generate predictions with progress bar\n",
    "    for example in tqdm(test_prompts, desc=\"üîç Evaluating\", leave=False):\n",
    "        # Get prompt without answer (remove answer part from make_prompt output)\n",
    "        full_prompt = example[\"prompt\"]\n",
    "        if '[/INST]' in full_prompt:\n",
    "            prompt_without_answer = full_prompt.split('[/INST]')[0] + '[/INST] Answer:'\n",
    "        else:\n",
    "            prompt_without_answer = full_prompt\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            prompt_without_answer,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=50,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer - look for content after [/INST] or after \"Answer:\" if present\n",
    "        if '[/INST]' in decoded:\n",
    "            answer_part = decoded.split('[/INST]')[-1].strip()\n",
    "            \n",
    "            # If there's an \"Answer:\" pattern in the generated text, extract after it\n",
    "            if \"Answer:\" in answer_part:\n",
    "                answer = answer_part.split(\"Answer:\")[-1].strip()\n",
    "            else:\n",
    "                answer = answer_part\n",
    "        else:\n",
    "            # Fallback: look for \"Answer:\" pattern in the entire decoded text\n",
    "            if \"Answer:\" in decoded:\n",
    "                answer = decoded.split(\"Answer:\")[-1].strip()\n",
    "            else:\n",
    "                answer = decoded.strip()\n",
    "        \n",
    "        # Clean up answer - remove any trailing instruction markers or unwanted text\n",
    "        answer = answer.split('\\n')[0].strip()  # Take only first line to avoid multi-line responses\n",
    "        \n",
    "        # Store results\n",
    "        preds.append(answer)\n",
    "        refs.append(example[\"reference\"])\n",
    "        raw_outputs.append(decoded)\n",
    "        prompts_list.append(example[\"prompt\"])\n",
    "        \n",
    "        # Extract question and context for detailed analysis\n",
    "        if \"Question:\" in example[\"prompt\"]:\n",
    "            question_part = example[\"prompt\"].split(\"Question:\")[-1].split(\"[/INST]\")[0].strip()\n",
    "            questions.append(question_part)\n",
    "        else:\n",
    "            questions.append(\"\")\n",
    "            \n",
    "        if \"Context:\" in example[\"prompt\"]:\n",
    "            context_part = example[\"prompt\"].split(\"Context:\")[-1].split(\"Question:\")[0].strip()\n",
    "            contexts.append(context_part[:200] + \"...\" if len(context_part) > 200 else context_part)\n",
    "        else:\n",
    "            contexts.append(\"\")\n",
    "    \n",
    "    # Compute BERTScore using silent function\n",
    "    try:\n",
    "        P, R, F1 = silent_bert_score(preds, refs, lang=\"en\")\n",
    "        bert_scores = {\n",
    "            \"precision\": P.mean().item(),\n",
    "            \"recall\": R.mean().item(),\n",
    "            \"f1\": F1.mean().item()\n",
    "        }\n",
    "        bert_f1_scores = [f.item() if hasattr(f, 'item') else f for f in F1]\n",
    "    except Exception as e:\n",
    "        bert_scores = {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "        bert_f1_scores = [0.0] * len(preds)\n",
    "    \n",
    "    # Compute exact match accuracy - improved logic for \"No answer\" cases\n",
    "    exact_matches = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_clean = pred.lower().strip()\n",
    "        ref_clean = ref.lower().strip()\n",
    "        \n",
    "        # Handle \"No answer\" cases\n",
    "        if ref_clean == \"no answer\":\n",
    "            # For \"no answer\" references, check if prediction indicates no answer\n",
    "            no_answer_indicators = [\"no answer\", \"cannot answer\", \"not provided\", \"no information\", \"unknown\"]\n",
    "            match = any(indicator in pred_clean for indicator in no_answer_indicators)\n",
    "            exact_matches.append(1 if match else 0)\n",
    "        else:\n",
    "            # For regular answers, check if reference is contained in prediction\n",
    "            match = ref_clean in pred_clean or pred_clean in ref_clean\n",
    "            exact_matches.append(1 if match else 0)\n",
    "    \n",
    "    exact_match_score = np.mean(exact_matches)\n",
    "    \n",
    "    # Compute F1 score (token overlap)\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = set(pred.lower().split())\n",
    "        ref_tokens = set(ref.lower().split())\n",
    "        \n",
    "        if len(pred_tokens) == 0 and len(ref_tokens) == 0:\n",
    "            f1_scores.append(1.0)\n",
    "        elif len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "        else:\n",
    "            common = len(pred_tokens & ref_tokens)\n",
    "            precision = common / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "            recall = common / len(ref_tokens) if len(ref_tokens) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            f1_scores.append(f1)\n",
    "    \n",
    "    f1_score = np.mean(f1_scores)\n",
    "    \n",
    "    # Compute semantic similarity (simple word overlap)\n",
    "    semantic_similarities = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_words = set(pred.lower().split())\n",
    "        ref_words = set(ref.lower().split())\n",
    "        \n",
    "        if len(pred_words) == 0 and len(ref_words) == 0:\n",
    "            semantic_similarities.append(1.0)\n",
    "        elif len(pred_words | ref_words) == 0:\n",
    "            semantic_similarities.append(0.0)\n",
    "        else:\n",
    "            jaccard = len(pred_words & ref_words) / len(pred_words | ref_words)\n",
    "            semantic_similarities.append(jaccard)\n",
    "    \n",
    "    semantic_similarity = np.mean(semantic_similarities)\n",
    "    \n",
    "    # Compute answer length statistics\n",
    "    pred_lengths = [len(pred.split()) for pred in preds]\n",
    "    ref_lengths = [len(ref.split()) for ref in refs]\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        \"test_size\": len(preds),\n",
    "        \"exact_match\": exact_match_score,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"bert_score_f1\": bert_scores[\"f1\"],\n",
    "        \"semantic_similarity\": semantic_similarity,\n",
    "        \"avg_prediction_length\": np.mean(pred_lengths),\n",
    "        \"avg_reference_length\": np.mean(ref_lengths),\n",
    "        \"predictions\": preds,\n",
    "        \"references\": refs,\n",
    "        \"questions\": questions,\n",
    "        \"contexts\": contexts,\n",
    "        \"individual_scores\": {\n",
    "            \"bert_f1\": bert_f1_scores,\n",
    "            \"token_f1\": f1_scores,\n",
    "            \"exact_match\": exact_matches,\n",
    "            \"semantic_similarity\": semantic_similarities\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Write detailed analysis to file instead of console\n",
    "    with open(\"detailed_evaluation_analysis.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"DETAILED TEST SET EVALUATION ANALYSIS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        # Summary metrics\n",
    "        f.write(\"EVALUATION RESULTS SUMMARY\\n\")\n",
    "        f.write(\"-\"*60 + \"\\n\")\n",
    "        f.write(f\"Test Set Size: {len(preds)}\\n\")\n",
    "        f.write(f\"Exact Match: {exact_match_score:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1_score:.4f}\\n\")\n",
    "        f.write(f\"BERTScore F1: {bert_scores['f1']:.4f}\\n\")\n",
    "        f.write(f\"Semantic Similarity: {semantic_similarity:.4f}\\n\")\n",
    "        f.write(f\"Avg Prediction Length: {np.mean(pred_lengths):.2f} words\\n\")\n",
    "        f.write(f\"Avg Reference Length: {np.mean(ref_lengths):.2f} words\\n\\n\")\n",
    "        \n",
    "        # Example analysis\n",
    "        f.write(\"DETAILED PREDICTION EXAMPLES\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        # Select diverse examples: best, worst, and random\n",
    "        if bert_f1_scores and len(bert_f1_scores) > 0:\n",
    "            sorted_indices = sorted(range(len(bert_f1_scores)), key=lambda i: bert_f1_scores[i], reverse=True)\n",
    "            \n",
    "            best_indices = sorted_indices[:3]  # Top 3\n",
    "            worst_indices = sorted_indices[-3:]  # Bottom 3\n",
    "            random_indices = random.sample(range(len(preds)), min(4, len(preds)))  # Random 4\n",
    "            \n",
    "            example_categories = [\n",
    "                (\"BEST PREDICTIONS\", best_indices),\n",
    "                (\"WORST PREDICTIONS\", worst_indices),\n",
    "                (\"RANDOM PREDICTIONS\", random_indices)\n",
    "            ]\n",
    "            \n",
    "            for category_name, indices in example_categories:\n",
    "                f.write(f\"\\n{category_name}:\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "                \n",
    "                for i, idx in enumerate(indices):\n",
    "                    if idx >= len(preds):\n",
    "                        continue\n",
    "                        \n",
    "                    f.write(f\"\\nExample {i+1} (Index {idx}):\\n\")\n",
    "                    f.write(f\"BERTScore F1: {bert_f1_scores[idx]:.4f}\\n\")\n",
    "                    f.write(f\"Token F1: {f1_scores[idx]:.4f}\\n\")\n",
    "                    f.write(f\"Exact Match: {'‚úì' if exact_matches[idx] else '‚úó'}\\n\")\n",
    "                    \n",
    "                    if idx < len(questions) and questions[idx]:\n",
    "                        f.write(f\"Question: {questions[idx]}\\n\")\n",
    "                    if idx < len(contexts) and contexts[idx]:\n",
    "                        f.write(f\"Context: {contexts[idx]}\\n\")\n",
    "                    \n",
    "                    f.write(f\"Reference Answer: {refs[idx]}\\n\")\n",
    "                    f.write(f\"Model Prediction: {preds[idx]}\\n\")\n",
    "                    \n",
    "                    # Analysis\n",
    "                    pred_words = len(preds[idx].split())\n",
    "                    ref_words = len(refs[idx].split())\n",
    "                    f.write(f\"Length: Pred={pred_words} words, Ref={ref_words} words\\n\")\n",
    "                    \n",
    "                    # Word overlap analysis\n",
    "                    pred_lower = preds[idx].lower()\n",
    "                    ref_lower = refs[idx].lower()\n",
    "                    common_words = set(pred_lower.split()) & set(ref_lower.split())\n",
    "                    f.write(f\"Common words: {len(common_words)}\\n\")\n",
    "                    \n",
    "                    f.write(\"-\" * 50 + \"\\n\")\n",
    "        \n",
    "        # Full predictions list\n",
    "        f.write(f\"\\n\\nFULL PREDICTIONS LIST\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, (prompt, pred, ref, bert_f1, token_f1, em) in enumerate(zip(\n",
    "            prompts_list, preds, refs, bert_f1_scores, f1_scores, exact_matches\n",
    "        )):\n",
    "            f.write(f\"Example {i+1}:\\n\")\n",
    "            f.write(f\"BERTScore F1: {bert_f1:.4f}\\n\")\n",
    "            f.write(f\"Token F1: {token_f1:.4f}\\n\")\n",
    "            f.write(f\"Exact Match: {'‚úì' if em else '‚úó'}\\n\")\n",
    "            \n",
    "            if \"Question:\" in prompt:\n",
    "                question = prompt.split(\"Question:\")[-1].split(\"[/INST]\")[0].strip()\n",
    "                f.write(f\"Question: {question}\\n\")\n",
    "            \n",
    "            f.write(f\"Reference: {ref}\\n\")\n",
    "            f.write(f\"Prediction: {pred}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d23f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation(base_model_name, generation_num, base_dataset_dir, device, \n",
    "                       save_results=True, results_dir=\"evaluation_results_squad01\", show_examples=True, num_examples=5):\n",
    "    \"\"\"\n",
    "    Evaluate a specific generation model on the test set with optional result saving.\n",
    "    Loads the model and tokenizer internally based on generation number.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Base model name (e.g., \"microsoft/Phi-3-mini-128k-instruct\")\n",
    "        generation_num: Generation number to evaluate\n",
    "        base_dataset_dir: Directory containing the original test dataset\n",
    "        device: Device for inference\n",
    "        save_results: Whether to save results to JSON file\n",
    "        results_dir: Directory to save evaluation results\n",
    "        show_examples: Whether to display example predictions\n",
    "        num_examples: Number of examples to show\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    # Determine if starting from base model or fine-tuned model\n",
    "    is_base_model = generation_num == 0\n",
    "    \n",
    "    # Extract model nickname for path construction\n",
    "    model_nick = extract_model_nick(base_model_name)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"üì• Loading tokenizer...\")\n",
    "    with tqdm(total=1, desc=\"üî§ Tokenizer Loading\", position=1, leave=False) as tokenizer_pbar:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer_pbar.update(1)\n",
    "    \n",
    "    # Load model based on generation number\n",
    "    print(\"üì• Loading model...\")\n",
    "    with tqdm(total=1, desc=\"ü§ñ Model Loading\", position=1, leave=False) as model_pbar:\n",
    "        if generation_num == 0:\n",
    "            # Load base model\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            # Load fine-tuned model from specific generation\n",
    "            model_path = f\"./phi3-squad2-gen{generation_num}-final\"\n",
    "            # Create offload directory\n",
    "            offload_cache_dir = \"./offload_cache\"\n",
    "            os.makedirs(offload_cache_dir, exist_ok=True)\n",
    "            \n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model for generation {generation_num} not found at {model_path}\")\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                offload_folder=offload_cache_dir,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        model_pbar.update(1)\n",
    "    \n",
    "    # Load original test dataset\n",
    "    with tqdm(total=1, desc=\"üìÇ Loading Test Data\", position=1, leave=False) as load_pbar:\n",
    "        original_dataset = load_from_disk(base_dataset_dir)\n",
    "        test_dataset = original_dataset['test']\n",
    "        load_pbar.update(1)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Run evaluation\n",
    "    with tqdm(total=1, desc=\"üìä Evaluating\", position=1, leave=False) as eval_pbar:\n",
    "        evaluation_results = evaluate_model(model, tokenizer, test_dataset, device)\n",
    "        eval_pbar.update(1)\n",
    "    \n",
    "    # Add metadata to results\n",
    "    evaluation_results.update({\n",
    "        'generation': generation_num,\n",
    "        'test_dataset_size': len(test_dataset),\n",
    "        'base_dataset_dir': base_dataset_dir,\n",
    "        'base_model_name': base_model_name,\n",
    "        'model_nick': model_nick,\n",
    "        'model_path': f\"./phi3-squad2-gen{generation_num}-final\" if generation_num > 0 else base_model_name,\n",
    "    })\n",
    "    \n",
    "    # Create results directory and detailed report file\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    detailed_report_file = os.path.join(results_dir, f\"generation_{generation_num}_detailed_report.txt\")\n",
    "    \n",
    "    # Write all detailed output to file instead of printing\n",
    "    with open(detailed_report_file, 'w', encoding='utf-8') as report_file:\n",
    "        # Redirect detailed output to file\n",
    "        report_file.write(f\"DETAILED EVALUATION REPORT - GENERATION {generation_num}\\n\")\n",
    "        report_file.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        # Basic metrics\n",
    "        report_file.write(f\"üìä Generation {generation_num} Evaluation Results:\\n\")\n",
    "        report_file.write(f\"   Exact Match: {evaluation_results['exact_match']:.3f}\\n\")\n",
    "        report_file.write(f\"   F1 Score: {evaluation_results['f1_score']:.3f}\\n\")\n",
    "        report_file.write(f\"   BERTScore F1: {evaluation_results['bert_score_f1']:.3f}\\n\")\n",
    "        report_file.write(f\"   Semantic Similarity: {evaluation_results['semantic_similarity']:.3f}\\n\\n\")\n",
    "    \n",
    "    # Only print brief summary to console\n",
    "    print(f\"üìä Generation {generation_num} Evaluation Completed:\")\n",
    "    print(f\"   Exact Match: {evaluation_results['exact_match']:.3f}\")\n",
    "    print(f\"   F1 Score: {evaluation_results['f1_score']:.3f}\")\n",
    "    print(f\"   BERTScore F1: {evaluation_results['bert_score_f1']:.3f}\")\n",
    "    print(f\"   Semantic Similarity: {evaluation_results['semantic_similarity']:.3f}\")\n",
    "    \n",
    "    # Save results if requested\n",
    "    if save_results:\n",
    "        results_file = os.path.join(results_dir, f\"generation_{generation_num}_results.json\")\n",
    "        \n",
    "        # Create a serializable version of results for JSON\n",
    "        json_results = {}\n",
    "        for key, value in evaluation_results.items():\n",
    "            if isinstance(value, (list, dict, str, int, float, bool, type(None))):\n",
    "                json_results[key] = value\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                json_results[key] = value.tolist()\n",
    "            else:\n",
    "                json_results[key] = str(value)\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Results saved to: {results_file}\")\n",
    "        print(f\"üìÑ Detailed report saved to: {detailed_report_file}\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"üéâ Evaluation completed for Generation {generation_num}!\")\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eac47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiple_generations(\n",
    "    base_model_name=\"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    base_dataset_dir=\"squad_v2_01percent\",\n",
    "    start_generation=0,\n",
    "    num_generations=4,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    save_results=True,\n",
    "    results_dir=\"evaluation_results_squad01\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate multiple generations in sequence with progress tracking and result comparison.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Base model name for tokenizer and generation 0\n",
    "        base_dataset_dir: Directory containing the original test dataset\n",
    "        start_generation: Starting generation number (0 for base model)\n",
    "        num_generations: Number of generations to evaluate\n",
    "        device: Device for inference\n",
    "        save_results: Whether to save results to JSON files\n",
    "        results_dir: Directory to save all evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all evaluation results by generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Starting evaluation of {num_generations} generations...\")\n",
    "    print(f\"üìã Generations: {start_generation} to {start_generation + num_generations - 1}\")\n",
    "    print(f\"üíæ Results will be saved to: {results_dir}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Dictionary to store all results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Create results directory\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Main progress bar for all generations\n",
    "    generation_progress = tqdm(\n",
    "        range(start_generation, start_generation + num_generations),\n",
    "        desc=\"üîÑ Evaluating Generations\",\n",
    "        unit=\"generation\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    # Track metrics for comparison\n",
    "    metrics_comparison = {\n",
    "        'generations': [],\n",
    "        'exact_match': [],\n",
    "        'f1_score': [],\n",
    "        'bert_score_f1': [],\n",
    "        'semantic_similarity': []\n",
    "    }\n",
    "    \n",
    "    for generation_num in generation_progress:\n",
    "        generation_progress.set_description(f\"üîÑ Evaluating Generation {generation_num}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUATING GENERATION {generation_num}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Evaluate this generation\n",
    "            results = evaluate_generation(\n",
    "                base_model_name=base_model_name,\n",
    "                generation_num=generation_num,\n",
    "                base_dataset_dir=base_dataset_dir,\n",
    "                device=device,\n",
    "                save_results=save_results,\n",
    "                results_dir=results_dir\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            all_results[f\"generation_{generation_num}\"] = results\n",
    "            \n",
    "            # Track metrics for comparison\n",
    "            metrics_comparison['generations'].append(generation_num)\n",
    "            metrics_comparison['exact_match'].append(results['exact_match'])\n",
    "            metrics_comparison['f1_score'].append(results['f1_score'])\n",
    "            metrics_comparison['bert_score_f1'].append(results['bert_score_f1'])\n",
    "            metrics_comparison['semantic_similarity'].append(results['semantic_similarity'])\n",
    "            \n",
    "            # Update progress bar with current metrics\n",
    "            generation_progress.set_postfix({\n",
    "                'EM': f\"{results['exact_match']:.3f}\",\n",
    "                'F1': f\"{results['f1_score']:.3f}\",\n",
    "                'BERT': f\"{results['bert_score_f1']:.3f}\",\n",
    "                'SIM': f\"{results['semantic_similarity']:.3f}\"\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error evaluating generation {generation_num}: {str(e)}\")\n",
    "            # Store error information\n",
    "            all_results[f\"generation_{generation_num}\"] = {\n",
    "                'error': str(e),\n",
    "                'generation': generation_num,\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            continue\n",
    "    \n",
    "    generation_progress.close()\n",
    "    \n",
    "    # Create comprehensive comparison report\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(f\"{'Generation':<12} {'Exact Match':<12} {'F1 Score':<12} {'BERTScore F1':<14} {'Semantic Sim':<14}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, gen in enumerate(metrics_comparison['generations']):\n",
    "        if f\"generation_{gen}\" in all_results and 'error' not in all_results[f\"generation_{gen}\"]:\n",
    "            print(f\"{gen:<12} {metrics_comparison['exact_match'][i]:<12.3f} \"\n",
    "                  f\"{metrics_comparison['f1_score'][i]:<12.3f} \"\n",
    "                  f\"{metrics_comparison['bert_score_f1'][i]:<14.3f} \"\n",
    "                  f\"{metrics_comparison['semantic_similarity'][i]:<14.3f}\")\n",
    "        else:\n",
    "            print(f\"{gen:<12} {'FAILED':<12} {'FAILED':<12} {'FAILED':<14} {'FAILED':<14}\")\n",
    "    \n",
    "    # Find best performing generation for each metric\n",
    "    if metrics_comparison['generations']:\n",
    "        print(f\"\\nüìà BEST PERFORMING GENERATIONS:\")\n",
    "        if metrics_comparison['exact_match']:\n",
    "            best_em_idx = np.argmax(metrics_comparison['exact_match'])\n",
    "            best_em_gen = metrics_comparison['generations'][best_em_idx]\n",
    "            print(f\"   Exact Match: Generation {best_em_gen} ({metrics_comparison['exact_match'][best_em_idx]:.3f})\")\n",
    "        \n",
    "        if metrics_comparison['f1_score']:\n",
    "            best_f1_idx = np.argmax(metrics_comparison['f1_score'])\n",
    "            best_f1_gen = metrics_comparison['generations'][best_f1_idx]\n",
    "            print(f\"   F1 Score: Generation {best_f1_gen} ({metrics_comparison['f1_score'][best_f1_idx]:.3f})\")\n",
    "        \n",
    "        if metrics_comparison['bert_score_f1']:\n",
    "            best_bert_idx = np.argmax(metrics_comparison['bert_score_f1'])\n",
    "            best_bert_gen = metrics_comparison['generations'][best_bert_idx]\n",
    "            print(f\"   BERTScore F1: Generation {best_bert_gen} ({metrics_comparison['bert_score_f1'][best_bert_idx]:.3f})\")\n",
    "        \n",
    "        if metrics_comparison['semantic_similarity']:\n",
    "            best_sim_idx = np.argmax(metrics_comparison['semantic_similarity'])\n",
    "            best_sim_gen = metrics_comparison['generations'][best_sim_idx]\n",
    "            print(f\"   Semantic Similarity: Generation {best_sim_gen} ({metrics_comparison['semantic_similarity'][best_sim_idx]:.3f})\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    if save_results:\n",
    "        # Save all results in one file\n",
    "        comprehensive_results_file = os.path.join(results_dir, \"comprehensive_evaluation_results.json\")\n",
    "        \n",
    "        # Create serializable version\n",
    "        json_all_results = {}\n",
    "        for gen_key, gen_results in all_results.items():\n",
    "            json_gen_results = {}\n",
    "            for key, value in gen_results.items():\n",
    "                if isinstance(value, (list, dict, str, int, float, bool, type(None))):\n",
    "                    json_gen_results[key] = value\n",
    "                elif isinstance(value, np.ndarray):\n",
    "                    json_gen_results[key] = value.tolist()\n",
    "                else:\n",
    "                    json_gen_results[key] = str(value)\n",
    "            json_all_results[gen_key] = json_gen_results\n",
    "        \n",
    "        # Add comparison metrics\n",
    "        json_all_results['comparison_metrics'] = metrics_comparison\n",
    "        json_all_results['evaluation_metadata'] = {\n",
    "            'base_model_name': base_model_name,\n",
    "            'base_dataset_dir': base_dataset_dir,\n",
    "            'start_generation': start_generation,\n",
    "            'num_generations': num_generations,\n",
    "            'total_generations_evaluated': len(metrics_comparison['generations']),\n",
    "            'evaluation_timestamp': str(pd.Timestamp.now()) if 'pd' in globals() else \"timestamp_not_available\"\n",
    "        }\n",
    "        \n",
    "        with open(comprehensive_results_file, 'w') as f:\n",
    "            json.dump(json_all_results, f, indent=2)\n",
    "        \n",
    "        # Save comparison table as CSV\n",
    "        comparison_csv_file = os.path.join(results_dir, \"generations_comparison.csv\")\n",
    "        comparison_df_data = {\n",
    "            'Generation': metrics_comparison['generations'],\n",
    "            'Exact_Match': metrics_comparison['exact_match'],\n",
    "            'F1_Score': metrics_comparison['f1_score'],\n",
    "            'BERTScore_F1': metrics_comparison['bert_score_f1'],\n",
    "            'Semantic_Similarity': metrics_comparison['semantic_similarity']\n",
    "        }\n",
    "        \n",
    "        # Create simple CSV manually\n",
    "        with open(comparison_csv_file, 'w') as f:\n",
    "            f.write(\"Generation,Exact_Match,F1_Score,BERTScore_F1,Semantic_Similarity\\n\")\n",
    "            for i in range(len(metrics_comparison['generations'])):\n",
    "                f.write(f\"{metrics_comparison['generations'][i]},\"\n",
    "                       f\"{metrics_comparison['exact_match'][i]},\"\n",
    "                       f\"{metrics_comparison['f1_score'][i]},\"\n",
    "                       f\"{metrics_comparison['bert_score_f1'][i]},\"\n",
    "                       f\"{metrics_comparison['semantic_similarity'][i]}\\n\")\n",
    "        \n",
    "        print(f\"\\nüíæ Comprehensive results saved to:\")\n",
    "        print(f\"   üìä {comprehensive_results_file}\")\n",
    "        print(f\"   üìã {comparison_csv_file}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Evaluation of {len(metrics_comparison['generations'])} generations completed!\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f306733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting evaluation of 4 generations...\n",
      "üìã Generations: 0 to 3\n",
      "üíæ Results will be saved to: evaluation_results_squad05\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîÑ Evaluating Generation 0:   0%|          | 0/4 [00:00<?, ?generation/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING GENERATION 0\n",
      "============================================================\n",
      "üì• Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfb349b83cf47989ecedf9e89042d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.norm.weight, model.layers.*.input_layernorm.weight, lm_head.weight, model.embed_tokens.weight, model.layers.*.post_attention_layernorm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[AThe following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.dense.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.bias\n",
      "üîÑ Evaluating Generation 1:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:55<02:45, 55.31s/generation, EM=0.273, F1=0.123, BERT=0.843, SIM=0.075]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generation 0 Evaluation Completed:\n",
      "   Exact Match: 0.273\n",
      "   F1 Score: 0.123\n",
      "   BERTScore F1: 0.843\n",
      "   Semantic Similarity: 0.075\n",
      "üíæ Results saved to: evaluation_results_squad05\\generation_0_results.json\n",
      "üìÑ Detailed report saved to: evaluation_results_squad05\\generation_0_detailed_report.txt\n",
      "üéâ Evaluation completed for Generation 0!\n",
      "\n",
      "============================================================\n",
      "EVALUATING GENERATION 1\n",
      "============================================================\n",
      "üì• Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbc2012e3574b92afb0f2dfec9af14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.norm.weight, model.layers.*.input_layernorm.weight, lm_head.weight, model.embed_tokens.weight, model.layers.*.post_attention_layernorm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[AThe following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.dense.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.bias\n",
      "üîÑ Evaluating Generation 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [01:45<01:44, 52.33s/generation, EM=0.091, F1=0.043, BERT=0.749, SIM=0.024]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generation 1 Evaluation Completed:\n",
      "   Exact Match: 0.091\n",
      "   F1 Score: 0.043\n",
      "   BERTScore F1: 0.749\n",
      "   Semantic Similarity: 0.024\n",
      "üíæ Results saved to: evaluation_results_squad05\\generation_1_results.json\n",
      "üìÑ Detailed report saved to: evaluation_results_squad05\\generation_1_detailed_report.txt\n",
      "üéâ Evaluation completed for Generation 1!\n",
      "\n",
      "============================================================\n",
      "EVALUATING GENERATION 2\n",
      "============================================================\n",
      "üì• Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7b24e130a84623a912de8b25e8e8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.norm.weight, model.layers.*.input_layernorm.weight, lm_head.weight, model.embed_tokens.weight, model.layers.*.post_attention_layernorm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[AThe following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.dense.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.bias\n",
      "üîÑ Evaluating Generation 3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [02:35<00:51, 51.22s/generation, EM=0.000, F1=0.026, BERT=0.800, SIM=0.015]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generation 2 Evaluation Completed:\n",
      "   Exact Match: 0.000\n",
      "   F1 Score: 0.026\n",
      "   BERTScore F1: 0.800\n",
      "   Semantic Similarity: 0.015\n",
      "üíæ Results saved to: evaluation_results_squad05\\generation_2_results.json\n",
      "üìÑ Detailed report saved to: evaluation_results_squad05\\generation_2_detailed_report.txt\n",
      "üéâ Evaluation completed for Generation 2!\n",
      "\n",
      "============================================================\n",
      "EVALUATING GENERATION 3\n",
      "============================================================\n",
      "üì• Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d911b9c26c482da64e0b6afa6933e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following TP rules were not applied on any of the layers: {'layers.*.self_attn.qkv_proj': 'colwise_rep', 'layers.*.self_attn.o_proj': 'rowwise_rep', 'layers.*.mlp.gate_up_proj': 'colwise_rep', 'layers.*.mlp.down_proj': 'rowwise_rep'}\n",
      "The following layers were not sharded: model.norm.weight, model.layers.*.input_layernorm.weight, lm_head.weight, model.embed_tokens.weight, model.layers.*.post_attention_layernorm.weight\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\manua\\.cache\\huggingface\\hub\\models--microsoft--phi-3-mini-128k-instruct\\snapshots\\072cb7562cb8c4adf682a8e186aaafa49469eb5d\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[AThe following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.dense.weight, pooler.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, embeddings.LayerNorm.bias, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.attention.self.query.bias\n",
      "üîÑ Evaluating Generation 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [03:25<00:00, 51.40s/generation, EM=0.000, F1=0.026, BERT=0.800, SIM=0.015]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generation 3 Evaluation Completed:\n",
      "   Exact Match: 0.000\n",
      "   F1 Score: 0.026\n",
      "   BERTScore F1: 0.800\n",
      "   Semantic Similarity: 0.015\n",
      "üíæ Results saved to: evaluation_results_squad05\\generation_3_results.json\n",
      "üìÑ Detailed report saved to: evaluation_results_squad05\\generation_3_detailed_report.txt\n",
      "üéâ Evaluation completed for Generation 3!\n",
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Generation   Exact Match  F1 Score     BERTScore F1   Semantic Sim  \n",
      "----------------------------------------------------------------------\n",
      "0            0.273        0.123        0.843          0.075         \n",
      "1            0.091        0.043        0.749          0.024         \n",
      "2            0.000        0.026        0.800          0.015         \n",
      "3            0.000        0.026        0.800          0.015         \n",
      "\n",
      "üìà BEST PERFORMING GENERATIONS:\n",
      "   Exact Match: Generation 0 (0.273)\n",
      "   F1 Score: Generation 0 (0.123)\n",
      "   BERTScore F1: Generation 0 (0.843)\n",
      "   Semantic Similarity: Generation 0 (0.075)\n",
      "\n",
      "üíæ Comprehensive results saved to:\n",
      "   üìä evaluation_results_squad05\\comprehensive_evaluation_results.json\n",
      "   üìã evaluation_results_squad05\\generations_comparison.csv\n",
      "\n",
      "üéâ Evaluation of 4 generations completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_multiple_generations(\n",
    "        base_model_name=\"microsoft/Phi-3-mini-128k-instruct\",\n",
    "        base_dataset_dir=\"squad_v2_01percent\", \n",
    "        start_generation=0,\n",
    "        num_generations=4,  # Will evaluate generations 0, 1, 2, 3\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        save_results=True,\n",
    "        results_dir=\"evaluation_results_squad05\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39842f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
